{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "     \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "     \"spark.dynamicAllocation.enabled\": true,\n",
        "     \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "     \"spark.dynamicAllocation.maxExecutors\": 8\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "processed_text_tbl_name = ''\n",
        "batch_root = ''\n",
        "batch_num = ''\n",
        "batch_description = ''\n",
        "input_container=''\n",
        "output_container=''\n",
        "blob_account_name = ''\n",
        "rules_container = ''\n",
        "file_system = ''\n",
        "processed_images_tbl_name = ''\n",
        "batch_file_count = 0\n",
        "azure_storage_domain = ''\n",
        "minted_tables_output_path = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# set to true if you wish to output data to investigate processing\n",
        "display_results = False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "#import datetime\n",
        "\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'processed_text_tbl_name': processed_text_tbl_name,\n",
        "    'batch_description': batch_description,\n",
        "    'batch_root': batch_root,\n",
        "    'batch_num': batch_num,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from azure.identity import ClientSecretCredential\n",
        "from azure.eventgrid import EventGridPublisherClient, EventGridEvent\n",
        "from azure.mgmt.keyvault import KeyVaultManagementClient\n",
        "from types import SimpleNamespace\n",
        "from datetime import datetime\n",
        "\n",
        "import json\n",
        "import rule_engine\n",
        "import urllib.parse\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql.functions import col, to_json, from_json, lit, explode, concat, udf, regexp_replace\n",
        "from pyspark.sql.types import StringType, MapType, BooleanType"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialise session and config\n",
        "sc = spark.sparkContext\n",
        "spark = SparkSession.builder.appName(f\"TextProcessing {mssparkutils.runtime.context}\").getOrCreate()\n",
        "\n",
        "config = json.loads(''.join(sc.textFile(f'{batch_root}/config.json').collect()))\n",
        "\n",
        "# Set log level\n",
        "if config[\"log_level\"] == \"INFO\":\n",
        "    logger.setLevel(logging.INFO)\n",
        "else:\n",
        "    logger.setLevel(logging.ERROR)\n",
        "    config[\"log_level\"] = \"ERROR\"\n",
        "\n",
        "    \n",
        "log_level = config[\"log_level\"]\n",
        "rulesets_config = config[\"rule_sets\"]\n",
        "web_app_uri = rulesets_config[\"webapp_uri\"]\n",
        "subscriber_uri = rulesets_config[\"teams_webhook_uri\"]\n",
        "alert_email = rulesets_config[\"alert_email\"]\n",
        "text_rulesets_config = rulesets_config[\"text_rule_sets\"]\n",
        "\n",
        "#get value from keyvault to build Event Grid Topic event\n",
        "subscription_id = TokenLibrary.getSecretWithLS(\"keyvault\", 'SubscriptionId')\n",
        "resource_group_name = TokenLibrary.getSecretWithLS(\"keyvault\", 'ResourceGroupName')\n",
        "event_grid_topic_name = TokenLibrary.getSecretWithLS(\"keyvault\", 'EventGridTopicName')\n",
        "event_grid_topic_endpoint = TokenLibrary.getSecretWithLS(\"keyvault\", 'EventGridTopicEndpointUri')\n",
        "tenant_id = TokenLibrary.getSecretWithLS(\"keyvault\", 'TenantID')\n",
        "client_id = TokenLibrary.getSecretWithLS(\"keyvault\", 'ADAppRegClientId')\n",
        "client_secret = TokenLibrary.getSecretWithLS(\"keyvault\", 'ADAppRegClientSecret')\n",
        "event_grid_topic = f'/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.EventGrid/topics/{event_grid_topic_name}'\n",
        "credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n",
        "client = EventGridPublisherClient(event_grid_topic_endpoint, credential)\n",
        "\n",
        "if display_results:\n",
        "    print(config)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyodbc\r\n",
        "from pyspark.sql.functions import current_timestamp\r\n",
        "# Dedicated and serverless SQL config\r\n",
        "dedicated_database = 'dedicated'\r\n",
        "database = 'minted'   \r\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\r\n",
        "\r\n",
        "# secrets\r\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\r\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\r\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseServerlessSQLEndpoint\")\r\n",
        "dedicated_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseDedicatedSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate rules for full text of documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# load the table translated text\n",
        "with tracer.span(name=f'Read the dataframe from the given table {processed_text_tbl_name}'):\n",
        "    docs_df = spark.read.parquet(f'{minted_tables_output_path}{processed_text_tbl_name}')\n",
        "df_original_text = docs_df.select(docs_df.file_name.alias(\"ruleset_file_name\"), docs_df.text_content_target_lang, docs_df.original_lang, docs_df.cluster)\n",
        "df_original_text = df_original_text.withColumn(\"Explanations\", lit(\"n/a\"))\n",
        "df_original_text = df_original_text.na.fill(value=-1)\n",
        "\n",
        "if display_results:\n",
        "    print(df_original_text.count())\n",
        "    df_original_text.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Evaluate fulltext conditions for documents'):\n",
        "    # Load the rulesets to search for and pass to flashtext (https://flashtext.readthedocs.io/en/latest/)\n",
        "    from flashtext import KeywordProcessor\n",
        "\n",
        "    # enumerate all the text_rulesets and add full text conditions to processor\n",
        "    i = 0\n",
        "    for ruleset in text_rulesets_config:\n",
        "        ruleset_name = ruleset[\"rule_set_name\"].replace(\" \", \"_\")\n",
        "\n",
        "        # add condition as keywords for each fulltext condition\n",
        "        keyword_processor = KeywordProcessor()\n",
        "        for fulltext_condition in ruleset[\"fulltext_conditions\"]:\n",
        "            condition_json = \"{\"+fulltext_condition[\"condition\"]+\"}\"\n",
        "            condition_dict = json.loads(condition_json) \n",
        "            keyword_processor.add_keywords_from_dict(condition_dict)\n",
        "\n",
        "        # UDF to call flashtext search for a text value\n",
        "        def fulltext_search(text):\n",
        "            fulltext_result = json.dumps(keyword_processor.extract_keywords(text, span_info=True))\n",
        "            return fulltext_result\n",
        "        udf_fulltext_search = udf(fulltext_search, StringType())            \n",
        "\n",
        "        # perform a single search for all fulltext conditions in the ruleset to save processing time\n",
        "        df_original_text_fulltext_output = df_original_text.withColumn(\"fulltext_result\", udf_fulltext_search(df_original_text.text_content_target_lang)) \n",
        "   \n",
        "        # add rows to output (all_flagged_df)\n",
        "        all_flagged_text_df_fulltext = df_original_text_fulltext_output.filter(df_original_text_fulltext_output.fulltext_result != \"[]\")        \n",
        "        all_flagged_text_df_fulltext = all_flagged_text_df_fulltext.withColumn(\"ruleset_name\", lit(ruleset[\"rule_set_name\"])) \n",
        "        all_flagged_text_df_fulltext = all_flagged_text_df_fulltext.drop(\"text_content_target_lang\")\n",
        "        \n",
        "        # tag each row as being an text type file and folder which contains their enrichments\n",
        "        all_flagged_text_df_fulltext = all_flagged_text_df_fulltext.withColumn(\"file_type\", lit('text'))\n",
        "        all_flagged_text_df_fulltext = all_flagged_text_df_fulltext.withColumn(\"file_enrichment_folder\", lit('/text_processing_json/'))\n",
        "        \n",
        "        #append results to all_flagged_df_fulltext which builds for all rulesets\n",
        "        if i==0:\n",
        "            all_flagged_text_df_final = all_flagged_text_df_fulltext\n",
        "        else:\n",
        "            all_flagged_text_df_final = all_flagged_text_df_final.unionAll(all_flagged_text_df_fulltext)\n",
        "        i=i+1\n",
        "\n",
        "if display_results:\n",
        "    all_flagged_text_df_final.groupBy(\"ruleset_name\").count().show()\n",
        "    all_flagged_text_df_final.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate rules for enrichments on images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# load the table with image enrichments\n",
        "with tracer.span(name=f'Read the dataframe from the given table {processed_images_tbl_name}'):\n",
        "    images_df = spark.read.parquet(f'{minted_tables_output_path}{processed_images_tbl_name}')\n",
        "df_images_processed = images_df.select(images_df.file_name.alias(\"ruleset_file_name\"), images_df.analysis_results, images_df.cluster, images_df.Explanations)\n",
        "#from pyspark.sql.functions import *\n",
        "df_images_processed = df_images_processed.withColumn('Explanations', regexp_replace('Explanations', \"'\", \"\"))\n",
        "df_images_processed = df_images_processed.withColumn(\"original_lang\", lit('n/a'))\n",
        "\n",
        "if display_results:\n",
        "    print(df_images_processed.count())\n",
        "    df_images_processed.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with tracer.span(name='Evaluate fulltext conditions for image enrichments'):\n",
        "    # Load the rulesets to search for and pass to flashtext (https://flashtext.readthedocs.io/en/latest/)\n",
        "    from flashtext import KeywordProcessor\n",
        "\n",
        "\n",
        "    # enumerate all the text_rulesets and add full text conditions to processor\n",
        "    i = 0\n",
        "    for ruleset in text_rulesets_config:\n",
        "        ruleset_name = ruleset[\"rule_set_name\"].replace(\" \", \"_\")\n",
        " \n",
        "        # add condition as keywords for each fulltext \n",
        "        keyword_processor = KeywordProcessor()        \n",
        "        for fulltext_condition in ruleset[\"fulltext_conditions\"]:\n",
        "            condition_json = \"{\"+fulltext_condition[\"condition\"]+\"}\"\n",
        "            condition_dict = json.loads(condition_json) \n",
        "            keyword_processor.add_keywords_from_dict(condition_dict)\n",
        "\n",
        "        # UDF to call flashtext search for a text value\n",
        "        def fulltext_search(text):\n",
        "            fulltext_result = json.dumps(keyword_processor.extract_keywords(text, span_info=True))\n",
        "            return fulltext_result\n",
        "        udf_fulltext_search = udf(fulltext_search, StringType())                        \n",
        "\n",
        "        # perform a single search for all fulltext conditions in the ruleset to save processing time\n",
        "        df_images_processed_fulltext_output = df_images_processed.withColumn(\"fulltext_result\", udf_fulltext_search(df_images_processed.analysis_results.cast(StringType()))) \n",
        "\n",
        "        # add rows to output (all_flagged_df)\n",
        "        all_flagged_images_df_fulltext = df_images_processed_fulltext_output.filter(df_images_processed_fulltext_output.fulltext_result != \"[]\")        \n",
        "        all_flagged_images_df_fulltext = all_flagged_images_df_fulltext.withColumn(\"ruleset_name\", lit(ruleset[\"rule_set_name\"])) \n",
        "        all_flagged_images_df_fulltext = all_flagged_images_df_fulltext.drop(\"analysis_results\")\n",
        "\n",
        "        # tag each row as being an image type file\n",
        "        all_flagged_images_df_fulltext = all_flagged_images_df_fulltext.withColumn(\"file_type\", lit('image'))\n",
        "        all_flagged_images_df_fulltext = all_flagged_images_df_fulltext.withColumn(\"file_enrichment_folder\", lit('/image_processing_json/'))\n",
        "\n",
        "        #append results to all_flagged_df_fulltext which builds for all rulesets\n",
        "        if i==0:\n",
        "            all_flagged_images_df_final = all_flagged_images_df_fulltext\n",
        "        else:\n",
        "            all_flagged_images_df_final = all_flagged_images_df_final.unionAll(all_flagged_images_df_fulltext)\n",
        "        i=i+1\n",
        "        print(all_flagged_images_df_final.count())\n",
        "\n",
        "\n",
        "if display_results:\n",
        "    all_flagged_images_df_final.groupBy(\"ruleset_name\").count().show()\n",
        "    all_flagged_images_df_final.show(1)\n",
        "    all_flagged_images_df_final.show()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Raise rule based alert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# merge results from image and documents, but reorder columns to match first\n",
        "images_df = all_flagged_images_df_final.select(\"ruleset_file_name\", \"ruleset_name\", \"fulltext_result\", \"file_type\", \"file_enrichment_folder\", \"original_lang\", \"cluster\", \"Explanations\")\n",
        "text_df = all_flagged_text_df_final.select(\"ruleset_file_name\", \"ruleset_name\", \"fulltext_result\", \"file_type\", \"file_enrichment_folder\", \"original_lang\", \"cluster\", \"Explanations\")\n",
        "all_flagged_combined_df_final = images_df.unionAll(text_df)\n",
        "all_flagged_combined_df_final.orderBy(['ruleset_file_name'], ascending = [True])\n",
        "\n",
        "if display_results:\n",
        "    all_flagged_combined_df_final.groupBy(\"ruleset_name\").count().show()\n",
        "    all_flagged_combined_df_final.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "with tracer.span(name='Persist ruleset evaluations as table'):\n",
        "    ruleset_eval_tbl_name = f'{batch_num}_ruleset_eval'\n",
        "    all_flagged_combined_df_final.write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{ruleset_eval_tbl_name}')\n",
        "    sql_command = f'''\n",
        "        IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{ruleset_eval_tbl_name}') \n",
        "        CREATE EXTERNAL TABLE [{ruleset_eval_tbl_name}] (\n",
        "            [ruleset_file_name] nvarchar(4000),\n",
        "            [ruleset_name] nvarchar(4000),\n",
        "            [fulltext_result] nvarchar(max),\n",
        "            [file_type] nvarchar(4000),\n",
        "            [file_enrichment_folder] nvarchar(1000),\n",
        "            [original_lang] nvarchar(4000),\n",
        "            [cluster] bigint, \n",
        "            [Explanations] nvarchar(max)\n",
        "        )\n",
        "        WITH (\n",
        "            LOCATION = 'minted_tables/{ruleset_eval_tbl_name}/**', \n",
        "            DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], \n",
        "            FILE_FORMAT = [SynapseParquetFormat]\n",
        "        )\n",
        "    '''\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "        with conn.cursor() as cursor:\n",
        "            cursor.execute(sql_command)\n",
        "\n",
        "with tracer.span(name='Persist as json and queue event grid topic'):\n",
        "    output_path = f'abfss://{output_container}@{blob_account_name}.dfs.{azure_storage_domain}/{batch_num}'\n",
        "    rules_output_path = f'abfss://{rules_container}@{blob_account_name}.dfs.{azure_storage_domain}/{batch_num}'\n",
        "    now = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S%Z\")\n",
        "    \n",
        "    # iterate over each ruleset\n",
        "    for ruleset in text_rulesets_config:\n",
        "        filtered_files = all_flagged_combined_df_final.filter(col(\"ruleset_name\") == ruleset[\"rule_set_name\"])\n",
        "        filtered_files = filtered_files.withColumn(\"file_enrichment_uri\", concat(lit(output_path), col(\"file_enrichment_folder\"), col(\"ruleset_file_name\"), lit(\".output.json\")))          \n",
        "        flagged_files_count = filtered_files.count()\n",
        " \n",
        "        # Collect the data to Python List\n",
        "        file_list_json = \"\"\n",
        "        filtered_files_list = filtered_files.collect()\n",
        "        for filtered_file in filtered_files_list:\n",
        "            file_p = f'{batch_root}/{filtered_file.ruleset_file_name}'\n",
        "            enrichment_p = filtered_file.file_enrichment_uri\n",
        "            search_results = filtered_file.fulltext_result\n",
        "            file_type = filtered_file.file_type\n",
        "            original_lang = filtered_file.original_lang\n",
        "            cluster = filtered_file.cluster\n",
        "            Explanations = filtered_file.Explanations\n",
        "            \n",
        "            # finalise file detail outut json\n",
        "            file_list_json = file_list_json + f'{{\"file_uri\": \"{file_p}\", \"file_type_class\": \"{file_type}\", \"file_enrichment_uri\": \\\n",
        "                \"{enrichment_p}\", \"original_lang\": \"{original_lang}\", \"cluster\": {cluster}, \"Explanations\": \"{Explanations}\", \\\n",
        "                \"fulltext_search_detail\": {search_results}}},'\n",
        "            file_list_json = file_list_json.replace(\"'\", '\"') \n",
        "\n",
        "        file_list_json = file_list_json[:-1]\n",
        "        # build output json\n",
        "        output_json = f'{{\"batch_id\": \"{batch_num}\",' \\\n",
        "            f'\"batch_description\": \"{batch_description}\",' \\\n",
        "            f'\"rule_set_config\": {json.dumps(ruleset)},' \\\n",
        "            f'\"eventDate\": \"{now}\",' \\\n",
        "            f'\"eventDetails\": [' \\\n",
        "            f'{file_list_json}' \\\n",
        "            f']}}'\n",
        "\n",
        "        if display_results:\n",
        "            print(output_json)\n",
        "\n",
        "        # write rules json output to the storage container ready for downstream use\n",
        "        p = f'{rules_output_path}/ruleset_events/{ruleset[\"rule_set_name\"]}.ruleset.output.json' #path to rules output file\n",
        "        mssparkutils.fs.put(p, output_json, overwrite=True)\n",
        "\n",
        "        #generate the Event Grid schema and send to Event Grid Topic and ultimately to a summary alert\n",
        "        webapp_alert_page_path = f'{batch_num}/ruleset_events/{ruleset[\"rule_set_name\"]}.ruleset.output.json' \n",
        "        webapp_alert_page_parameter = '{\"Path\": \"' + webapp_alert_page_path + '\", \"Filter\": {\"original_lang\": \"\", \"cluster\": -1, \"Explanations\": \"\"}}'   \n",
        "        ruleset_event_data = f'{{\"batch_id\": \"{batch_num}\",' \\\n",
        "            f'\"batch_description\": \"{batch_description}\",' \\\n",
        "            f'\"rule_set_config\": {json.dumps(ruleset)},' \\\n",
        "            f'\"eventDate\": \"{now}\",' \\\n",
        "            f'\"eventMetrics\": {{' \\\n",
        "            f'\"rule_events_count\": \"{flagged_files_count}\",' \\\n",
        "            f'\"files_processed_count\": \"{batch_file_count}\",' \\\n",
        "            f'\"event_detail_uri\": \"https://{web_app_uri}/alert/{urllib.parse.quote(webapp_alert_page_parameter)}\"' \\\n",
        "            f'}},' \\\n",
        "            f'\"teams_webhook_endpoint\": \"{subscriber_uri}\",' \\\n",
        "            f'\"alert_email\": \"{alert_email}\"' \\\n",
        "            f'}}'\n",
        "        ruleset_event_data_obj = json.loads(ruleset_event_data)\n",
        "\n",
        "        try:\n",
        "            #queue event grid message\n",
        "            event = EventGridEvent(data= ruleset_event_data_obj, subject=\"MINTED/rulesetevent\", event_type=\"MINTED.ruleTriggered\", data_version=\"1.0\", topic=event_grid_topic)\n",
        "            client.send(event)\n",
        "        except Exception as e:\n",
        "            logger.exception(e)\n",
        "            raise e"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from time import sleep\r\n",
        "#from datetime import datetime, timedelta\r\n",
        "# Update Status Table\r\n",
        "def get_recent_status(batch_num, driver, dedicated_sql_endpoint, dedicated_database, sql_user_name, sql_user_pwd):\r\n",
        "    query = f\"\"\"\r\n",
        "        SELECT TOP (1) \r\n",
        "        [num_stages_complete], [description]\r\n",
        "        FROM [dbo].[batch_status] \r\n",
        "        WHERE [batch_id] = ? \r\n",
        "        ORDER BY [num_stages_complete] DESC;\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    with pyodbc.connect(f'DRIVER={driver};SERVER=tcp:{dedicated_sql_endpoint};PORT=1433;DATABASE={dedicated_database};UID={sql_user_name};PWD={sql_user_pwd}',autocommit=True) as conn:\r\n",
        "        with conn.cursor() as cursor:\r\n",
        "            cursor.execute(query, batch_num)\r\n",
        "            num_stages_complete, description = cursor.fetchone()\r\n",
        "            return num_stages_complete, description\r\n",
        "\r\n",
        "def update_status_table(status_text, minted_tables_path, batch_num, driver, dedicated_sql_endpoint, sql_user_name, sql_user_pwd):\r\n",
        "    retries = 0 \r\n",
        "    exc = ''\r\n",
        "    while retries < 10:\r\n",
        "        try:\r\n",
        "            stages_complete, description = get_recent_status(batch_num, driver, dedicated_sql_endpoint, dedicated_database, sql_user_name, sql_user_pwd)\r\n",
        "            stages_complete += 1\r\n",
        "            status = f'[{stages_complete}/10] {status_text}'\r\n",
        "            x = datetime.now()\r\n",
        "            time_stamp = x.strftime(\"%Y-%m-%d %H:%M:%S\")\r\n",
        "\r\n",
        "            sql_command = f\"UPDATE batch_status SET status = ?, update_time_stamp = ?, num_stages_complete = ? WHERE batch_id = ?\"\r\n",
        "            with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+dedicated_sql_endpoint+';PORT=1433;DATABASE='+dedicated_database+';UID='+sql_user_name+';PWD='+ sql_user_pwd+'',autocommit=True) as conn:\r\n",
        "                with conn.cursor() as cursor:\r\n",
        "                    cursor.execute(sql_command, status, time_stamp, stages_complete, batch_num)\r\n",
        "                    cursor.commit()\r\n",
        "            return \r\n",
        "        except Exception as e:\r\n",
        "            exc_str = str(e)\r\n",
        "            exc = e \r\n",
        "            logger.warning(f'Failed to update status table: {exc_str}, retrying . . .')\r\n",
        "            retries += 1\r\n",
        "            sleep(3)\r\n",
        "\r\n",
        "    raise exc\r\n",
        "\r\n",
        "update_status_table('Text Ruleset Evaluation Complete', minted_tables_output_path, batch_num, driver, dedicated_sql_endpoint, sql_user_name, sql_user_pwd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Complete notebook outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# return name of new table\n",
        "output = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'ruleset_eval_tbl_name': ruleset_eval_tbl_name,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "\n",
        "# Return the object to the pipeline\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\n",
        "mssparkutils.notebook.exit(output['custom_dimensions'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
