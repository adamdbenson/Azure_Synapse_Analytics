{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "     \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "     \"spark.dynamicAllocation.enabled\": true,\n",
        "     \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "     \"spark.dynamicAllocation.maxExecutors\": 4\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import numpy as np\n",
        "import io\n",
        "import pandas as pd\n",
        "import ntpath\n",
        "import os\n",
        "\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import col, pandas_udf, lit, struct, PandasUDFType, udf\n",
        "import pyspark.sql.types as Types\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.ml.clustering import KMeans, BisectingKMeans\n",
        "from pyspark.ml import Pipeline\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from PIL import Image\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "image_features_tbl_name = ''\n",
        "batch_root = ''\n",
        "batch_num = ''\n",
        "file_system = ''\n",
        "image_clustering_config = ''\n",
        "blob_account_name = ''\n",
        "azure_storage_domain = ''\n",
        "minted_tables_output_path = \"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'image_features_tbl_name': image_features_tbl_name,\n",
        "    'batch_root': batch_root,\n",
        "    'batch_num': batch_num,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import uuid\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import StringType, StructType, StructField, IntegerType, FloatType\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialise session and config\n",
        "sc = spark.sparkContext\n",
        "spark = SparkSession.builder.appName(f\"ImageProcessing {mssparkutils.runtime.context}\").getOrCreate()\n",
        "\n",
        "def read_batch_config(batch_root: str):\n",
        "    \"\"\"\n",
        "    We read the config file using the Java File System API as we do not need to let multiple nodes read individual lines and join it\n",
        "    all back together again\n",
        "    \"\"\"\n",
        "    # Change our file system from 'synapse' to 'input'\n",
        "    sc._jsc.hadoopConfiguration().set(\"fs.defaultFS\", file_system)\n",
        "\n",
        "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
        "    config_path = sc._jvm.org.apache.hadoop.fs.Path(f'{batch_root}/config.json')\n",
        "\n",
        "    # If we don't have a batch config, copy the global one.\n",
        "    if fs.exists(config_path) != True:\n",
        "        logger.error(f'{config_path} not found.')\n",
        "\n",
        "    # Open our file directly rather than through spark\n",
        "    input_stream = fs.open(config_path)  # FSDataInputStream\n",
        "\n",
        "    config_string = sc._jvm.java.io.BufferedReader(\n",
        "        sc._jvm.java.io.InputStreamReader(input_stream, sc._jvm.java.nio.charset.StandardCharsets.UTF_8)\n",
        "        ).lines().collect(sc._jvm.java.util.stream.Collectors.joining(\"\\n\"))\n",
        "\n",
        "    # Load it into json    \n",
        "    return json.loads(''.join(config_string), object_hook=lambda dictionary: SimpleNamespace(**dictionary))\n",
        "\n",
        "with tracer.span(name=f\"Load config: {mssparkutils.runtime.context['notebookname']}\"):\n",
        "    try:\n",
        "        config = read_batch_config(batch_root)\n",
        "        job_config = config.__dict__[image_clustering_config]\n",
        "    except Exception as e:\n",
        "        logger.exception(e)\n",
        "        raise e\n",
        "\n",
        "    # Set log level\n",
        "    if config.log_level == \"INFO\":\n",
        "        logger.setLevel(logging.INFO)\n",
        "    else:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "        config.log_level = \"ERROR\"\n",
        "\n",
        "    job_config_parameters = {'custom_dimensions': {\n",
        "        'batch_num': batch_num,\n",
        "        'k': job_config.k,\n",
        "        'pca1': job_config.pca1,\n",
        "        'perplexity': job_config.perplexity,\n",
        "        'algorithm' : job_config.algorithm,\n",
        "        'max_iter': job_config.max_iter,\n",
        "        'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "    } }\n",
        "    \n",
        "    logger.info(f\"{mssparkutils.runtime.context['notebookname']}: JOB_CONFIG\", extra=job_config_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyodbc\r\n",
        "from pyspark.sql.functions import col\r\n",
        "# serverless SQL config\r\n",
        "database = 'minted'   \r\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\r\n",
        "\r\n",
        "# secrets\r\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\r\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\r\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseServerlessSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Load images table'):\n",
        "    df_with_vectors = spark.read.parquet(f'{minted_tables_output_path}{image_features_tbl_name}')\n",
        "    df_with_vectors = df_with_vectors.select(col('path'), col('file_name'), col('features'), col('inceptionv3'), col('desc'), col('content'))\n",
        "\n",
        "    # Until we have more functional test data, we will need to exit here as clustering will fail on the current test data set as it contains 1 record\n",
        "\n",
        "    if df_with_vectors.count() <= 1:\n",
        "        # return name of new table\n",
        "        empty_RDD = spark.sparkContext.emptyRDD()\n",
        "        columns = StructType([\n",
        "            StructField('original_uri', StringType()),\n",
        "            StructField('file_name', StringType()),\n",
        "            StructField('Explanations', StringType()),\n",
        "            StructField('cluster', IntegerType()),\n",
        "            StructField('X', FloatType()),\n",
        "            StructField('Y', FloatType()),\n",
        "        ])\n",
        "        df = spark.createDataFrame(data=empty_RDD, schema=columns)\n",
        "        clustered_image_tbl_name = f'{batch_num}_clustered_image_' + str(image_clustering_config[-1])\n",
        "        df.write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{clustered_image_tbl_name}')\n",
        "\n",
        "        output = {'custom_dimensions': {\n",
        "            'batch_num': batch_num,\n",
        "            'clustered_image_tbl_name': clustered_image_tbl_name,\n",
        "            'notebook_name': mssparkutils.runtime.context['notebookname'],\n",
        "            'error': \"Not enough records to cluster\"\n",
        "        } }\n",
        "        logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\n",
        "        mssparkutils.notebook.exit(output['custom_dimensions'])\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Create image clustering pipeline'):\n",
        "    k = int(job_config.k) \n",
        "    pca_1 = PCA(k=int(job_config.pca1), inputCol=\"features\")\n",
        "    pca_1.setOutputCol(\"pca_features\")\n",
        "\n",
        "    if job_config.algorithm == \"kmeans\":\n",
        "        kmeans = KMeans(k=k, seed=42, initMode=\"k-means||\", distanceMeasure=\"cosine\")\n",
        "    else:\n",
        "        kmeans = BisectingKMeans(k=k, seed=42, distanceMeasure=\"cosine\", maxIter=int(job_config.max_iter))\n",
        "    pipeline = Pipeline(stages=[pca_1, kmeans])\n",
        "\n",
        "with tracer.span(name='Fit image pipeline'):\n",
        "    model = pipeline.fit(df_with_vectors)\n",
        "\n",
        "with tracer.span(name='Transform image pipeline'):    \n",
        "    results = model.transform(df_with_vectors)\n",
        "\n",
        "with tracer.span(name='Convert to pandas image dataframe'):\n",
        "    pandas_df = results.toPandas()\n",
        "\n",
        "with tracer.span(name='T-SNE dimensionality reduction'):\n",
        "    series = pandas_df['features'].apply(lambda x : np.array(x.toArray())).to_numpy().reshape(-1,1)\n",
        "    features = np.apply_along_axis(lambda x : x[0], 1, series)\n",
        "    tsne = TSNE(verbose=1, perplexity=int(job_config.perplexity)) \n",
        "    X_embedded = tsne.fit_transform(features)\n",
        "\n",
        "with tracer.span(name='Clean up image dataframe'):\n",
        "    pandas_df.rename(columns = {'path':'original_uri', 'prediction':'cluster', 'desc': 'Explanations'}, inplace = True)\n",
        "    pandas_df['X'] = X_embedded[:,0]\n",
        "    pandas_df['Y'] = X_embedded[:,1]\n",
        "    pandas_df.drop(columns = [\"content\", \"inceptionv3\", \"features\", \"pca_features\"], inplace = True)\n",
        "\n",
        "with tracer.span(name='Move back to spark RDD'):\n",
        "    df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "with tracer.span(name='Persist to clustered image table'):\n",
        "    clustered_image_tbl_name = f'{batch_num}_clustered_image_' + str(image_clustering_config[-1])\n",
        "    df.write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{clustered_image_tbl_name}')\n",
        "    ext_table_command = f\"\"\"\n",
        "        IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{clustered_image_tbl_name}') \n",
        "        CREATE EXTERNAL TABLE [{clustered_image_tbl_name}] (\n",
        "            [original_uri] nvarchar(4000),\n",
        "            [file_name] nvarchar(4000),\n",
        "            [Explanations] nvarchar(4000),\n",
        "            [cluster] bigint,\n",
        "            [X] float,\n",
        "            [Y] float\n",
        "        ) \n",
        "        WITH (\n",
        "            LOCATION = 'minted_tables/{clustered_image_tbl_name}/**', \n",
        "            DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], \n",
        "            FILE_FORMAT = [SynapseParquetFormat]\n",
        "        )\n",
        "    \"\"\"\n",
        "    with pyodbc.connect(f'DRIVER={driver};SERVER=tcp:{serverless_sql_endpoint};PORT=1433;DATABASE={database};UID={sql_user_name};PWD={sql_user_pwd}') as conn:\n",
        "        with conn.cursor() as cursor:\n",
        "            cursor.execute(ext_table_command)\n",
        "\n",
        "# return name of new table\n",
        "output = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'clustered_image_tbl_name': clustered_image_tbl_name,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "\n",
        "# Return the object to the pipeline\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\n",
        "mssparkutils.notebook.exit(output['custom_dimensions'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
