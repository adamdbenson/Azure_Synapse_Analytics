{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Text Prep\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "     \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "     \"spark.dynamicAllocation.enabled\": true,\n",
        "     \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "     \"spark.dynamicAllocation.maxExecutors\": 8,\n",
        "     \"spark.jars.packages\": \"com.microsoft.azure:synapseml_2.12:0.10.0-19-c3a445c5-SNAPSHOT\",\n",
        "     \"spark.jars.repositories\": \"https://mmlspark.azureedge.net/maven\",\n",
        "     \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12,com.fasterxml.jackson.core:jackson-databind\",\n",
        "     \"spark.yarn.user.classpath.first\": \"true\"\n",
        "   }\n",
        "}\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "documents_tbl_name = \"\"\n",
        "batch_root = \"\"\n",
        "batch_num = \"\"\n",
        "file_system = \"\"\n",
        "document_cracking_timeout = 180\n",
        "blob_account_name = \"\"\n",
        "minted_tables_output_path = \"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyodbc\r\n",
        "from pyspark.sql.functions import current_timestamp\r\n",
        "# Dedicated and serverless SQL config\r\n",
        "dedicated_database = \"dedicated\"\r\n",
        "database = 'minted'   \r\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\r\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\r\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\r\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseServerlessSQLEndpoint\")\r\n",
        "dedicated_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseDedicatedSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "# Column names \n",
        "file_path_col = \"file_path\"\n",
        "file_name_col = \"file_name\"\n",
        "file_type_col = \"file_type\"\n",
        "text_content_col = \"text_content\"\n",
        "entropy_error_col = \"entropy_error\"\n",
        "original_lang_col = \"original_lang\"\n",
        "original_lang_translate_col = \"original_lang_translate\"\n",
        "original_lang_prob_col = \"original_lang_prob\"\n",
        "text_content_target_lang_col = f\"text_content_target_lang\"\n",
        "extraction_error_col = \"extraction_error\"\n",
        "language_detection_error_col = \"language_detection_error\"\n",
        "translation_error_col = \"translation_error\"\n",
        "\n",
        "key_col = file_path_col\n",
        "\n",
        "output_cols = [ \n",
        "    file_name_col, \n",
        "    file_type_col,\n",
        "    text_content_col,\n",
        "    original_lang_col, \n",
        "    text_content_target_lang_col\n",
        "]\n",
        "\n",
        "error_cols = [\n",
        "    extraction_error_col,\n",
        "    entropy_error_col,\n",
        "    language_detection_error_col,\n",
        "    translation_error_col\n",
        "]\n",
        "\n",
        "# NOTE: these columns are dropped after the processing is finished\n",
        "text_content_truncated_col = \"text_content_truncated\"\n",
        "text_content_split_col = \"text_content_split\"\n",
        "language_detector_key_col = \"language_detector_key\"\n",
        "translation_key_col = \"translation_key\"\n",
        "\n",
        "# Load secrets\n",
        "translation_keys = mssparkutils.credentials.getSecretWithLS(\"keyvault\", 'TranslationKeys').split(',')\n",
        "language_detector_keys = mssparkutils.credentials.getSecretWithLS(\"keyvault\", 'TextAnalyticsKeys').split(',')\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "\n",
        "# Parameters for truncating and batching\n",
        "truncated_text_len = 1000\n",
        "# For Language Detection, we can send up to 1000 documents per request\n",
        "# see https://docs.microsoft.com/en-us/azure/cognitive-services/language-service/language-detection/how-to/call-api#data-limits\n",
        "detect_batch_size = 10\n",
        "\n",
        "# For Translation, we can send up to 10000 characters per request\n",
        "# see https://docs.microsoft.com/en-us/azure/cognitive-services/translator/request-limits\n",
        "translate_document_min_length = 9900\n",
        "translate_document_max_length = 10000\n",
        "\n",
        "# Useful for debugging\n",
        "display_dataframes = False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "import datetime\n",
        "\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "  'documents_tbl_name': documents_tbl_name,\n",
        "  'batch_root': batch_root,\n",
        "  'batch_num': batch_num,\n",
        "  'file_system': file_system,\n",
        "  'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from time import sleep\r\n",
        "# Update Status Table\r\n",
        "def get_recent_status(batch_num, driver, dedicated_sql_endpoint, dedicated_database, sql_user_name, sql_user_pwd):\r\n",
        "    query = f\"\"\"\r\n",
        "        SELECT TOP (1) \r\n",
        "        [num_stages_complete], [description]\r\n",
        "        FROM [dbo].[batch_status] \r\n",
        "        WHERE [batch_id] = ?\r\n",
        "        ORDER BY [num_stages_complete] DESC;\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    with pyodbc.connect(f'DRIVER={driver};SERVER=tcp:{dedicated_sql_endpoint};PORT=1433;DATABASE={dedicated_database};UID={sql_user_name};PWD={sql_user_pwd}',autocommit=True) as conn:\r\n",
        "        with conn.cursor() as cursor:\r\n",
        "            cursor.execute(query, batch_num)\r\n",
        "            num_stages_complete, description = cursor.fetchone()\r\n",
        "            return num_stages_complete, description\r\n",
        "\r\n",
        "def update_status_table(status_text, minted_tables_path, batch_num, driver, dedicated_sql_endpoint, sql_user_name, sql_user_pwd):\r\n",
        "    retries = 0 \r\n",
        "    exc = ''\r\n",
        "    while retries < 10:\r\n",
        "        try:\r\n",
        "            stages_complete, description = get_recent_status(batch_num, driver, dedicated_sql_endpoint, dedicated_database, sql_user_name, sql_user_pwd)\r\n",
        "            stages_complete += 1\r\n",
        "            status = f'[{stages_complete}/10] {status_text}'\r\n",
        "            x = datetime.datetime.now()\r\n",
        "            time_stamp = x.strftime(\"%Y-%m-%d %H:%M:%S\")\r\n",
        "\r\n",
        "            sql_command = f\"UPDATE batch_status SET status = ?, update_time_stamp = ?, num_stages_complete = ? WHERE batch_id = ?\"\r\n",
        "            with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+dedicated_sql_endpoint+';PORT=1433;DATABASE='+dedicated_database+';UID='+sql_user_name+';PWD='+ sql_user_pwd+'',autocommit=True) as conn:\r\n",
        "                with conn.cursor() as cursor:\r\n",
        "                    cursor.execute(sql_command, status, time_stamp, stages_complete, batch_num)\r\n",
        "                    cursor.commit()\r\n",
        "            return \r\n",
        "        except Exception as e:\r\n",
        "            exc_str = str(e)\r\n",
        "            exc = e \r\n",
        "            logger.warning(f'Failed to update status table: {exc_str}, retrying . . .')\r\n",
        "            retries += 1\r\n",
        "            sleep(3)\r\n",
        "\r\n",
        "    raise exc\r\n",
        "\r\n",
        "update_status_table('Text Prep Started', minted_tables_output_path, batch_num, driver, dedicated_sql_endpoint, sql_user_name, sql_user_pwd)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "import json\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import re\n",
        "from types import SimpleNamespace\n",
        "from typing import List\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import StringType, StructType, StructField\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "from synapse.ml.stages import FlattenBatch, FixedMiniBatchTransformer, DynamicMiniBatchTransformer\n",
        "from synapse.ml.cognitive import Translate, LanguageDetector\n",
        "from synapse.ml.featurize.text import PageSplitter\n",
        "\n",
        "def read_batch_config(batch_root: str):\n",
        "    \"\"\"\n",
        "    We read the config file using the Java File System API as we do not need to let multiple nodes read individual lines and join it\n",
        "    all back together again\n",
        "    \"\"\"\n",
        "    # Change our file system from 'synapse' to 'input'\n",
        "    sc._jsc.hadoopConfiguration().set(\"fs.defaultFS\", file_system)\n",
        "\n",
        "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
        "    config_path = sc._jvm.org.apache.hadoop.fs.Path(f'{batch_root}/config.json')\n",
        "\n",
        "    # If we don't have a batch config, copy the global one.\n",
        "    if fs.exists(config_path) != True:\n",
        "        logger.error(f'{config_path} not found.')\n",
        "\n",
        "    # Open our file directly rather than through spark\n",
        "    input_stream = fs.open(config_path)  # FSDataInputStream\n",
        "\n",
        "    config_string = sc._jvm.java.io.BufferedReader(\n",
        "        sc._jvm.java.io.InputStreamReader(input_stream, sc._jvm.java.nio.charset.StandardCharsets.UTF_8)\n",
        "        ).lines().collect(sc._jvm.java.util.stream.Collectors.joining(\"\\n\"))\n",
        "\n",
        "    # Load it into json    \n",
        "    return json.loads(''.join(config_string), object_hook=lambda dictionary: SimpleNamespace(**dictionary))\n",
        "\n",
        "with tracer.span(name='Initialise Spark session'):\n",
        "    sc = spark.sparkContext\n",
        "    spark = SparkSession.builder.appName(f\"TextProcessing {mssparkutils.runtime.context}\").getOrCreate()\n",
        "\n",
        "with tracer.span(name=f\"Load config: {mssparkutils.runtime.context['notebookname']}\"):\n",
        "    try:\n",
        "        config = read_batch_config(batch_root)\n",
        "    except Exception as e:\n",
        "        logger.exception(e)\n",
        "        raise e\n",
        "\n",
        "    # Set log level\n",
        "    if config.log_level == \"INFO\":\n",
        "        logger.setLevel(logging.INFO)\n",
        "    else:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "        config.log_level = \"ERROR\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "with tracer.span(name='Extract document contents'):\n",
        "    # Extract the text from complex document types\n",
        "    documents_cracked_view_name = f\"{batch_num}_cracked_view\"\n",
        "\n",
        "    mssparkutils.notebook.run(\"2_Text_Extraction\", document_cracking_timeout, {\n",
        "        \"file_system\": file_system,\n",
        "        \"documents_tbl_name\": f'{documents_tbl_name}',\n",
        "        \"documents_cracked_view_name\": documents_cracked_view_name,\n",
        "        \"minted_tables_output_path\": f'{minted_tables_output_path}'\n",
        "    })\n",
        "\n",
        "    documents_cracked_tbl_name = f\"{batch_num}_documents_cracked\"\n",
        "    df = spark.sql(f\"\"\"\n",
        "        SELECT {file_name_col}, {file_type_col}, {file_path_col}, {text_content_col}, {extraction_error_col} \n",
        "        FROM {documents_cracked_view_name}\n",
        "    \"\"\")\n",
        "    \n",
        "    # Repartition df by file_path to optimise upcoming joins\n",
        "    df = df.repartition(file_path_col)\n",
        "\n",
        "    if display_dataframes:\n",
        "        df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Attempting JSON parsing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "def walk(current_node):\r\n",
        "    ''' Traverse a parsed JSON, concatenating string values with newlines and\r\n",
        "        dropping key names + non-string leaf node values '''\r\n",
        "    my_strings = []\r\n",
        "    if type(current_node) == list:\r\n",
        "        for item in current_node:\r\n",
        "            my_strings.append(walk(item))\r\n",
        "    elif type(current_node) == dict:\r\n",
        "        for item in current_node.values():\r\n",
        "            my_strings.append(walk(item))\r\n",
        "    elif type(current_node) == str:\r\n",
        "        my_strings.append(current_node)\r\n",
        "    else:\r\n",
        "        return('')\r\n",
        "    return('\\n'.join([i for i in my_strings if len(i) > 0]))\r\n",
        "\r\n",
        "def attempt_json_handling(input_string):\r\n",
        "    ''' If a JSON file, traverse tree, concatenating string values with newlines '''\r\n",
        "    try:\r\n",
        "        # This will fail for non-json docs\r\n",
        "        output_string = walk(json.loads(input_string))\r\n",
        "        return(output_string)\r\n",
        "    except:\r\n",
        "        return(input_string)\r\n",
        "\r\n",
        "udf_attempt_json_handling = F.udf(lambda z: attempt_json_handling(z), StringType())\r\n",
        "df = df.withColumn(text_content_col, udf_attempt_json_handling(col(text_content_col)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Detecting text with unusually high or low entropy\r\n",
        "\r\n",
        "Some languages have especially large character sets (Chinese, Japanese). We will explicitly prevent these documents from being filtered out later in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Don't attempt to filter on entropy if the document is very short.\r\n",
        "# (Short documents tend to have a larger range of entropy values.)\r\n",
        "entropy_min_char_length = 100\r\n",
        "\r\n",
        "# These values were arrived at by examining the perf2 and Abbottabad datasets.\r\n",
        "entropy_min = 2.7\r\n",
        "entropy_max = 3.5\r\n",
        "\r\n",
        "# Simplify multiple contiguous whitespace characters so they don't skew the\r\n",
        "# calculated entropy too much.\r\n",
        "whitespace_simplifier=re.compile(r\"\\s+\")\r\n",
        "\r\n",
        "# These are languages where we know we need to tolerate high character entropy.\r\n",
        "# This list is probably not exhaustive. We will use this set at the end of the\r\n",
        "# notebook to retain documents where these languages were detected, even if\r\n",
        "# they threw an entropy error.\r\n",
        "high_entropy_languages = {\r\n",
        "    'yue', # Cantonese (traditional)\r\n",
        "    'lzh', # Chinese (literary)\t\t\t\r\n",
        "    'zh-Hans', # Chinese Simplified\r\n",
        "    'zh-Hant', # Chinese Traditional\r\n",
        "    'ja' # Japanese\r\n",
        "}\r\n",
        "\r\n",
        "def check_for_entropy_issues(input_string):\r\n",
        "    if type(input_string) != str:\r\n",
        "        return('Could not calculate entropy for text content of type {}'.format(\r\n",
        "            type(input_string)))\r\n",
        "    input_string = whitespace_simplifier.sub(' ', input_string)\r\n",
        "    char_counts = defaultdict(lambda: 0)\r\n",
        "    n_chars = len(input_string)\r\n",
        "\r\n",
        "    if n_chars < entropy_min_char_length:\r\n",
        "        return(None)\r\n",
        "    \r\n",
        "    # Calculate entropy\r\n",
        "    for character in input_string:\r\n",
        "        char_counts[character] += 1\r\n",
        "\r\n",
        "    entropy = 0.0\r\n",
        "    for val in char_counts.values():\r\n",
        "        p = val / n_chars\r\n",
        "        entropy -= p * math.log(p)\r\n",
        "\r\n",
        "    # Return an error message about extreme entropy, if appropriate\r\n",
        "    if (entropy < entropy_min):\r\n",
        "        return('Char-level entropy too low ({:0.6f} < {:0.6f}) minimum'.format(\r\n",
        "            entropy,\r\n",
        "            entropy_min\r\n",
        "        ))\r\n",
        "    elif (entropy > entropy_max):\r\n",
        "        return('Char-level entropy too high ({:0.6f} > {:0.6f}) maximum'.format(\r\n",
        "            entropy,\r\n",
        "            entropy_max\r\n",
        "        ))\r\n",
        "    else:\r\n",
        "        return(None)\r\n",
        "\r\n",
        "udf_entropy = F.udf(lambda z: check_for_entropy_issues(z), StringType())\r\n",
        "df = df.withColumn(entropy_error_col, udf_entropy(col(text_content_col)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detecting language in the source text, producing a column `original_lang`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "with tracer.span(name='Set Batch Size'):\n",
        "    # For performance, we assume that the whole text will be in the same language as the first truncated_text_lencharacters.\n",
        "    # This allows us to truncate the text and then take advantage of the batch API to reduce number of calls to Cognitive Services.\n",
        "\n",
        "    # Truncate text\n",
        "    df_detect = df.withColumn(text_content_truncated_col, F.substring(col(text_content_col), 0, truncated_text_len))\n",
        "\n",
        "    # Group rows into batches\n",
        "    fmbt = (FixedMiniBatchTransformer()\n",
        "          .setBatchSize(detect_batch_size))\n",
        "\n",
        "    df_detect = fmbt.transform(df_detect)\n",
        "\n",
        "    if display_dataframes:\n",
        "        df_detect.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def rand_key(keys: List[str]) :\n",
        "    return random.sample(keys, 1)[0]\n",
        "\n",
        "with tracer.span(name='Distribute translation keys across rows'):\n",
        "    udf_language_detector_key = F.udf(lambda: rand_key(language_detector_keys), StringType())\n",
        "    df_detect = df_detect.withColumn(language_detector_key_col, udf_language_detector_key())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "with tracer.span(name='Detect Language'):\n",
        "    # Language detection\n",
        "    # NOTE: Language detection is also performed as part of Translate API,\n",
        "    # however, we don't run Translate on text in target language, as it is quite expensive and about 3x time slower than Detect.\n",
        "    detect = (\n",
        "        LanguageDetector()\n",
        "        .setSubscriptionKeyCol(language_detector_key_col)\n",
        "        .setLocation(config.location)\n",
        "        .setTextCol(text_content_truncated_col)\n",
        "        .setOutputCol(original_lang_col)\n",
        "        .setErrorCol(language_detection_error_col)\n",
        "        <<SYNAPSE_ML_LANG_DETECT_ENDPOINT_CMD>>\n",
        "    )\n",
        "\n",
        "    df_detect_results_batched = detect.transform(df_detect)\n",
        "\n",
        "    if display_dataframes:\n",
        "        df_detect_results_batched.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "error_response_schema = StructType(\n",
        "    [StructField(\"error\", StructType(\n",
        "        [StructField(\"code\", StringType()), StructField(\"message\", StringType())]\n",
        "    ))]\n",
        ")\n",
        "\n",
        "with tracer.span(name='Join Detect results back with source dataframe'):\n",
        "    # Flatten the columns to separate individual files again\n",
        "    flattener = FlattenBatch()\n",
        "    df_detect_results = flattener.transform(df_detect_results_batched)\n",
        "    # Unwrap the result column\n",
        "    df_detect_results = df_detect_results\\\n",
        "        .withColumn(original_lang_col, df_detect_results[original_lang_col].detectedLanguage.iso6391Name)\\\n",
        "        .withColumn(language_detection_error_col, F.from_json(\n",
        "            df_detect_results[language_detection_error_col][\"response\"], error_response_schema)[\"error\"][\"message\"])\n",
        "\n",
        "    if display_dataframes:\n",
        "        df_detect_results.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Translate any text that isn't already in the target language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import uuid\r\n",
        "import requests\r\n",
        "import pandas as pd\r\n",
        "import time\r\n",
        "\r\n",
        "class TranslationException(Exception):\r\n",
        "    pass\r\n",
        "\r\n",
        "class TextTranslation:\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        translation_location,\r\n",
        "        translation_keys,\r\n",
        "        translation_endpoint,\r\n",
        "        target_lang='en',\r\n",
        "        original_lang_column='original_lang',\r\n",
        "        input_text_column='text_content',\r\n",
        "        output_column='text_content_target_lang',\r\n",
        "        error_column='translation_error',\r\n",
        "        min_page_len=49000,\r\n",
        "        max_page_len=50000,\r\n",
        "        boundary_regex='\\\\s',\r\n",
        "\r\n",
        "    ):\r\n",
        "        self.translation_endpoint = translation_endpoint\r\n",
        "        self.translation_url = f'{self.translation_endpoint}translate'\r\n",
        "        self.translation_location = translation_location\r\n",
        "        self.translation_keys = translation_keys\r\n",
        "        self.target_lang = target_lang\r\n",
        "        self.original_lang_column = original_lang_column\r\n",
        "        self.input_text_column = input_text_column\r\n",
        "        self.output_column = output_column\r\n",
        "        self.error_column = error_column\r\n",
        "        self.min_page_len = min_page_len\r\n",
        "        self.max_page_len = max_page_len\r\n",
        "        self.boundary_regex = boundary_regex\r\n",
        "    \r\n",
        "    def transform_df(self, df):\r\n",
        "        return pd.DataFrame.from_records([self.transform(row) for _idx, row in df.iterrows()])\r\n",
        "        \r\n",
        "\r\n",
        "    def transform(self, row):\r\n",
        "        return self.translate_row(row)\r\n",
        "\r\n",
        "    def __call__(self, df_iter):\r\n",
        "        for df in df_iter:\r\n",
        "            yield self.transform_df(df)\r\n",
        "        \r\n",
        "    def page_splitter(self, input_text):\r\n",
        "        if len(input_text) < self.max_page_len:\r\n",
        "            return [input_text]\r\n",
        "\r\n",
        "        page_start_idx = 0\r\n",
        "        left_idx = self.min_page_len\r\n",
        "        right_idx = self.max_page_len\r\n",
        "        pages = []\r\n",
        "        while right_idx < len(input_text):\r\n",
        "            # look between min_len and max_len for a boundary\r\n",
        "            string_slice = input_text[left_idx:right_idx]\r\n",
        "            boundary_indices = [match.end() for match in re.finditer(self.boundary_regex, string_slice)]\r\n",
        "            # if we don't find one we take the max_len of string\r\n",
        "            if len(boundary_indices) == 0:\r\n",
        "                pages.append(input_text[page_start_idx:right_idx])\r\n",
        "                page_start_idx = right_idx\r\n",
        "            else:\r\n",
        "                boundary_idx = boundary_indices[-1] \r\n",
        "                page_end_idx = left_idx + boundary_idx\r\n",
        "                pages.append(input_text[page_start_idx:page_end_idx])\r\n",
        "                page_start_idx = page_end_idx\r\n",
        "\r\n",
        "            left_idx = page_start_idx + self.min_page_len\r\n",
        "            right_idx = page_start_idx + self.max_page_len\r\n",
        "\r\n",
        "        # handle the end of the text\r\n",
        "        if page_start_idx < len(input_text): \r\n",
        "            pages.append(input_text[page_start_idx:])\r\n",
        "\r\n",
        "        return pages\r\n",
        "    \r\n",
        "    @property\r\n",
        "    def translation_key(self):\r\n",
        "        return random.sample(self.translation_keys, 1)[0]        \r\n",
        "\r\n",
        "    def translate_page(self, page):\r\n",
        "        params = {\r\n",
        "            'api-version': '3.0', \r\n",
        "            'to': [self.target_lang]\r\n",
        "        }\r\n",
        "\r\n",
        "        headers = {\r\n",
        "            'Ocp-Apim-Subscription-Key': self.translation_key, \r\n",
        "            'Ocp-Apim-Subscription-Region': self.translation_location, \r\n",
        "            'Content-type': 'application/json', \r\n",
        "            'X-ClientTraceId': str(uuid.uuid4())\r\n",
        "        }\r\n",
        "\r\n",
        "        body = [{'text': page}]\r\n",
        "        wait_time = 3\r\n",
        "        retries = 0\r\n",
        "        translation_exception_str = ''\r\n",
        "        while retries < 10:        \r\n",
        "            req = requests.post(self.translation_url, params=params, headers=headers, json=body)\r\n",
        "            req_data = req.json()\r\n",
        "            if req.ok:\r\n",
        "                detected_language = req_data[0]['detectedLanguage']\r\n",
        "                page_original_lang = detected_language['language']\r\n",
        "                page_original_lang_score = detected_language['score']\r\n",
        "                translated_page = req_data[0]['translations'][0]['text']\r\n",
        "                return page_original_lang, page_original_lang_score, translated_page\r\n",
        "            # if req.status_code == 429:\r\n",
        "            #     time.sleep(3)\r\n",
        "\r\n",
        "            # else:\r\n",
        "            translation_exception_str = json.dumps(req_data)\r\n",
        "            time.sleep(wait_time)\r\n",
        "            wait_time += 5\r\n",
        "            retries += 1\r\n",
        "\r\n",
        "        raise TranslationException(translation_exception_str)\r\n",
        "\r\n",
        "    def translate_text(self, input_text):\r\n",
        "        pages = self.page_splitter(input_text)\r\n",
        "        original_lang = ''\r\n",
        "        original_lang_score = 0 \r\n",
        "        translated_text = ''\r\n",
        "        for page in pages:\r\n",
        "            page_original_lang, page_original_lang_score, translated_page = self.translate_page(page)\r\n",
        "            translated_text = translated_text + translated_page\r\n",
        "            if page_original_lang_score > original_lang_score:\r\n",
        "                original_lang = page_original_lang\r\n",
        "        return original_lang, translated_text        \r\n",
        "\r\n",
        "\r\n",
        "    def translate_row(self, row):\r\n",
        "        input_text = row[self.input_text_column]\r\n",
        "        row[self.output_column] = None\r\n",
        "        row[self.error_column] = None\r\n",
        "        \r\n",
        "        if input_text is None:\r\n",
        "            return row\r\n",
        "\r\n",
        "        if not row[self.original_lang_column] == self.target_lang:\r\n",
        "            try:\r\n",
        "                original_lang, text_content_target_lang = self.translate_text(input_text)\r\n",
        "                row[self.output_column] = text_content_target_lang\r\n",
        "                row[self.original_lang_column] = original_lang\r\n",
        "            except TranslationException as e:\r\n",
        "                row[self.error_column] = str(e)\r\n",
        "        else:\r\n",
        "            row[self.output_column] = input_text\r\n",
        "\r\n",
        "\r\n",
        "        return row \r\n",
        "\r\n",
        "\r\n",
        "output_schema = StructType([\r\n",
        "    StructField(file_name_col, StringType()),\r\n",
        "    StructField(file_type_col, StringType()),\r\n",
        "    StructField(file_path_col, StringType()),\r\n",
        "    StructField(text_content_col, StringType()),\r\n",
        "    StructField(extraction_error_col, StringType()),\r\n",
        "    StructField(entropy_error_col, StringType()),\r\n",
        "    StructField(text_content_truncated_col, StringType()),\r\n",
        "    StructField(language_detector_key_col, StringType()),\r\n",
        "    StructField(language_detection_error_col, StringType()),\r\n",
        "    StructField(original_lang_col, StringType()),\r\n",
        "    StructField(text_content_target_lang_col, StringType()),\r\n",
        "    StructField(translation_error_col, StringType())\r\n",
        "])\r\n",
        "\r\n",
        "text_translator = TextTranslation(\r\n",
        "    config.location,\r\n",
        "    translation_keys,\r\n",
        "    config.translation_endpoint,\r\n",
        "    target_lang=config.prep.target_language,\r\n",
        "    original_lang_column=original_lang_col,\r\n",
        "    input_text_column=text_content_col,\r\n",
        "    output_column=text_content_target_lang_col,\r\n",
        "    error_column=translation_error_col,\r\n",
        "    )\r\n",
        "\r\n",
        "df_translate = df_detect_results.mapInPandas(text_translator, output_schema)\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Persist resulting dataframe and error dataframes as Synapse tables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_results = df_translate\n",
        "df_results.cache() # cache as we will split out success/error tables from here\n",
        "\n",
        "with tracer.span(name='Persist document contents as table'):\n",
        "    # Remove null rows as downstream notebooks won't be able to work with them.\n",
        "    # Also drop rows with entropy errors since they would appear as nonsense\n",
        "    # entries in document clusters, etc. -- unless the detected original language\n",
        "    # is one where we know to expect high entropy levels.\n",
        "    df_output = df_results\\\n",
        "                    .where(\n",
        "                        (df_results[entropy_error_col].isNull()) |\n",
        "                        (df_results[original_lang_col].isin(high_entropy_languages))\n",
        "                    ).select(file_path_col, *output_cols)\\\n",
        "                    .na.drop(subset=[text_content_target_lang_col])\n",
        "    df_error = df_results\\\n",
        "                    .where(\n",
        "                        (df_results[text_content_target_lang_col] == \"\") |\n",
        "                        (df_results[text_content_target_lang_col].isNull()) |\n",
        "                        ((df_results[entropy_error_col].isNotNull()) &\n",
        "                        (~df_results[original_lang_col].isin(high_entropy_languages)))\n",
        "                        )\\\n",
        "                    .select(file_path_col, *error_cols)\n",
        "\n",
        "    documents_contents_tbl_name = f\"{batch_num}_documents_contents\"\n",
        "    df_output.write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{documents_contents_tbl_name}')\n",
        "    df_output.printSchema()\n",
        "\n",
        "    text_prep_errors_tbl_name = f\"{batch_num}_text_prep_errors\"\n",
        "    df_error.write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{text_prep_errors_tbl_name}')\n",
        "    df_error.printSchema()\n",
        "\n",
        "    # create remote sql tables over the parquet file\n",
        "    df_output_sql_command = f\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{documents_contents_tbl_name}') CREATE EXTERNAL TABLE [{documents_contents_tbl_name}] ([file_name] nvarchar(4000), [file_type] nvarchar(4000), [file_path] nvarchar(4000)) WITH (LOCATION = 'minted_tables/{documents_contents_tbl_name}/**', DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "    df_error = f\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{text_prep_errors_tbl_name}') CREATE EXTERNAL TABLE [{text_prep_errors_tbl_name}] ([file_name] nvarchar(4000), [extraction_error] nvarchar(4000), [entropy_error] nvarchar(4000), [language_detection_error] nvarchar(4000), [translation_error] nvarchar(4000)) WITH (LOCATION = 'minted_tables/{text_prep_errors_tbl_name}/**', DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "      with conn.cursor() as cursor:\n",
        "        cursor.execute(df_output_sql_command)\n",
        "        cursor.execute(df_error)\n",
        "\n",
        "output = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'documents_contents_tbl_name': documents_contents_tbl_name,\n",
        "    'text_prep_errors_tbl_name': text_prep_errors_tbl_name,\n",
        "    'file_system': file_system,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "\n",
        "# Return the object to the pipeline\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\n",
        "mssparkutils.notebook.exit(output['custom_dimensions'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
