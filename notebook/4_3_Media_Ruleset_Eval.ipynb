{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "     \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "     \"spark.dynamicAllocation.enabled\": true,\n",
        "     \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "     \"spark.dynamicAllocation.maxExecutors\": 8\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "# This is a parameters cell where we define the batch_file details as params to be passed in by the pipeline\n",
        "key_vault_name = ''\n",
        "batch_num = ''\n",
        "batch_root = ''\n",
        "file_system = ''\n",
        "output_container = ''\n",
        "media_path = ''\n",
        "enriched_media_tbl_name = ''\n",
        "rules_container = ''\n",
        "batch_description = ''\n",
        "azure_storage_domain = ''\n",
        "blob_account_name = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# set to true if you wish to output data to investigate processing\n",
        "display_results = False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "config_integration.trace_integrations(['logging'])\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "media_contents_tbl_name = f'{batch_num}_submitted_media'\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'file_system': file_system,\n",
        "    'media_contents_tbl_name': media_contents_tbl_name,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyodbc\r\n",
        "from pyspark.sql.functions import current_timestamp\r\n",
        "# Dedicated and serverless SQL config\r\n",
        "dedicated_database = 'dedicated'\r\n",
        "database = 'minted'   \r\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\r\n",
        "minted_tables_path = f'abfss://synapse@{blob_account_name}.dfs.{azure_storage_domain}/minted_tables/'\r\n",
        "\r\n",
        "# secrets\r\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\r\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\r\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseServerlessSQLEndpoint\")\r\n",
        "dedicated_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseDedicatedSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from azure.identity import ClientSecretCredential\n",
        "from azure.eventgrid import EventGridPublisherClient, EventGridEvent\n",
        "from azure.mgmt.keyvault import KeyVaultManagementClient\n",
        "from types import SimpleNamespace\n",
        "from datetime import datetime\n",
        "import json\n",
        "import rule_engine\n",
        "import urllib.parse\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql.functions import col, to_json, from_json, lit, explode, concat, udf, regexp_replace, json_tuple\n",
        "from pyspark.sql.types import StringType, MapType, BooleanType\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Load keys, set defaults\n",
        "with tracer.span(name='load values from key vault'):\n",
        "    instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "    subscription_id = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SubscriptionId\")\n",
        "    resource_group_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"ResourceGroupName\")\n",
        "    tenant_id = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"TenantID\")\n",
        "    client_id = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"ADAppRegClientId\")\n",
        "    client_secret = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"ADAppRegClientSecret\")\n",
        "    storage_account_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"StorageAccountName\")\n",
        "    storage_account_key = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"StorageAccountKey\")\n",
        "\n",
        "    ruleset_eval_tbl_name = f'{batch_num}_media_ruleset_eval'\n",
        "\n",
        "    azure_resource_manager = \"<<TF_VAR_azure_arm_management_api>>\";\n",
        "    credential = ClientSecretCredential(tenant_id, client_id, client_secret)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initialise session and config\n",
        "sc = spark.sparkContext\n",
        "spark = SparkSession.builder.appName(f\"Media Processing {mssparkutils.runtime.context}\").getOrCreate()\n",
        "config = json.loads(''.join(sc.textFile(f'{batch_root}/config.json').collect()))\n",
        "\n",
        "# Set log level\n",
        "if config[\"log_level\"] == \"INFO\":\n",
        "    logger.setLevel(logging.INFO)\n",
        "else:\n",
        "    logger.setLevel(logging.ERROR)\n",
        "    config[\"log_level\"] = \"ERROR\"\n",
        "\n",
        "    \n",
        "log_level = config[\"log_level\"]\n",
        "rulesets_config = config[\"rule_sets\"]\n",
        "web_app_uri = rulesets_config[\"webapp_uri\"]\n",
        "subscriber_uri = rulesets_config[\"teams_webhook_uri\"]\n",
        "alert_email = rulesets_config[\"alert_email\"]\n",
        "text_rulesets_config = rulesets_config[\"text_rule_sets\"]\n",
        "\n",
        "# get value from keyvault to build Event Grid Topic event\n",
        "event_grid_topic_name = mssparkutils.credentials.getSecretWithLS('keyvault', 'EventGridTopicName')\n",
        "event_grid_topic_endpoint = mssparkutils.credentials.getSecretWithLS('keyvault', 'EventGridTopicEndpointUri')\n",
        "event_grid_topic = f'/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.EventGrid/topics/{event_grid_topic_name}'\n",
        "credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n",
        "client = EventGridPublisherClient(event_grid_topic_endpoint, credential)\n",
        "\n",
        "if display_results:\n",
        "    print(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Evaluate rules for full text of media if this is the last expected callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# get count of how many media files we have in the batch - we only process alerts and rules once we have processed all media files\n",
        "submitted_media_tbl_name = f'{batch_num}_submitted_media'\n",
        "df_enriched_media_sql = f\"SELECT COUNT(*) FROM [dbo].[{submitted_media_tbl_name}] WHERE state = 'Processing'\"\n",
        "media_file_count = 0\n",
        "with pyodbc.connect(f'DRIVER={driver};SERVER=tcp:{serverless_sql_endpoint};PORT=1433;DATABASE={database};UID={sql_user_name};PWD={sql_user_pwd}') as conn:\n",
        "        with conn.cursor() as cursor:\n",
        "            cursor.execute(df_enriched_media_sql)\n",
        "            media_file_count, *_ = cursor.fetchone()\n",
        "print(media_file_count)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# load the table of media enrichments from the previous notebook. We will run a search against these enrichments\n",
        "with tracer.span(name=f'Read the dataframe from the given table {enriched_media_tbl_name}'):\n",
        "    media_df = spark.read.parquet(f'{minted_tables_path}{enriched_media_tbl_name}')\n",
        "    media_df = media_df.select(col(\"media_path\"),col(\"media_file_name\"),col(\"video_id\"),col(\"enrichments\"),col(\"original_lang\"),json_tuple(col(\"enrichments\"),\"state\")) \\\n",
        "        .toDF(\"media_path\",\"media_file_name\",\"video_id\",\"enrichments\", \"original_lang\", \"state\")\n",
        "    processed_count = media_df.filter(media_df.state == \"Processed\").count()\n",
        "    failed_count = media_df.filter(media_df.state == \"Failed\").count()\n",
        "\n",
        "    if display_results:\n",
        "        media_df.show()\n",
        "        print(\"processed files =\", processed_count)\n",
        "        print(\"failed files =\", failed_count)\n",
        "\n",
        "    # abort notebook if all files are not completed - we haven't received all the callbacks\n",
        "    if media_file_count > processed_count + failed_count:\n",
        "        output = {\n",
        "            'custom_dimensions': {\n",
        "                'batch_num': batch_num,\n",
        "                'ruleset_eval_tbl_name': ruleset_eval_tbl_name,\n",
        "                'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "            } \n",
        "        }\n",
        "        # exit notebook if this isn't the last expected callback\n",
        "        logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT \", extra=output)\n",
        "        mssparkutils.notebook.exit(\"exited as the full set of media files is not complete - we only raise a single alert\")\n",
        "\n",
        "if display_results:\n",
        "    media_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Evaluate fulltext conditions for documents'):\n",
        "\n",
        "    # Only process rules/alerts if the full media batch is completed - we exitied the notebook of the last cell if that is the case\n",
        "    # Load the rulesets to search for and pass to flashtext (https://flashtext.readthedocs.io/en/latest/)\n",
        "    from flashtext import KeywordProcessor\n",
        "\n",
        "    # enumerate all the text_rulesets and add full text conditions to processor\n",
        "    i = 0\n",
        "    for ruleset in text_rulesets_config:\n",
        "        ruleset_name = ruleset[\"rule_set_name\"].replace(\" \", \"_\")\n",
        "\n",
        "        # add condition as keywords for each fulltext condition\n",
        "        keyword_processor = KeywordProcessor()\n",
        "        for fulltext_condition in ruleset[\"fulltext_conditions\"]:\n",
        "            condition_json = \"{\"+fulltext_condition[\"condition\"]+\"}\"\n",
        "            condition_dict = json.loads(condition_json) \n",
        "            keyword_processor.add_keywords_from_dict(condition_dict)\n",
        "\n",
        "        # UDF to call flashtext search for a text value\n",
        "        def fulltext_search(text):\n",
        "            fulltext_result = json.dumps(keyword_processor.extract_keywords(text, span_info=True))\n",
        "            return fulltext_result\n",
        "        udf_fulltext_search = udf(fulltext_search, StringType())            \n",
        "\n",
        "        # perform a single search for all fulltext conditions in the ruleset to save processing time\n",
        "        media_df_fulltext_output = media_df.withColumn(\"fulltext_result\", udf_fulltext_search(media_df.enrichments))  \n",
        "\n",
        "        # add rows to output (all_flagged_df)\n",
        "        all_flagged_media = media_df_fulltext_output.filter(media_df_fulltext_output.fulltext_result != \"[]\")        \n",
        "        all_flagged_media = all_flagged_media.withColumn(\"ruleset_name\", lit(ruleset[\"rule_set_name\"]))   \n",
        "\n",
        "if display_results:\n",
        "    all_flagged_media.show()   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Get count of file in the original manifest\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# load the table with original manifest\n",
        "manifest_df_name = f'{batch_num}_manifest'    \n",
        "with tracer.span(name=f'Read the dataframe from the given table {manifest_df_name}'):\n",
        "    batch_df = spark.read.parquet(f'{minted_tables_path}{manifest_df_name}')\n",
        "    batch_file_count = batch_df.count()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Raise rule based alert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Persist as json and queue event grid topic'):\n",
        "    output_path = f'abfss://{output_container}@{storage_account_name}.dfs.{azure_storage_domain}/{batch_num}'\n",
        "    rules_output_path = f'abfss://{rules_container}@{storage_account_name}.dfs.{azure_storage_domain}/{batch_num}'\n",
        "    now = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S%Z\")\n",
        "    files_count = media_file_count\n",
        "    media_enrichment_folder = \"/media_processing_json/\"\n",
        "    \n",
        "    # iterate over each ruleset\n",
        "    for ruleset in text_rulesets_config:\n",
        "        filtered_files = all_flagged_media.filter(col(\"ruleset_name\") == ruleset[\"rule_set_name\"])\n",
        "        filtered_files = filtered_files.withColumn(\"file_enrichment_uri\", concat(lit(output_path), lit(media_enrichment_folder), col(\"media_file_name\"), lit(\".output.json\")))          \n",
        "        flagged_files_count = filtered_files.count()\n",
        "        print(flagged_files_count)\n",
        "\n",
        "        # Only process an event if the count of matching files is > 0\n",
        "        if flagged_files_count > 0:\n",
        "\n",
        "            # Collect the data to Python List\n",
        "            file_list_json = []\n",
        "            filtered_files_list = filtered_files.collect()\n",
        "            for filtered_file in filtered_files_list:\n",
        "                file_p = f'{batch_root}/{filtered_file.media_file_name}'\n",
        "                enrichment_p = filtered_file.file_enrichment_uri\n",
        "                search_results = filtered_file.fulltext_result\n",
        "                file_type = \"media\"\n",
        "                original_lang = filtered_file.original_lang\n",
        "                cluster = \"n/a\"\n",
        "                Explanations = \"n/a\"\n",
        "                \n",
        "                # finalise file detail outut json\n",
        "                file_list_json.append({\n",
        "                    \"file_uri\": file_p,\n",
        "                    \"file_type_class\": file_type,\n",
        "                    \"file_enrichment_uri\": enrichment_p,\n",
        "                    \"original_lang\": original_lang,\n",
        "                    \"cluster\": cluster,\n",
        "                    \"Explanations\": Explanations,\n",
        "                    \"fulltext_search_detail\": search_results\n",
        "                })\n",
        "\n",
        "            # build output json encoded string\n",
        "            output_json = json.dumps({\n",
        "                \"batch_id\": batch_num,\n",
        "                \"batch_description\": batch_description,\n",
        "                \"rule_set_config\": ruleset,\n",
        "                \"eventDate\": now,\n",
        "                \"eventDetails\": file_list_json\n",
        "            })\n",
        "            \n",
        "            # write rules json output to the storage container ready for downstream use\n",
        "            p = f'{rules_output_path}/ruleset_events/{ruleset[\"rule_set_name\"]}.media.ruleset.output.json' #path to rules output file\n",
        "            mssparkutils.fs.put(p, output_json, overwrite=True)     \n",
        "\n",
        "            # generate the Event Grid schema and send to Event Grid Topic and ultimately to a summary alert\n",
        "            webapp_alert_page_path = f'{batch_num}/ruleset_events/{ruleset[\"rule_set_name\"]}.media.ruleset.output.json'\n",
        "\n",
        "            webapp_alert_page_parameter = json.dumps({\n",
        "                \"Path\": webapp_alert_page_path,\n",
        "                \"Filter\": {\n",
        "                    \"original_lang\": \"\",\n",
        "                    \"cluster\": -1, \n",
        "                    \"Explanations\": \"\"\n",
        "                }\n",
        "            })\n",
        "            \n",
        "            ruleset_event_data_obj = {\n",
        "                \"batch_id\": batch_num,\n",
        "                \"batch_description\": batch_description,\n",
        "                \"rule_set_config\": ruleset,\n",
        "                \"eventDate\": now,\n",
        "                \"eventMetrics\": {\n",
        "                    \"rule_events_count\": flagged_files_count,\n",
        "                    \"files_processed_count\": batch_file_count,\n",
        "                    \"event_detail_uri\": f\"https://{web_app_uri}/alert/{urllib.parse.quote(webapp_alert_page_parameter)}\"\n",
        "                },\n",
        "                \"teams_webhook_endpoint\": subscriber_uri,\n",
        "                \"alert_email\": alert_email   \n",
        "            }\n",
        "            \n",
        "            print(ruleset_event_data_obj) \n",
        "\n",
        "            try:\n",
        "                #queue event grid message\n",
        "                event = EventGridEvent(data= ruleset_event_data_obj, subject=\"MINTED/rulesetmediaevent\", event_type=\"MINTED.ruleTriggered\", data_version=\"1.0\", topic=event_grid_topic)\n",
        "                client.send(event)\n",
        "            except Exception as e:\n",
        "                logger.exception(e)\n",
        "                raise e                  \n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# persist output to SQL\n",
        "with tracer.span(name='Persist ruleset evaluations as table'):\n",
        "    all_flagged_media.write.mode(\"overwrite\").parquet(f'{minted_tables_path}{ruleset_eval_tbl_name}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from time import sleep\r\n",
        "# Update Status Table\r\n",
        "def get_recent_status(batch_num, driver, dedicated_sql_endpoint, dedicated_database, sql_user_name, sql_user_pwd):\r\n",
        "    query = f\"\"\"\r\n",
        "        SELECT TOP (1) \r\n",
        "        [num_stages_complete], [description]\r\n",
        "        FROM [dbo].[batch_status] \r\n",
        "        WHERE [batch_id] = ?\r\n",
        "        ORDER BY [num_stages_complete] DESC;\r\n",
        "    \"\"\"\r\n",
        "    with pyodbc.connect(f'DRIVER={driver};SERVER=tcp:{dedicated_sql_endpoint};PORT=1433;DATABASE={dedicated_database};UID={sql_user_name};PWD={sql_user_pwd}',autocommit=True) as conn:\r\n",
        "        with conn.cursor() as cursor:\r\n",
        "            cursor.execute(query, batch_num)\r\n",
        "            num_stages_complete, description = cursor.fetchone()\r\n",
        "            return num_stages_complete, description\r\n",
        "\r\n",
        "def update_status_table(status_text, minted_tables_path, batch_num, driver, dedicated_sql_endpoint, sql_user_name, sql_user_pwd):\r\n",
        "    retries = 0 \r\n",
        "    exc = ''\r\n",
        "    while retries < 10:\r\n",
        "        try:\r\n",
        "            stages_complete, description = get_recent_status(batch_num, driver, dedicated_sql_endpoint, dedicated_database, sql_user_name, sql_user_pwd)\r\n",
        "            stages_complete += 1\r\n",
        "            status = f'[{stages_complete}/10] {status_text}'\r\n",
        "            x = datetime.now()\r\n",
        "            time_stamp = x.strftime(\"%Y-%m-%d %H:%M:%S\")\r\n",
        "\r\n",
        "            sql_command = f\"UPDATE batch_status SET status = ?, update_time_stamp = ?, num_stages_complete = ? WHERE batch_id = ?\"\r\n",
        "            with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+dedicated_sql_endpoint+';PORT=1433;DATABASE='+dedicated_database+';UID='+sql_user_name+';PWD='+ sql_user_pwd+'',autocommit=True) as conn:\r\n",
        "                with conn.cursor() as cursor:\r\n",
        "                    cursor.execute(sql_command, status, time_stamp, stages_complete, batch_num)\r\n",
        "                    cursor.commit()\r\n",
        "            return \r\n",
        "        except Exception as e:\r\n",
        "            exc_str = str(e)\r\n",
        "            exc = e \r\n",
        "            logger.warning(f'Failed to update status table: {exc_str}, retrying . . .')\r\n",
        "            retries += 1\r\n",
        "            sleep(3)\r\n",
        "\r\n",
        "    raise exc\r\n",
        "\r\n",
        "update_status_table('Media Processing Complete', minted_tables_path, batch_num, driver, dedicated_sql_endpoint, sql_user_name, sql_user_pwd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Complete notebook outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# return name of new table\n",
        "output = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'ruleset_eval_tbl_name': ruleset_eval_tbl_name,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "\n",
        "# Return the object to the pipeline\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\n",
        "mssparkutils.notebook.exit(output['custom_dimensions'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
