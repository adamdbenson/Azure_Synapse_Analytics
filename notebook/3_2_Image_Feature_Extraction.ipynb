{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "     \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "     \"spark.dynamicAllocation.enabled\": true,\n",
        "     \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "     \"spark.dynamicAllocation.maxExecutors\": 4\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "image_contents_tbl_name = ''\n",
        "batch_root = ''\n",
        "batch_num = ''\n",
        "file_system = ''\n",
        "minted_tables_output_path = \"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import numpy as np\n",
        "import io\n",
        "import pandas as pd\n",
        "import ntpath\n",
        "import os\n",
        "\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import col, pandas_udf, lit, struct, PandasUDFType, udf\n",
        "import pyspark.sql.types as Types\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "\n",
        "from PIL import Image\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'image_contents_tbl_name': image_contents_tbl_name,\n",
        "    'batch_root': batch_root,\n",
        "    'batch_num': batch_num,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import uuid\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import StringType, StructType, StructField\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialise session and config\n",
        "sc = spark.sparkContext\n",
        "spark = SparkSession.builder.appName(f\"ImageProcessing {mssparkutils.runtime.context}\").getOrCreate()\n",
        "\n",
        "def read_batch_config(batch_root: str):\n",
        "    \"\"\"\n",
        "    We read the config file using the Java File System API as we do not need to let multiple nodes read individual lines and join it\n",
        "    all back together again\n",
        "    \"\"\"\n",
        "    # Change our file system from 'synapse' to 'input'\n",
        "    sc._jsc.hadoopConfiguration().set(\"fs.defaultFS\", file_system)\n",
        "\n",
        "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
        "    config_path = sc._jvm.org.apache.hadoop.fs.Path(f'{batch_root}/config.json')\n",
        "\n",
        "    # If we don't have a batch config, copy the global one.\n",
        "    if fs.exists(config_path) != True:\n",
        "        logger.error(f'{config_path} not found.')\n",
        "\n",
        "    # Open our file directly rather than through spark\n",
        "    input_stream = fs.open(config_path)  # FSDataInputStream\n",
        "\n",
        "    config_string = sc._jvm.java.io.BufferedReader(\n",
        "        sc._jvm.java.io.InputStreamReader(input_stream, sc._jvm.java.nio.charset.StandardCharsets.UTF_8)\n",
        "        ).lines().collect(sc._jvm.java.util.stream.Collectors.joining(\"\\n\"))\n",
        "\n",
        "    # Load it into json    \n",
        "    return json.loads(''.join(config_string), object_hook=lambda dictionary: SimpleNamespace(**dictionary))\n",
        "\n",
        "with tracer.span(name=f\"Load config: {mssparkutils.runtime.context['notebookname']}\"):\n",
        "    try:\n",
        "        config = read_batch_config(batch_root)\n",
        "    except Exception as e:\n",
        "        logger.exception(e)\n",
        "        raise e\n",
        "\n",
        "    # Set log level\n",
        "    if config.log_level == \"INFO\":\n",
        "        logger.setLevel(logging.INFO)\n",
        "    else:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "        config.log_level = \"ERROR\"\n",
        "\n",
        "    job_config_parameters = {'custom_dimensions': {\n",
        "        'batch_num': batch_num,\n",
        "        'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "    } }\n",
        "    \n",
        "    logger.info(f\"{mssparkutils.runtime.context['notebookname']}: JOB_CONFIG\", extra=job_config_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyodbc\r\n",
        "from pyspark.sql.functions import col\r\n",
        "# serverless SQL config\r\n",
        "database = 'minted'   \r\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\r\n",
        "\r\n",
        "# secrets\r\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\r\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\r\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseServerlessSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Load images table'):\n",
        "    images_df = spark.read.parquet(f'{minted_tables_output_path}{image_contents_tbl_name}')\n",
        "    images_df = images_df.select(col('path'), col('file_name'), col('file_type'), col('content'))\n",
        "    # images_df = spark.sql(\"SELECT path, file_name, file_type, content FROM \" + image_contents_tbl_name)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def get_filename(row):\n",
        "    return ntpath.basename(row)\n",
        "\n",
        "def preprocess(img_data):\n",
        "  try:\n",
        "    img = Image.open(io.BytesIO(img_data)).convert('RGB')\n",
        "    img = img.resize([299, 299])\n",
        "    x = np.asarray(img, dtype=\"float32\")\n",
        "  except:\n",
        "    x = np.zeros((299, 299, 3))\n",
        "  return preprocess_input(x)\n",
        "\n",
        "def keras_model_udf(model_fn):\n",
        "  def predict(image_batch_iter):\n",
        "    model = model_fn()\n",
        "    for img_series in image_batch_iter:\n",
        "      processed_images = np.array([preprocess(img) for img in img_series])\n",
        "      predictions = model.predict(processed_images, batch_size=64)\n",
        "      predicted_labels = [x[0] for x in decode_predictions(predictions, top=1)]\n",
        "      results = []\n",
        "      for i, tuples in enumerate(predicted_labels):\n",
        "        all_predictions = tuples + (predictions[i],)\n",
        "        results.append(all_predictions)\n",
        "\n",
        "      yield pd.DataFrame(results)\n",
        "\n",
        "  return_type = \"class: string, desc: string, score:float, inceptionv3: array<float>\"\n",
        "  return pandas_udf(return_type, PandasUDFType.SCALAR_ITER)(predict)  \n",
        "\n",
        "def inceptionv3_fn():\n",
        "    model = InceptionV3(weights='imagenet')\n",
        "    model.set_weights(bc_model_weights.value)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Extract features and predict InceptionV3'):\n",
        "    model = InceptionV3()\n",
        "    bc_model_weights = sc.broadcast(model.get_weights())\n",
        "    inceptionv3_udf = keras_model_udf(inceptionv3_fn)\n",
        "    predictions = images_df.withColumn(\"preds\", inceptionv3_udf(col(\"content\")))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Build vectors dataframe'):\n",
        "    list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
        "    df_with_vectors = predictions.select(\n",
        "        predictions[\"path\"],\n",
        "        predictions[\"content\"],\n",
        "        predictions[\"file_name\"],\n",
        "        predictions[\"file_type\"],\n",
        "        predictions[\"preds.desc\"], \n",
        "        predictions[\"preds.inceptionv3\"], \n",
        "        list_to_vector_udf(predictions[\"preds.inceptionv3\"]).alias(\"features\")\n",
        "    )\n",
        "with tracer.span(name='Persist to feature image table'):\n",
        "    image_features_tbl_name = f'{batch_num}_image_features'\n",
        "    df_with_vectors.write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{image_features_tbl_name}')\n",
        "    sql_command = f'''\n",
        "        IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{image_features_tbl_name}') \n",
        "        CREATE EXTERNAL TABLE [{image_features_tbl_name}] (\n",
        "            [path] nvarchar(1000), \n",
        "            [content] varbinary(max), \n",
        "            [file_name] nvarchar(1000), \n",
        "            [file_type] nvarchar(1000), \n",
        "            [desc] nvarchar(4000), \n",
        "            [inceptionv3] varchar(max),\n",
        "            [features] varchar(max)\n",
        "        )\n",
        "        WITH (\n",
        "            LOCATION = 'minted_tables/{image_features_tbl_name}/**', \n",
        "            DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], \n",
        "            FILE_FORMAT = [SynapseParquetFormat]\n",
        "        )\n",
        "    '''\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "        with conn.cursor() as cursor:\n",
        "            cursor.execute(sql_command)\n",
        "\n",
        "# return name of new table\n",
        "output = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'image_features_tbl_name': image_features_tbl_name,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "\n",
        "# Return the object to the pipeline\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\n",
        "mssparkutils.notebook.exit(output['custom_dimensions'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
