{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "     \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "     \"spark.dynamicAllocation.enabled\": true,\n",
        "     \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "     \"spark.dynamicAllocation.maxExecutors\": 8\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "enriched_text_tbl_name = ''\n",
        "summarized_text_xsum_tbl_name = ''\n",
        "summarized_text_dailymail_tbl_name = ''\n",
        "clustered_text_tbl_name = ''\n",
        "clustered_multimodal_tbl_name = ''\n",
        "documents_contents_tbl_name = ''\n",
        "text_prep_errors_tbl_name = ''\n",
        "batch_root = ''\n",
        "batch_num = ''\n",
        "batch_description = ''\n",
        "input_container=''\n",
        "output_container=''\n",
        "blob_account_name = ''\n",
        "azure_storage_domain = ''\n",
        "minted_tables_output_path = ''\n",
        "manifest_file_name = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyodbc\r\n",
        "import pandas as pd\r\n",
        "from pyspark.sql.functions import current_timestamp\r\n",
        "# Dedicated and serverless SQL config\r\n",
        "dedicated_database = \"dedicated\"\r\n",
        "database = 'minted'   \r\n",
        "driver = '{ODBC Driver 17 for SQL Server}'\r\n",
        "\r\n",
        "# Secrets\r\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\r\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\r\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseServerlessSQLEndpoint\")\r\n",
        "dedicated_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseDedicatedSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'documents_contents_tbl_name': documents_contents_tbl_name,\n",
        "    'enriched_text_tbl_name': enriched_text_tbl_name,\n",
        "    'clustered_text_tbl_name': clustered_text_tbl_name,\n",
        "    'batch_description': batch_description,\n",
        "    'batch_root': batch_root,\n",
        "    'batch_num': batch_num,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Column names \n",
        "file_path_col = \"file_path\"\n",
        "file_name_col = \"file_name\"\n",
        "file_type_col = \"file_type\"\n",
        "text_content_col = \"text_content\"\n",
        "original_lang_col = \"original_lang\"\n",
        "text_content_target_lang_col = f\"text_content_target_lang\"\n",
        "extraction_error_col = \"extraction_error\"\n",
        "language_detection_error_col = \"language_detection_error\"\n",
        "translation_error_col = \"translation_error\"\n",
        "\n",
        "key_phrases_col = \"key_phrases\"\n",
        "named_entities_col = \"named_entities\"\n",
        "pii_col = \"pii\"\n",
        "pii_redacted_text_col = \"pii_redacted_text\"\n",
        "text_analysis_error_col = \"text_analysis_error\"\n",
        "summarized_text_xsum_col = \"summarized_text_xsum\"\n",
        "summarization_xsum_error_col = \"summarization_xsum_error\"\n",
        "summarized_text_dailymail_col = \"summarized_text_dailymail\"\n",
        "summarization_dailymail_error_col = \"summarization_dailymail_error\"\n",
        "processed_text_col = \"processed_text\"\n",
        "cluster_col = \"cluster\"\n",
        "x_col = \"X\"\n",
        "y_col = \"Y\"\n",
        "errors_col = \"errors\"\n",
        "\n",
        "# For multimodal clustering results\n",
        "summarized_text_xsum_col_1 = \"summarized_text_xsum_1\"\n",
        "topic_name_col = \"topic_name\"\n",
        "topic_name_col_1 = \"topic_name_1\"\n",
        "cluster_col_1 = \"cluster_1\"\n",
        "x_col_1 = \"X_1\"\n",
        "y_col_1 = \"Y_1\"\n",
        "\n",
        "key_col = file_path_col\n",
        "\n",
        "clustering_cols = [file_path_col, processed_text_col, cluster_col, x_col, y_col, original_lang_col]\n",
        "\n",
        "# For multimodal clustering results\n",
        "multimodal_clustering_cols = [file_path_col, summarized_text_xsum_col_1, topic_name_col_1, cluster_col_1, x_col_1, y_col_1]\n",
        "\n",
        "output_cols = [ \n",
        "    file_name_col, \n",
        "    file_type_col,\n",
        "    text_content_col,\n",
        "    original_lang_col, \n",
        "    text_content_target_lang_col\n",
        "]\n",
        "\n",
        "error_cols = [\n",
        "    extraction_error_col,\n",
        "    language_detection_error_col,\n",
        "    translation_error_col,\n",
        "    text_analysis_error_col\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "import os\n",
        "import csv\n",
        "from types import SimpleNamespace\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Initialise session and config\n",
        "sc = spark.sparkContext\n",
        "spark = SparkSession.builder.appName(f\"TextProcessing {mssparkutils.runtime.context}\").getOrCreate()\n",
        "\n",
        "config = json.loads(''.join(sc.textFile(f'{batch_root}/config.json').collect()), object_hook=lambda dictionary: SimpleNamespace(**dictionary))\n",
        "\n",
        "# Set log level\n",
        "if config.log_level == \"INFO\":\n",
        "    logger.setLevel(logging.INFO)\n",
        "else:\n",
        "    logger.setLevel(logging.ERROR)\n",
        "    config.log_level = \"ERROR\"\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with tracer.span(name=f'Read the dataframe from the given table {documents_contents_tbl_name}'):\n",
        "\n",
        "    documents_contents_df = spark.read.parquet(f'{minted_tables_output_path}{documents_contents_tbl_name}')\n",
        "    enriched_text_df = spark.read.parquet(f'{minted_tables_output_path}{enriched_text_tbl_name}')\n",
        "    clustered_text_df = spark.read.parquet(f'{minted_tables_output_path}{clustered_text_tbl_name}')\n",
        "    text_prep_errors_df = spark.read.parquet(f'{minted_tables_output_path}{text_prep_errors_tbl_name}')\n",
        "    \n",
        "    # For multimodal clustering results\n",
        "    clustered_multimodal_df = (spark.read.parquet(f'{minted_tables_output_path}{clustered_multimodal_tbl_name}')\n",
        "        .withColumnRenamed(summarized_text_xsum_col, summarized_text_xsum_col_1)\n",
        "        .withColumnRenamed(topic_name_col, topic_name_col_1)\n",
        "        .withColumnRenamed(cluster_col, cluster_col_1)\n",
        "        .withColumnRenamed(x_col, x_col_1)\n",
        "        .withColumnRenamed(y_col, y_col_1)\n",
        "    )\n",
        "    clustered_multimodal_df = (clustered_multimodal_df\n",
        "        .where(clustered_multimodal_df.type_of_file == 'text')\n",
        "    )\t\n",
        "    clustered_multimodal_df = (clustered_multimodal_df\n",
        "        .drop(clustered_multimodal_df.type_of_file)\n",
        "    )\n",
        "\n",
        "    docs_df = (documents_contents_df\n",
        "        .join(enriched_text_df, [documents_contents_df[file_path_col] == enriched_text_df[file_path_col]],'left_outer')\n",
        "        .join(clustered_text_df, [documents_contents_df[file_path_col] == clustered_text_df[file_path_col]],'left_outer')\n",
        "        .join(text_prep_errors_df, [documents_contents_df[file_path_col] == text_prep_errors_df[file_path_col]],'left_outer')\n",
        "        .join(clustered_multimodal_df, [documents_contents_df[file_path_col] == clustered_multimodal_df[file_path_col]],'left_outer') # For multimodal clustering results\n",
        "        .select(\n",
        "            documents_contents_df.file_path,\n",
        "            documents_contents_df.file_name,\n",
        "            documents_contents_df.file_type,\n",
        "            documents_contents_df.text_content,\n",
        "            documents_contents_df.original_lang,\n",
        "            documents_contents_df.text_content_target_lang,\n",
        "            enriched_text_df[key_phrases_col],\n",
        "            enriched_text_df[named_entities_col],\n",
        "            enriched_text_df[pii_col],\n",
        "            enriched_text_df[pii_redacted_text_col],\n",
        "            enriched_text_df[text_analysis_error_col],\n",
        "            clustered_text_df[processed_text_col],\n",
        "            clustered_text_df[cluster_col],\n",
        "            clustered_text_df[x_col],\n",
        "            clustered_text_df[y_col],\n",
        "            text_prep_errors_df[extraction_error_col],\n",
        "            text_prep_errors_df[language_detection_error_col],\n",
        "            text_prep_errors_df[translation_error_col],\n",
        "            clustered_multimodal_df[summarized_text_xsum_col_1], # For multimodal clustering results\n",
        "            clustered_multimodal_df[topic_name_col_1], # For multimodal clustering results\n",
        "            clustered_multimodal_df[cluster_col_1], # For multimodal clustering results\n",
        "            clustered_multimodal_df[x_col_1], # For multimodal clustering results\n",
        "            clustered_multimodal_df[y_col_1] # For multimodal clustering results\n",
        "        )\n",
        "        .withColumn(\"batch_num\", F.lit(batch_num))\n",
        "    )\n",
        "    # Re-group errors to simplify processing on the UI\n",
        "    docs_df = docs_df.withColumn(errors_col, F.array(*[ \n",
        "        F.struct(F.when((docs_df[column].isNull() | (docs_df[column] == \"\")), F.lit(\"\")).otherwise(docs_df[column]).alias(\"message\"),\n",
        "        F.lit(column.replace(\"_error\", \"\")).alias(\"stage\")) \n",
        "        for column in error_cols]))\n",
        "    docs_df = docs_df.drop(*error_cols)\n",
        "\n",
        "with tracer.span(name='Persist processed text as SQL table'):\n",
        "\n",
        "    # The batch_num_document_enrichment_errors dataframe is used as a simplified query in the powerBI query\n",
        "    batch_num_document_enrichment_errors = docs_df.select(\"errors.stage\", \"errors.message\")\n",
        "    batch_num_document_enrichment_errors = batch_num_document_enrichment_errors.selectExpr('inline(arrays_zip(stage,message))') \\\n",
        "        .groupBy(\"stage\", \"message\").count() \\\n",
        "        .withColumnRenamed(\"stage\", \"errors.stage\") \\\n",
        "        .withColumnRenamed(\"message\", \"errors.message\") \\\n",
        "        .withColumnRenamed(\"count\", \"Count\")\n",
        "    batch_num_document_enrichment_errors_tbl_name = f'{batch_num}_document_enrichment_errors'\n",
        "    batch_num_document_enrichment_errors.write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{batch_num_document_enrichment_errors_tbl_name}')\n",
        "    \n",
        "    # Create the external table for batch_num_document_enrichment_errors to be used in powerBI\n",
        "    sql_command_document_enrichment_errors = f'''\n",
        "        IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{batch_num_document_enrichment_errors_tbl_name}') \n",
        "        CREATE EXTERNAL TABLE [{batch_num_document_enrichment_errors_tbl_name}] (\n",
        "            [errors.stage] nvarchar(1000), \n",
        "            [errors.message] nvarchar(1000),\n",
        "            [Count] bigint, \n",
        "        )\n",
        "        WITH (\n",
        "            LOCATION = 'minted_tables/{batch_num_document_enrichment_errors_tbl_name}/**', \n",
        "            DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], \n",
        "            FILE_FORMAT = [SynapseParquetFormat]\n",
        "        )\n",
        "    '''\n",
        "    # Table for batch_num processed_text\n",
        "    processed_text_tbl_name = f'{batch_num}_processed_text'\n",
        "    docs_df.write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{processed_text_tbl_name}')\n",
        "    \n",
        "    docs_df_sql_command = f\"\"\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{processed_text_tbl_name}') \n",
        "        CREATE EXTERNAL TABLE [{processed_text_tbl_name}] \n",
        "        (\n",
        "            [file_path] nvarchar(4000), \n",
        "            [file_name] nvarchar(4000), \n",
        "            [file_type] nvarchar(4000), \n",
        "            [text_content] nvarchar(4000), \n",
        "            [original_lang] nvarchar(4000),\n",
        "            [text_content_target_lang] nvarchar(4000),\n",
        "            [key_phrases] varchar(MAX),\n",
        "            [named_entities] varchar(MAX),\n",
        "            [pii] varchar(MAX),\n",
        "            [pii_redacted_text] nvarchar(4000),\n",
        "            [processed_text] nvarchar(4000),\n",
        "            [cluster] bigint,\n",
        "            [X] float,\n",
        "            [Y] float,\n",
        "            [summarized_text_xsum_1] nvarchar(4000),\n",
        "            [topic_name_1] nvarchar(4000),\n",
        "            [cluster_1] bigint,\n",
        "            [X_1] float,\n",
        "            [Y_1] float,\n",
        "            [batch_num] nvarchar(4000),\n",
        "            [errors] varchar(MAX)\n",
        "        ) WITH (\n",
        "                LOCATION = 'minted_tables/{processed_text_tbl_name}/**', \n",
        "                DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], \n",
        "                FILE_FORMAT = [SynapseParquetFormat]\n",
        "                )\"\"\"\n",
        "\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "      with conn.cursor() as cursor:\n",
        "        cursor.execute(docs_df_sql_command)\n",
        "        cursor.execute(sql_command_document_enrichment_errors)\n",
        "\n",
        "with tracer.span(name='Persist processed text as json'):\n",
        "    output_path = f'abfss://{output_container}@{blob_account_name}.dfs.{azure_storage_domain}/{batch_num}'\n",
        "\n",
        "    docs_df = docs_df \\\n",
        "        .withColumn(\"json\", F.to_json(F.struct(col(\"*\"))))\n",
        "\n",
        "    out_lst = docs_df.collect()\n",
        "\n",
        "    for row in out_lst:\n",
        "        json_path = f'{output_path}/text_processing_json/{row.file_name}.output.json'\n",
        "        mssparkutils.fs.put(json_path, row.json, overwrite=True)\n",
        "\n",
        "with tracer.span(name='Persist clustering results as CSV and a table'):\n",
        "    # clustered_df = docs_df.select(*clustering_cols)\n",
        "    # clustered_df = clustered_df \\\n",
        "    #     .withColumn(\"batch_num\", F.lit(batch_num))\n",
        "\n",
        "    # clustered_df_1 = docs_df.select(*clustering_cols)\n",
        "    clustered_df_1 = docs_df.select(col('file_path'), \\\n",
        "        col('processed_text'), \\\n",
        "        col('cluster'), \\\n",
        "        col('X'), \\\n",
        "        col('Y'), \\\n",
        "\t\tcol('original_lang'), \\\n",
        "\t\tcol('batch_num'))\n",
        "\t\n",
        "    # clustered_df_2 = docs_df.select(*multimodal_clustering_cols)\n",
        "    clustered_df_2 = docs_df.select(col('file_path'), \\\n",
        "        col(summarized_text_xsum_col_1).alias(summarized_text_xsum_col), \\\n",
        "\t\tcol(topic_name_col_1).alias(topic_name_col), \\\n",
        "        col(cluster_col_1).alias(cluster_col), \\\n",
        "        col(x_col_1).alias(x_col), \\\n",
        "        col(y_col_1).alias(y_col),\n",
        "        col('original_lang'), \\\n",
        "\t\tcol('batch_num'))\n",
        "\n",
        "    # Save the clustering results type 1 as a single\n",
        "    clustered_text_report_tbl_name_1 = f'{batch_num}_clustered_text_report_1'\n",
        "    clustered_df_1.write.mode(\"overwrite\").parquet(f\"{minted_tables_output_path}{clustered_text_report_tbl_name_1}\")\n",
        "    clustered_lst_1 = clustered_df_1.collect()\n",
        "\t\n",
        "    clustered_df_sql_command_1 = f\"\"\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{clustered_text_report_tbl_name_1}') \n",
        "    CREATE EXTERNAL TABLE [{clustered_text_report_tbl_name_1}] \n",
        "    (\n",
        "        [file_path] nvarchar(4000), \n",
        "        [processed_text] nvarchar(max), \n",
        "        [cluster] bigint,\n",
        "        [X] float,\n",
        "        [Y] float,\n",
        "        [original_lang] nvarchar(4000),\n",
        "        [batch_num] nvarchar(4000)\n",
        "    ) WITH (\n",
        "            LOCATION = 'minted_tables/{clustered_text_report_tbl_name_1}/**', \n",
        "            DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], \n",
        "            FILE_FORMAT = [SynapseParquetFormat]\n",
        "            )\"\"\"\n",
        "\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "      with conn.cursor() as cursor:\n",
        "        cursor.execute(clustered_df_sql_command_1)\n",
        "\n",
        "    # Output as CSV\n",
        "    # Saving to a 'local' file first as we can't save directly to the outut container via open()\n",
        "    with open('clusters.csv', 'w') as f:\n",
        "        write = csv.writer(f)\n",
        "        write.writerow(clustered_df_1.columns)\n",
        "        write.writerows(clustered_lst_1)\n",
        "\n",
        "    with open('clusters.csv', 'r') as f:\n",
        "        contents = f.read() \n",
        "        mssparkutils.fs.put(f'{output_path}/text_processing_clustering/clusters.csv', contents, overwrite=True)\n",
        "\n",
        "    # copy the manifest + config to output\n",
        "    mssparkutils.fs.cp(f'{batch_root}/{manifest_file_name}', f'{output_path}/manifest.txt')\n",
        "    mssparkutils.fs.cp(f'{batch_root}/config.json', output_path)\n",
        "\t\n",
        "\t# Save the clustering results type 2 as a single\n",
        "    clustered_text_report_tbl_name_2 = f'{batch_num}_clustered_text_report_2'\n",
        "    clustered_df_2.write.mode(\"overwrite\").parquet(f\"{minted_tables_output_path}{clustered_text_report_tbl_name_2}\")\n",
        "    clustered_lst_2 = clustered_df_2.collect()\n",
        "\t\n",
        "    clustered_df_sql_command_2 = f\"\"\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{clustered_text_report_tbl_name_2}') \n",
        "    CREATE EXTERNAL TABLE [{clustered_text_report_tbl_name_2}] \n",
        "    (\n",
        "        [file_path] nvarchar(4000), \n",
        "        [summarized_text_xsum] nvarchar(max), \n",
        "\t\t[topic_name] nvarchar(max), \n",
        "        [cluster] bigint,\n",
        "        [X] float,\n",
        "        [Y] float,\n",
        "        [original_lang] nvarchar(4000),\n",
        "        [batch_num] nvarchar(4000)\n",
        "    ) WITH (\n",
        "            LOCATION = 'minted_tables/{clustered_text_report_tbl_name_2}/**', \n",
        "            DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], \n",
        "            FILE_FORMAT = [SynapseParquetFormat]\n",
        "            )\"\"\"\n",
        "\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "      with conn.cursor() as cursor:\n",
        "        cursor.execute(clustered_df_sql_command_2)\n",
        "\n",
        "    # Output as CSV\n",
        "    # Saving to a 'local' file first as we can't save directly to the outut container via open()\n",
        "    with open('clusters.csv', 'w') as f:\n",
        "        write = csv.writer(f)\n",
        "        write.writerow(clustered_df_2.columns)\n",
        "        write.writerows(clustered_lst_2)\n",
        "\n",
        "    with open('clusters.csv', 'r') as f:\n",
        "        contents = f.read() \n",
        "        mssparkutils.fs.put(f'{output_path}/text_processing_clustering/clusters.csv', contents, overwrite=True)\n",
        "\n",
        "    # copy the manifest + config to output\n",
        "    mssparkutils.fs.cp(f'{batch_root}/{manifest_file_name}', f'{output_path}/manifest.txt')\n",
        "    mssparkutils.fs.cp(f'{batch_root}/config.json', output_path)\n",
        "\n",
        "    # # Save the clustering results as a single\n",
        "    # clustered_text_report_tbl_name = f'{batch_num}_clustered_text_report'\n",
        "    # clustered_df.write.mode(\"overwrite\").parquet(f\"{minted_tables_output_path}{clustered_text_report_tbl_name}\")\n",
        "    # clustered_lst = clustered_df.collect()\n",
        "    # clustered_df_sql_command = f\"\"\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{clustered_text_report_tbl_name}') \n",
        "    # CREATE EXTERNAL TABLE [{clustered_text_report_tbl_name}] \n",
        "    # (\n",
        "    #     [file_path] nvarchar(4000), \n",
        "    #     [processed_text] nvarchar(max), \n",
        "    #     [cluster] bigint,\n",
        "    #     [X] float,\n",
        "    #     [Y] float,\n",
        "    #     [original_lang] nvarchar(4000),\n",
        "    #     [batch_num] nvarchar(4000)\n",
        "    # ) WITH (\n",
        "    #         LOCATION = 'minted_tables/{clustered_text_report_tbl_name}/**', \n",
        "    #         DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], \n",
        "    #         FILE_FORMAT = [SynapseParquetFormat]\n",
        "    #         )\"\"\"\n",
        "\n",
        "    # with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "    #   with conn.cursor() as cursor:\n",
        "    #     cursor.execute(clustered_df_sql_command)\n",
        "\n",
        "    # # Output as CSV\n",
        "    # # Saving to a 'local' file first as we can't save directly to the outut container via open()\n",
        "    # with open('clusters.csv', 'w') as f:\n",
        "    #     write = csv.writer(f)\n",
        "    #     write.writerow(clustered_df.columns)\n",
        "    #     write.writerows(clustered_lst)\n",
        "\n",
        "    # with open('clusters.csv', 'r') as f:\n",
        "    #     contents = f.read() \n",
        "    #     mssparkutils.fs.put(f'{output_path}/text_processing_clustering/clusters.csv', contents, overwrite=True)\n",
        "\n",
        "    # # copy the manifest + config to output\n",
        "    # mssparkutils.fs.cp(f'{batch_root}/{manifest_file_name}', output_path)\n",
        "    # mssparkutils.fs.cp(f'{batch_root}/config.json', output_path)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql.functions import explode, count, countDistinct, udf, row_number\r\n",
        "from pyspark.sql.types import FloatType, StringType, IntegerType, StructType, StructField\r\n",
        "import math\r\n",
        "from pyspark.sql.window import Window\r\n",
        "\r\n",
        "max_num_terms_to_retain = 5\r\n",
        "\r\n",
        "with tracer.span(name=f'Calculate TF-IDF from the given table {processed_text_tbl_name}'):\r\n",
        "    small_docs_df = docs_df.select(col('cluster'), col('key_phrases')).na.drop('any')\r\n",
        "    small_docs_df = small_docs_df.withColumn('term', explode('key_phrases')).drop('key_phrases')\r\n",
        "\r\n",
        "    # Generate a table of term frequencies within each cluster\r\n",
        "    terms_with_tf = small_docs_df.groupBy('cluster', 'term').agg(count('cluster').alias('tf'))\r\n",
        "\r\n",
        "    # Calculate the number of clusters in which each term appears, and then take\r\n",
        "    # the (slightly modified) inverse of that to generate the \"IDF\" table\r\n",
        "    num_clusters = small_docs_df.select(countDistinct('cluster')).collect()[0][0]\r\n",
        "    calc_idf_udf = udf(lambda x: math.log((num_clusters + 1) / (x + 1)), FloatType())\r\n",
        "    terms_with_idf = small_docs_df.groupBy('term').agg(countDistinct('cluster').alias('df')) \\\r\n",
        "        .withColumn('idf', calc_idf_udf(col('df'))).drop('df')\r\n",
        "\r\n",
        "    # Multiply term frequency by inverse document frequency to get TF-IDF\r\n",
        "    tf_idf_df = terms_with_tf.join(terms_with_idf, on='term', how='left') \\\r\n",
        "        .withColumn('tf_idf', col('tf') * col('idf')).drop('tf', 'idf')\r\n",
        "\r\n",
        "with tracer.span(name='Concatenate the top TF-IDF terms for each cluster to form a label'):\r\n",
        "    # Retain the top terms for each cluster\r\n",
        "    windower = Window.partitionBy('cluster').orderBy(col('tf_idf').desc())\r\n",
        "    pd_df = tf_idf_df.withColumn('row', row_number().over(windower)) \\\r\n",
        "        .filter(col('row') <= max_num_terms_to_retain).drop('tf_idf').toPandas()\r\n",
        "    \r\n",
        "\r\n",
        "    # Concatenate the top terms for each cluster as a comma-separated string\r\n",
        "    pd_df = pd_df.groupby('cluster').apply(lambda x: ', '.join(x.sort_values(by='row', ascending=True)['term'])).reset_index()\r\n",
        "    if pd_df.empty:\r\n",
        "        logger.info(\"DataFrame is empty\")\r\n",
        "        pd_df = pd.DataFrame([], columns=['cluster', 'label'])\r\n",
        "    else:\r\n",
        "        pd_df.columns = ['cluster', 'label']\r\n",
        "\r\n",
        "with tracer.span(name='Persist cluster labels as table'):\r\n",
        "    cluster_labels_df_schema = StructType([\r\n",
        "        StructField('cluster', IntegerType(), True),\r\n",
        "        StructField('label', StringType(), True)])\r\n",
        "    cluster_labels_df = spark.createDataFrame(pd_df, schema=cluster_labels_df_schema)\r\n",
        "\r\n",
        "    cluster_labels_tbl_name = f'{batch_num}_cluster_labels'\r\n",
        "    cluster_labels_df.write.mode(\"overwrite\").parquet(f\"{minted_tables_output_path}{cluster_labels_tbl_name}\")\r\n",
        "\r\n",
        "    cluster_labels_sql_command = f\"\"\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{cluster_labels_tbl_name}') \r\n",
        "    CREATE EXTERNAL TABLE [{cluster_labels_tbl_name}] \r\n",
        "    (\r\n",
        "        [cluster] bigint,\r\n",
        "        [label] nvarchar(4000)\r\n",
        "    ) WITH (\r\n",
        "            LOCATION = 'minted_tables/{cluster_labels_tbl_name}/**', \r\n",
        "            DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], \r\n",
        "            FILE_FORMAT = [SynapseParquetFormat]\r\n",
        "            )\"\"\"\r\n",
        "\r\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\r\n",
        "        with conn.cursor() as cursor:\r\n",
        "            cursor.execute(cluster_labels_sql_command)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Generate a table for the top words for each cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# clustered_df includes the main table with processed text\r\n",
        "# cluster_labels_df includes the sluster labels\r\n",
        "\r\n",
        "# Join the clustered_df and cluster_labels_df by cluster_col\r\n",
        "top_wrods_df = (clustered_df_1\r\n",
        ".join(cluster_labels_df, [clustered_df_1[cluster_col] == cluster_labels_df[cluster_col]],'left_outer')\r\n",
        ".select(\r\n",
        "    clustered_df_1.processed_text,\r\n",
        "    clustered_df_1.cluster,\r\n",
        "    cluster_labels_df.label\r\n",
        "    )\r\n",
        ")\r\n",
        "\r\n",
        "# Drop undefined or null rows\r\n",
        "top_wrods_df = top_wrods_df.na.drop()\r\n",
        "\r\n",
        "# Group by cluster and concatenate all the processed_text\r\n",
        "top_wrods_df = top_wrods_df.groupBy(\"cluster\") \\\r\n",
        "  .agg(\r\n",
        "      F.concat_ws(' ', F.collect_list(\"processed_text\")).alias(\"words\")\r\n",
        "   )\r\n",
        "# Expand the concatenated processed_text to individual words per row\r\n",
        "top_wrods_df = top_wrods_df.withColumn('words',explode(F.split('words',' ')))\r\n",
        "\r\n",
        "# Group by cluster and row\r\n",
        "top_wrods_df = top_wrods_df.groupBy(\"cluster\",\"words\").count()\r\n",
        "\r\n",
        "# Rank the words by count and order by descending \r\n",
        "my_window = Window.partitionBy(\"cluster\").orderBy(F.col(\"count\").desc())\r\n",
        "\r\n",
        "# Select top 50 words per cluster\r\n",
        "top_wrods_df=top_wrods_df.withColumn(\"row_rank_by_cluster\",F.row_number().over(my_window))\\\r\n",
        ".filter(col('row_rank_by_cluster') <= 50).drop(\"row_rank_by_cluster\")\r\n",
        "\r\n",
        "cluster_top_words_tbl_name = f'{batch_num}_cluster_top_words'\r\n",
        "top_wrods_df.write.mode(\"overwrite\").parquet(f\"{minted_tables_output_path}{cluster_top_words_tbl_name}\")\r\n",
        "\r\n",
        "cluster_top_words_sql_command = f\"\"\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{cluster_top_words_tbl_name}') \r\n",
        "CREATE EXTERNAL TABLE [{cluster_top_words_tbl_name}] \r\n",
        "(\r\n",
        "    [cluster] bigint,\r\n",
        "    [words] nvarchar(4000),\r\n",
        "    [count] bigint\r\n",
        ") WITH (\r\n",
        "        LOCATION = 'minted_tables/{cluster_top_words_tbl_name}/**', \r\n",
        "        DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], \r\n",
        "        FILE_FORMAT = [SynapseParquetFormat]\r\n",
        "        )\"\"\"\r\n",
        "\r\n",
        "with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\r\n",
        "    with conn.cursor() as cursor:\r\n",
        "        cursor.execute(cluster_top_words_sql_command)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from time import sleep\r\n",
        "import datetime\r\n",
        "# Update Status Table\r\n",
        "def get_recent_status(batch_num, driver, dedicated_sql_endpoint, dedicated_database, sql_user_name, sql_user_pwd):\r\n",
        "    query = f\"\"\"\r\n",
        "        SELECT TOP (1) \r\n",
        "        [num_stages_complete], [description]\r\n",
        "        FROM [dbo].[batch_status] \r\n",
        "        WHERE [batch_id] = ? \r\n",
        "        ORDER BY [num_stages_complete] DESC;\r\n",
        "    \"\"\"\r\n",
        "    with pyodbc.connect(f'DRIVER={driver};SERVER=tcp:{dedicated_sql_endpoint};PORT=1433;DATABASE={dedicated_database};UID={sql_user_name};PWD={sql_user_pwd}',autocommit=True) as conn:\r\n",
        "        with conn.cursor() as cursor:\r\n",
        "            cursor.execute(query, batch_num)\r\n",
        "            num_stages_complete, description = cursor.fetchone()\r\n",
        "            return num_stages_complete, description\r\n",
        "\r\n",
        "def update_status_table(status_text, minted_tables_path, batch_num, driver, dedicated_sql_endpoint, sql_user_name, sql_user_pwd):\r\n",
        "    retries = 0 \r\n",
        "    exc = ''\r\n",
        "    while retries < 10:\r\n",
        "        try:\r\n",
        "            stages_complete, description = get_recent_status(batch_num, driver, dedicated_sql_endpoint, dedicated_database, sql_user_name, sql_user_pwd)\r\n",
        "            stages_complete += 1\r\n",
        "            status = f'[{stages_complete}/9] {status_text}'\r\n",
        "            x = datetime.datetime.now()\r\n",
        "            time_stamp = x.strftime(\"%Y-%m-%d %H:%M:%S\")\r\n",
        "\r\n",
        "            sql_command = f\"UPDATE batch_status SET status = ?, update_time_stamp = ?, num_stages_complete = ? WHERE batch_id = ?\"\r\n",
        "            with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+dedicated_sql_endpoint+';PORT=1433;DATABASE='+dedicated_database+';UID='+sql_user_name+';PWD='+ sql_user_pwd+'',autocommit=True) as conn:\r\n",
        "                with conn.cursor() as cursor:\r\n",
        "                    cursor.execute(sql_command, status, time_stamp, stages_complete, batch_num)\r\n",
        "                    cursor.commit()\r\n",
        "            return \r\n",
        "        except Exception as e:\r\n",
        "            exc_str = str(e)\r\n",
        "            exc = e \r\n",
        "            logger.warning(f'Failed to update status table: {exc_str}, retrying . . .')\r\n",
        "            retries += 1\r\n",
        "            sleep(3)\r\n",
        "\r\n",
        "    raise exc\r\n",
        "\r\n",
        "update_status_table('Text Analysis Complete', minted_tables_output_path, batch_num, driver, dedicated_sql_endpoint, sql_user_name, sql_user_pwd)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# return name of new table\n",
        "output = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'processed_text_tbl_name': processed_text_tbl_name,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "\n",
        "# Return the object to the pipeline\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\n",
        "mssparkutils.notebook.exit(output['custom_dimensions'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
