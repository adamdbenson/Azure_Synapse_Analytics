{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "     \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "     \"spark.dynamicAllocation.enabled\": true,\n",
        "     \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "     \"spark.dynamicAllocation.maxExecutors\": 8\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "# This is a parameters cell where we define the batch_file details as params to be passed in by the pipeline\n",
        "log_level = ''\n",
        "batch_num = ''\n",
        "batch_root = ''\n",
        "output_container = ''\n",
        "file_system = ''\n",
        "json_path = ''\n",
        "video_id = ''\n",
        "state = ''\n",
        "media_path = ''\n",
        "media_file_name = ''\n",
        "batch_description = ''\n",
        "azure_storage_domain = ''\n",
        "blob_account_name = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from azure.identity import ClientSecretCredential\n",
        "from azure.mgmt.keyvault import KeyVaultManagementClient\n",
        "from pyspark.sql import SparkSession\n",
        "import json\n",
        "from types import SimpleNamespace\n",
        "from requests.structures import CaseInsensitiveDict\n",
        "import requests as req\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "config_integration.trace_integrations(['logging'])\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "media_contents_tbl_name = f'{batch_num}_submitted_media'\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'file_system': file_system,\n",
        "    'media_contents_tbl_name': media_contents_tbl_name,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Load keys, set defaults\n",
        "with tracer.span(name='load values from key vault'):\n",
        "    instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "    subscription_id = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SubscriptionId\")\n",
        "    resource_group_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"ResourceGroupName\")\n",
        "    subscription_id = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SubscriptionId\")\n",
        "    tenant_id = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"TenantID\")\n",
        "    client_id = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"ADAppRegClientId\")\n",
        "    client_secret = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"ADAppRegClientSecret\")\n",
        "    storage_account_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"StorageAccountName\")\n",
        "    storage_account_key = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"StorageAccountKey\")\n",
        "    vi_account_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"VideoIndexerAccountName\")\n",
        "    apiUrl = \"<<TF_VAR_azure_avam_api_domain>>\" #api's are documented here... https://api-portal.videoindexer.ai/\n",
        "\n",
        "    azure_resource_manager = \"<<TF_VAR_azure_arm_management_api>>\";\n",
        "    credential = ClientSecretCredential(tenant_id, client_id, client_secret)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initialise session and config\n",
        "sc = spark.sparkContext\n",
        "spark = SparkSession.builder.appName(f\"TextProcessing {mssparkutils.runtime.context}\").getOrCreate()\n",
        "\n",
        "def read_batch_config(batch_root: str):\n",
        "    \"\"\"\n",
        "    We read the config file using the Java File System API as we do not need to let multiple nodes read individual lines and join it\n",
        "    all back together again\n",
        "    \"\"\"\n",
        "    # Change our file system from 'synapse' to 'input'\n",
        "    sc._jsc.hadoopConfiguration().set(\"fs.defaultFS\", file_system)\n",
        "\n",
        "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
        "    config_path = sc._jvm.org.apache.hadoop.fs.Path(f'{batch_root}/config.json')\n",
        "\n",
        "    # If we don't have a batch config, copy the global one.\n",
        "    if fs.exists(config_path) != True:\n",
        "        logger.error(f'{config_path} not found.')\n",
        "\n",
        "    # Open our file directly rather than through spark\n",
        "    input_stream = fs.open(config_path)  # FSDataInputStream\n",
        "\n",
        "    config_string = sc._jvm.java.io.BufferedReader(\n",
        "        sc._jvm.java.io.InputStreamReader(input_stream, sc._jvm.java.nio.charset.StandardCharsets.UTF_8)\n",
        "        ).lines().collect(sc._jvm.java.util.stream.Collectors.joining(\"\\n\"))\n",
        "\n",
        "    # Load it into json    \n",
        "    return json.loads(''.join(config_string), object_hook=lambda dictionary: SimpleNamespace(**dictionary))\n",
        "\n",
        "with tracer.span(name=f\"Load config: {mssparkutils.runtime.context['notebookname']}\"):\n",
        "    try:\n",
        "        config = read_batch_config(batch_root)\n",
        "    except Exception as e:\n",
        "        logger.exception(e)\n",
        "        raise e\n",
        "\n",
        "    # Set log level\n",
        "    if config.log_level == \"INFO\":\n",
        "        logger.setLevel(logging.INFO)\n",
        "    else:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "        config.log_level = \"ERROR\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyodbc\r\n",
        "# serverless SQL config\r\n",
        "database = 'minted'   \r\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\r\n",
        "minted_tables_path = f'abfss://synapse@{blob_account_name}.dfs.{azure_storage_domain}/minted_tables/'\r\n",
        "\r\n",
        "# secrets\r\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\r\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\r\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseServerlessSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Retrieve access tokens and retrive data from VI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Get ARM access token (bearer token)'):\n",
        "    # Get ARM access token (bearer token)\n",
        "    token_context = \"<<TF_VAR_azure_arm_management_api>>/.default\"\n",
        "    arm_token = credential.get_token(token_context).token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Get VI account details'):\n",
        "    api_version = config.video_indexer_api_version\n",
        "    # Get Account information\n",
        "    # these top level API's are documented and testable here... https://docs.microsoft.com/en-us/rest/api/videoindexer/accounts/list\n",
        "    request_url = f'{azure_resource_manager}/subscriptions/{subscription_id}/resourcegroups/{resource_group_name}/providers/Microsoft.VideoIndexer/accounts/{vi_account_name}/?api-version={api_version}'\n",
        "    headers = CaseInsensitiveDict()\n",
        "    headers[\"Accept\"] = \"application/json\"\n",
        "    headers[\"Authorization\"] = \"Bearer \" + arm_token\n",
        "    response = req.get(request_url, headers=headers)\n",
        "    response = response.json()\n",
        "    vi_account_id = response['properties']['accountId']\n",
        "    vi_account_location = response['location']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Get account level access token for Azure Video Analyzer for Media'):\n",
        "    # Get account level access token for Azure Video Analyzer for Media \n",
        "    request_url = f'{azure_resource_manager}/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.VideoIndexer/accounts/{vi_account_name}/generateAccessToken?api-version={api_version}'\n",
        "    headers = CaseInsensitiveDict()\n",
        "    headers[\"Accept\"] = \"application/json\"\n",
        "    headers[\"Authorization\"] = \"Bearer \" + arm_token\n",
        "    body = '{\"permissionType\":\"Contributor\",\"scope\":\"Account\",\"projectId\":null,\"videoId\":null}'\n",
        "    body = json.loads(body)\n",
        "    response = req.post(request_url, headers=headers, json=body)\n",
        "    response = response.json()\n",
        "    account_access_token = response[\"accessToken\"] "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Retrieve media enrichments'):\n",
        "    # Retrieve media enrichments\n",
        "    params = CaseInsensitiveDict()\n",
        "    params[\"accessToken\"] = account_access_token\n",
        "    headers = CaseInsensitiveDict()\n",
        "    request_url = f'{apiUrl}/{vi_account_location}/Accounts/{vi_account_id}/Videos/{video_id}/Index'\n",
        "    response = req.get(request_url, headers=headers, params=params)\n",
        "    response_json = response.json()\n",
        "    response_str = json.dumps(response_json)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# write output to a df\n",
        "columns = [\"media_path\", \"media_file_name\", \"video_id\", \"enrichments\", \"original_lang\"]\n",
        "data = [[media_path, media_file_name, video_id, response_str, response_json[\"videos\"][0][\"insights\"][\"sourceLanguage\"] ]]\n",
        "df_enriched_media = spark.createDataFrame(data, columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Persist Enrichments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Persist processed text as json'):\n",
        "    submitted_media_tbl_name = f'{batch_num}_submitted_media'\n",
        "    df_enriched_media_sql = spark.read.parquet(f'{minted_tables_path}{submitted_media_tbl_name}')\n",
        "    df_enriched_media_sql = df_enriched_media_sql.where(df_enriched_media_sql.path == media_path)\n",
        "    file_type = df_enriched_media_sql.first()['file_type']\n",
        "    media_output_dict = {\n",
        "        'file_path': media_path, \n",
        "        'file_name': media_file_name, \n",
        "        'file_type': file_type, \n",
        "        'batch_num': batch_num, \n",
        "        'media_enrichment': json.loads(response_str)\n",
        "    }\n",
        "    media_output_json = json.dumps(media_output_dict, indent=4)\n",
        "    mssparkutils.fs.put(json_path, media_output_json, overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Perist media enrichments to sql'):  \n",
        "    # set the sql table name\n",
        "    enriched_media_tbl_name = f'{batch_num}_enriched_media'\n",
        "    enriched_media_tbl_name = enriched_media_tbl_name.replace(\"/\", \"_\")\n",
        "    enriched_media_tbl_name = enriched_media_tbl_name.replace(\".\", \"_\")\n",
        "\n",
        "    df_enriched_media.write.mode(\"append\").parquet(f'{minted_tables_path}{enriched_media_tbl_name}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# output and exit\n",
        "output = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'enriched_media_tbl_name': enriched_media_tbl_name,\n",
        "    'file_system': file_system,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "\n",
        "# Return the object to the pipeline\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\n",
        "mssparkutils.notebook.exit(output['custom_dimensions'])\n",
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
