{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Dataset Prep\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "     \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "     \"spark.dynamicAllocation.enabled\": true,\n",
        "     \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "     \"spark.dynamicAllocation.maxExecutors\": 8\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "# This is a parameters cell where we define the batch_file details as params to be passed in by the pipeline\n",
        "manifest_file_path=''\n",
        "manifest_container=''\n",
        "blob_account_name = ''\n",
        "global_config_location = ''\n",
        "azure_storage_domain = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import os\n",
        "import json\n",
        "from types import SimpleNamespace\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import regexp_extract, regexp_replace, concat, lit, col, current_timestamp\n",
        "from pyspark.sql.types import StringType, TimestampType, StructType, StructField, IntegerType\n",
        "from py4j.protocol import Py4JJavaError\n",
        "import re\n",
        "import pyodbc\n",
        "from pathlib import Path\n",
        "\n",
        "# Initialise paths and batch root\n",
        "batch_path = f'abfss://{manifest_container}@{blob_account_name}.dfs.{azure_storage_domain}'\n",
        "batch_folder = os.path.dirname(manifest_file_path)\n",
        "batch_root = f'{batch_path}{batch_folder}'\n",
        "manifest_full_path = f'{batch_path}{manifest_file_path}'\n",
        "manifest_file_name = Path(manifest_file_path).name\n",
        "# Dedicated and serverless SQL config\n",
        "dedicated_database = 'dedicated'\n",
        "database = 'minted'\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\n",
        "output_path = f'abfss://synapse@{blob_account_name}.dfs.{azure_storage_domain}/minted_tables/'\n",
        "\n",
        "# Load secrets\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseServerlessSQLEndpoint\")\n",
        "dedicated_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseDedicatedSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "import datetime\n",
        "\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "  'batch_path': batch_path,\n",
        "  'batch_folder': batch_folder,\n",
        "  'batch_root': batch_root,\n",
        "  'manifest_full_path': manifest_full_path,\n",
        "  'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f\"Load config: {mssparkutils.runtime.context['notebookname']}\"):\n",
        "    # Initialise session, create (if necessary) and read batch config\n",
        "    sc = spark.sparkContext\n",
        "    spark = SparkSession.builder.appName(f\"TextProcessing {mssparkutils.runtime.context}\").getOrCreate()\n",
        "\n",
        "    def copy_global_config(config_path: str, global_config_path: str):\n",
        "        \"\"\"\n",
        "        This method makes sure that a config is availabile in the batch root.\n",
        "        If a config file isn't already there, it is copied over form global_config_path.\n",
        "        If there is no config under global_config_path, this function will crash (indicating an error in pipeline set up.)\n",
        "        \"\"\"\n",
        "        logger.info(\"Loading global config\")\n",
        "        try:\n",
        "            mssparkutils.fs.cp(global_config_path, config_path)    \n",
        "        except Py4JJavaError as e:\n",
        "            logger.exception(e)\n",
        "            raise e\n",
        "\n",
        "    def read_batch_config(batch_root: str, global_config_path: str):\n",
        "        \"\"\"\n",
        "        We read the config file using the Java File System API as we do not need to let multiple nodes read individual lines and join it\n",
        "        all back together again\n",
        "        \"\"\"\n",
        "        # Change our file system from 'synapse' to 'input'\n",
        "        sc._jsc.hadoopConfiguration().set(\"fs.defaultFS\", batch_path)\n",
        "\n",
        "        fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
        "        config_path = sc._jvm.org.apache.hadoop.fs.Path(f'{batch_root}/config.json')\n",
        "\n",
        "        # If we don't have a batch config, copy the global one.\n",
        "        if fs.exists(config_path) != True:\n",
        "            copy_global_config(f'{batch_root}/config.json', global_config_path)\n",
        "\n",
        "        # Open our file directly rather than through spark\n",
        "        input_stream = fs.open(config_path)  # FSDataInputStream\n",
        "\n",
        "        config_string = sc._jvm.java.io.BufferedReader(\n",
        "            sc._jvm.java.io.InputStreamReader(input_stream, sc._jvm.java.nio.charset.StandardCharsets.UTF_8)\n",
        "            ).lines().collect(sc._jvm.java.util.stream.Collectors.joining(\"\\n\"))\n",
        "\n",
        "        # # Load it into json    \n",
        "        return json.loads(''.join(config_string), object_hook=lambda dictionary: SimpleNamespace(**dictionary))\n",
        "\n",
        "    # NOTE: this path should be in sync with Terraform configuration which uploads this file\n",
        "    config = read_batch_config(batch_root, global_config_path=f'abfss://configuration@{blob_account_name}.dfs.{azure_storage_domain}/config.global.json')\n",
        "\n",
        "    # Set log level\n",
        "    if config.log_level == \"INFO\":\n",
        "        logger.setLevel(logging.INFO)\n",
        "    else:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "        config.log_level = \"ERROR\"    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from time import sleep\r\n",
        "def update_status_table(batch_num, description):\r\n",
        "    retries = 0\r\n",
        "    exc = ''\r\n",
        "    while retries < 10:\r\n",
        "        try:\r\n",
        "            x = datetime.datetime.now()\r\n",
        "            time_stamp = x.strftime(\"%Y-%m-%d %H:%M:%S\")\r\n",
        "            \r\n",
        "            sql_command = (\r\n",
        "                f\"IF NOT EXISTS (SELECT * FROM batch_status WHERE batch_id = ?) INSERT INTO batch_status (batch_id, date_submitted, description, status, update_time_stamp, num_stages_complete) VALUES (?, ?, ?, '[1/10] Pipeline Started', ?, 1)\"\r\n",
        "                 \"ELSE UPDATE batch_status SET description = ?, status = '[1/10] Pipeline Started', update_time_stamp = ?, num_stages_complete = 1\"\r\n",
        "            )\r\n",
        "            with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+dedicated_sql_endpoint+';PORT=1433;DATABASE='+dedicated_database+';UID='+sql_user_name+';PWD='+ sql_user_pwd+'',autocommit=True) as conn:\r\n",
        "                with conn.cursor() as cursor:\r\n",
        "                    cursor.execute(sql_command, batch_num, batch_num, time_stamp, description, time_stamp, description, time_stamp)\r\n",
        "                    cursor.commit()\r\n",
        "            return \r\n",
        "        except Exception as e:\r\n",
        "            exc_str = str(e)\r\n",
        "            exc = e\r\n",
        "            logger.warning(f'Failed to update status table: {exc_str}, retrying . . .')\r\n",
        "            retries += 1\r\n",
        "            sleep(3)\r\n",
        "\r\n",
        "    raise exc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "tags": []
      },
      "source": [
        "try:\n",
        "  with tracer.span(name='Convert manifest to tables'):\n",
        "    # read the manifest rows into a dataframe\n",
        "    docs_columns = [\"file_path\", \"file_extension\"]\n",
        "    df = spark.read.text(manifest_full_path)\n",
        "    headers = df.head(2)\n",
        "    batch_num = headers[0].asDict()['value']\n",
        "    description = headers[1].asDict()['value']\n",
        "\n",
        "    '''\n",
        "    Sanitise batch_num to letters, numbers and _ \n",
        "    Also converts to lower case, because power BI is case sensitive and synapse/azure storage forces \n",
        "    everything\n",
        "    '''\n",
        "    batch_num = \"\".join(re.findall('[a-z]|[A-Z]|[0-9]|_', batch_num)).lower()\n",
        "\n",
        "    # separate the file extension into a new column\n",
        "    df = df.withColumnRenamed(\"value\", \"file_name\") \\\n",
        "      .withColumn(\"file_type\", regexp_replace(regexp_extract(\"file_name\", \"\\.[0-9a-zA-Z]+$\", 0), \"\\.\", \"\")) \\\n",
        "      .withColumn(\"file_path\", concat(lit(batch_root + \"/\"), col(\"file_name\")))\n",
        "\n",
        "    #save the contents of the manifest file for use in Power BI summary pages\n",
        "    batch_df = df.withColumn('batch_num', lit(f'{batch_num}')).withColumn('batch_desc',lit(f'{description}')).filter((col('file_type').isNotNull()) & (col('file_type') != ''))\n",
        "    batch_df_name = f'{batch_num}_manifest'\n",
        "\n",
        "    batch_status_schema = StructType([\n",
        "        StructField(\"batch_id\", StringType(), True),\n",
        "        StructField(\"date_submitted\", TimestampType(), True),\n",
        "        StructField(\"description\", StringType(), True),\n",
        "        StructField(\"status\", StringType(), True),\n",
        "        StructField(\"update_time_stamp\", TimestampType(), True),\n",
        "        StructField(\"num_stages_complete\", IntegerType(), True)])\n",
        "\n",
        "    batch_status_df = spark.createDataFrame([], StructType([]))\n",
        "    print(\"Path: \" + f'{output_path}batch_status/')\n",
        "    try:\n",
        "        batch_status_df = spark.read.format(\"parquet\").schema(batch_status_schema).load(f'{output_path}batch_status/')\n",
        "    except:\n",
        "        emptyRDD = spark.sparkContext.emptyRDD()\n",
        "        batch_status_df = spark.createDataFrame(emptyRDD,batch_status_schema)\n",
        "        batch_status_df.write.parquet(f'{output_path}batch_status')\n",
        "    \n",
        "    update_status_table(batch_num, description)\n",
        "\n",
        "    # select rows into new dataframes, per file type\n",
        "    doc_file_types = [\"txt\", \"TXT\", \"docx\", \"DOCX\", \"doc\", \"DOC\", \"pdf\", \"PDF\", \"pptx\", \"PPTX\", \"ppt\", \"PPT\", \"html\", \"HTML\", \"htm\", \"HTM\", \"json\", \"JSON\"]\n",
        "    docs_df = df.where(df.file_type.isin(doc_file_types))\n",
        "    docs_df_name = f'{batch_num}_documents'\n",
        "\n",
        "    img_file_types = [\"jpg\", \"JPG\", \"jpeg\", \"JPEG\", \"png\", \"PNG\", \"gif\", \"GIF\", \"bmp\", \"BMP\", \"tif\", \"TIF\"]\n",
        "    img_df = df.where(df.file_type.isin(img_file_types))\n",
        "    img_df_name = f'{batch_num}_images'\n",
        "\n",
        "    media_file_types = [\"avi\", \"AVI\", \"mp4\", \"MP4\", \"mp3\", \"MP3\", \"mpg\", \"MPG\", \"wmv\", \"WMV\", \"wav\", \"WAV\", \"mov\", \"MOV\"]\n",
        "    media_df = df.where(df.file_type.isin(media_file_types))\n",
        "    media_df_name = f'{batch_num}_media'\n",
        "\n",
        "    # get count of files in manifest (pre-processed)\n",
        "    batch_file_count = batch_df.count()\n",
        "    media_file_count = media_df.count()\n",
        "    image_file_count = img_df.count()\n",
        "    text_file_count = docs_df.count()\n",
        "\n",
        "    # persist new dataframes as tables\n",
        "    batch_df.write.mode(\"overwrite\").parquet(f'{output_path}{batch_df_name}')\n",
        "    docs_df.write.mode(\"overwrite\").parquet(f'{output_path}{docs_df_name}')\n",
        "    img_df.write.mode(\"overwrite\").parquet(f'{output_path}{img_df_name}')\n",
        "    media_df.write.mode(\"overwrite\").parquet(f'{output_path}{media_df_name}')\n",
        "  \n",
        "    # create remote sql tables over the parquet files\n",
        "    batch_df_sql_command = f\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{batch_df_name}') CREATE EXTERNAL TABLE [{batch_df_name}] ([file_name] nvarchar(4000), [file_type] nvarchar(4000), [file_path] nvarchar(4000), [batch_num] nvarchar(4000), [batch_desc] nvarchar(4000)) WITH (LOCATION = 'minted_tables/{batch_df_name}/**', DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "    docs_df_sql_command = f\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{docs_df_name}') CREATE EXTERNAL TABLE [{docs_df_name}] ([file_name] nvarchar(4000), [file_type] nvarchar(4000), [file_path] nvarchar(4000)) WITH (LOCATION = 'minted_tables/{docs_df_name}/**', DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "    img_df_sql_command = f\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{img_df_name}') CREATE EXTERNAL TABLE [{img_df_name}] ([file_name] nvarchar(4000), [file_type] nvarchar(4000), [file_path] nvarchar(4000)) WITH (LOCATION = 'minted_tables/{img_df_name}/**', DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "    media_df_sql_command = f\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{media_df_name}') CREATE EXTERNAL TABLE [{media_df_name}] ([file_name] nvarchar(4000), [file_type] nvarchar(4000), [file_path] nvarchar(4000)) WITH (LOCATION = 'minted_tables/{media_df_name}/**', DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "    \n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "      with conn.cursor() as cursor:\n",
        "        cursor.execute(batch_df_sql_command)\n",
        "        cursor.execute(docs_df_sql_command)\n",
        "        cursor.execute(img_df_sql_command)\n",
        "        cursor.execute(media_df_sql_command)\n",
        "\n",
        "    # return json block with names of tables\n",
        "    output = {'custom_dimensions': {\n",
        "        'batch_tbl_name': batch_df_name,\n",
        "        'documents_tbl_name': docs_df_name,\n",
        "        'images_tbl_name': img_df_name,\n",
        "        'media_tbl_name': media_df_name,\n",
        "        'batch_num': batch_num,\n",
        "        'batch_description': description,\n",
        "        'batch_root': batch_root,\n",
        "        'manifest_file_name': manifest_file_name,\n",
        "        'file_system': batch_path,\n",
        "        'notebook_name': mssparkutils.runtime.context['notebookname'],\n",
        "        'batch_file_count': batch_file_count,\n",
        "        'media_file_count': media_file_count,\n",
        "        'image_file_count': image_file_count,\n",
        "        'text_file_count': text_file_count,\n",
        "        'blob_account_name': blob_account_name,\n",
        "        'minted_tables_output_path': output_path,\n",
        "        'instrumentation_connection_string': instrumentation_connection_string\n",
        "    } }\n",
        "\n",
        "except Exception as e:\n",
        "  logger.exception(e)\n",
        "  raise e\n",
        "\n",
        "# return the object to the pipeline\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\n",
        "mssparkutils.notebook.exit(output['custom_dimensions'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
