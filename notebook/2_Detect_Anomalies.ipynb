{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "    \"conf\": {\n",
        "        \"spark.jars.packages\": \"com.databricks:spark-xml_2.12:0.14.0\",\n",
        "        \"spark.jars.repositories\": \"https://repo1.maven.org/maven2/\"\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "azure_storage_domain = ''\n",
        "input_container = ''\n",
        "blob_account_name = ''\n",
        "image_file_path = ''\n",
        "output_container = ''\n",
        "key_vault_name = ''\n",
        "config_path = ''\n",
        "kml_path = ''\n",
        "output_path = ''\n",
        "ais_image = ''\n",
        "anomaly_image = ''\n",
        "ship_bb_image_high_res = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "import base64\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# NOTE: this path should be in sync with Terraform configuration which uploads this file\n",
        "global_config_path = f'abfss://configuration@{blob_account_name}.dfs.{azure_storage_domain}/anomdet.config.global.json'\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'input_container': input_container,\n",
        "    'image_file_path': image_file_path,\n",
        "    'blob_account_name': blob_account_name,\n",
        "    'global_config_path': global_config_path,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        " \n",
        "logger.info(f\"INITIALISED: {mssparkutils.runtime.context['notebookname']}\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import os, io, sys, math\n",
        "import json\n",
        "import glob\n",
        "import logging\n",
        "import requests\n",
        "import copy\n",
        "\n",
        "from requests.exceptions import HTTPError\n",
        "from pathlib import Path\n",
        "from pyspark.sql import SparkSession\n",
        "from py4j.protocol import Py4JJavaError\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from azure.storage.blob import BlobServiceClient, ContainerClient, generate_account_sas, generate_container_sas, generate_blob_sas, ResourceTypes\n",
        "from azure.storage.blob import AccountSasPermissions, ContainerSasPermissions, BlobSasPermissions\n",
        "from azure.identity import ClientSecretCredential\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initialise paths\n",
        "image_path = f'https://{blob_account_name}.blob.{azure_storage_domain}/{input_container}/'\n",
        "image_path_abfss = f'abfss://{input_container}@{blob_account_name}.dfs.{azure_storage_domain}/'\n",
        "image_folder = os.path.dirname(image_file_path)\n",
        "image_root = f\"{image_path}{image_folder}\"\n",
        "image_root_abfss = f'{image_path_abfss}{image_folder}'\n",
        "image_full_path = f\"{image_path}{image_file_path}\"\n",
        "output_dir = f'https://{blob_account_name}.blob.{azure_storage_domain}/{output_container}/{output_path}'\n",
        "output_dir_abfss = f'abfss://{output_container}@{blob_account_name}.dfs.{azure_storage_domain}/{output_path}'\n",
        "output_root = f'{output_dir}/{image_folder}'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f'Preparing config from global config and loading into memory'):\n",
        "    # Initialise session, create (if necessary) and read config\n",
        "    sc = spark.sparkContext\n",
        "    spark = SparkSession.builder.appName(f\"AnomalyDetection {mssparkutils.runtime.context}\").getOrCreate()\n",
        "\n",
        "    def prepare_config(image_root: str, global_config_path: str):\n",
        "        \"\"\"\n",
        "        This method makes sure that a config is availabile in the batch root.\n",
        "        If a config file isn't already there, it is copied over form global_config_path.\n",
        "        If there is no config under global_config_path, this function will crash (indicating an error in pipeline set up.)\n",
        "        \"\"\"\n",
        "        image_config_path = f'{image_root_abfss}/anomdet.config.json'\n",
        "        try: \n",
        "            mssparkutils.fs.head(image_config_path)\n",
        "        except Py4JJavaError as e:\n",
        "            if 'java.io.FileNotFoundException' in str(e):\n",
        "                # File doesn't exist, copying over the global config path\n",
        "                mssparkutils.fs.cp(global_config_path, image_config_path)    \n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "\n",
        "    prepare_config(image_root=image_root, global_config_path=global_config_path)\n",
        "\n",
        "    config = json.loads(''.join(sc.textFile(f'{image_root_abfss}/{config_path}').collect()))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f'Getting Credentials, creating BlobServiceClient and sas_token'):\n",
        "    tenant_id = mssparkutils.credentials.getSecretWithLS('keyvault', 'TenantID')\n",
        "    client_id = mssparkutils.credentials.getSecretWithLS('keyvault', 'ADAppRegClientId')\n",
        "    client_secret = mssparkutils.credentials.getSecretWithLS('keyvault', 'ADAppRegClientSecret')\n",
        "    storage_account_key = mssparkutils.credentials.getSecretWithLS('keyvault', 'StorageAccountKey')\n",
        "    credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n",
        "    service = BlobServiceClient(account_url=f'https://{blob_account_name}.blob.{azure_storage_domain}/', credential=credential)\n",
        "    sas_token = generate_account_sas(\n",
        "        account_name=f'{blob_account_name}',\n",
        "        account_key=f'{storage_account_key}',\n",
        "        resource_types=ResourceTypes(service=False,container=True, object=True),\n",
        "        permission=AccountSasPermissions(read=True, list=True, write=True, add=True, create=True, update=True),\n",
        "        expiry=datetime.utcnow() + timedelta(hours=1)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f'Preparing AIS to send off to GDAL server'):\n",
        "\n",
        "    import json\n",
        "    from pyspark.sql.functions import struct, col, split, to_timestamp, to_json, spark_partition_id, desc, count\n",
        "\n",
        "    src = f\"abfss://{output_container}@{blob_account_name}.dfs.{azure_storage_domain}/{kml_path}\"\n",
        "\n",
        "    try:\n",
        "        df = spark.read \\\n",
        "            .option(\"rootTag\", \"Document\") \\\n",
        "            .option(\"rowTag\", \"Placemark\") \\\n",
        "            .format(\"com.databricks.spark.xml\") \\\n",
        "            .load(src)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.info(f'Getting file from source: {src}: {e}')\n",
        "    else:\n",
        "        logger.info(f'created dataframe from source: {src}')\n",
        "\n",
        "    long_list_of_boats = df.withColumn(\"UTC\", to_timestamp(col('ExtendedData').Data[8].value, \"yyyy-MM-dd HH:mm:ss\")) \\\n",
        "                            .withColumn(\"MMSI\", col('ExtendedData').Data[1].value)\n",
        "    long_list_of_boats.createOrReplaceTempView(\"ship_profiles\")\n",
        "    \n",
        "    query = \"(SELECT * FROM ship_profiles WHERE UTC BETWEEN '2021-06-23 03:27:54' AND '2021-06-23 03:37:54')\"\n",
        "    boats_in_time = spark.sql(query)\n",
        "    boats_in_time.createOrReplaceTempView(\"ship_profiles_in_time\")\n",
        "    mmsi_agg_time_slice = boats_in_time.groupby(\"MMSI\").count()\n",
        "    mmsi_agg_time_slice.createOrReplaceTempView(\"MMSI_grouped\")\n",
        "    time_sliced_boats_agg = boats_in_time.join(mmsi_agg_time_slice, \"MMSI\", \"outer\")\n",
        "    time_sliced_boats_agg.printSchema()\n",
        "    time_sliced_boats_agg.select(count('MMSI')).show()\n",
        "    time_sliced_boats_agg.orderBy(desc(\"count\")).show()\n",
        "    ais_remote_dir_name = 'AIS_sliced_and_grouped'\n",
        "    try:\n",
        "        ret = time_sliced_boats_agg.coalesce(1).write.mode(\"Overwrite\").json(f'{output_dir_abfss}/{ais_remote_dir_name}')\n",
        "    except Exception as err:\n",
        "        logger.error(f'Other error occurred: {err}')\n",
        "    else:\n",
        "        logger.info(f'Success. Response: {ret}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Perform matching and get json-formatted information on anomalies + successful pairings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f'Send AIS and BB data to GDAL server to do anomaly detection'):\n",
        "    # Define a helper function for calling the endpoint\n",
        "    def call_detect_anomalies(gdal_endpoint,anom_det_meta,api_key):\n",
        "        resp = \"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                # Request headers\n",
        "                \"Content-Type\": \"application/json\",\n",
        "                \"Gdal-Subscription-Key\": api_key,\n",
        "                \"KEY\": api_key\n",
        "            }\n",
        "            body = anom_det_meta\n",
        "            url = f\"{gdal_endpoint}/anomaly_detection/\"\n",
        "            resp = requests.post(url=url, json=body, headers=headers)\n",
        "        except Exception as e:\n",
        "            logger.error('Exception', e)\n",
        "        return resp\n",
        "\n",
        "    # Get SAS keys for:\n",
        "    # - the input image\n",
        "    try:\n",
        "        img_sas_tkn = generate_blob_sas(account_name=blob_account_name, \n",
        "                                    container_name=input_container,\n",
        "                                    blob_name=image_file_path,\n",
        "                                    account_key=storage_account_key,\n",
        "                                    permission=BlobSasPermissions(read=True),\n",
        "                                    expiry=datetime.utcnow() + timedelta(hours=1))\n",
        "    except Exception as e:\n",
        "        logger.info(f\"unable to create remote bb json blob sas: {e} \")\n",
        "    else:\n",
        "        logger.info(f\"created blob sas for file: https://{blob_account_name}.blob.{azure_storage_domain}/{input_container}/{image_file_path}\")\n",
        "\n",
        "    # - the bounding box list\n",
        "    bb_json_name = 'bb_' + os.path.splitext(image_file_path)[0]+'.json'\n",
        "    bb_remote_json_path = f'{output_path}/{bb_json_name}'\n",
        "    try:\n",
        "        bb_sas_tkn = generate_blob_sas(account_name=blob_account_name, \n",
        "                                    container_name=output_container,\n",
        "                                    blob_name=bb_remote_json_path,\n",
        "                                    account_key=storage_account_key,\n",
        "                                    permission=BlobSasPermissions(read=True),\n",
        "                                    expiry=datetime.utcnow() + timedelta(hours=1))\n",
        "    except Exception as e:\n",
        "        logger.info(f\"unable to create remote bb json blob sas: {e} \")\n",
        "    else:\n",
        "        logger.info(f\"created blob sas for file: https://{blob_account_name}.blob.{azure_storage_domain}/{output_container}/{bb_remote_json_path}\")\n",
        "\n",
        "    # - the AIS point list\n",
        "    #   use container client to find ais data and then copy it to the anomaly_detection path\n",
        "    service_client = BlobServiceClient(account_url=f'https://{blob_account_name}.blob.{azure_storage_domain}/', credential=sas_token)    \n",
        "    container_client = service_client.get_container_client(output_container)\n",
        "    blobs_list = container_client.list_blobs(name_starts_with=f\"{output_path}/{ais_remote_dir_name}\")\n",
        "    ais_remote_path = \"\"\n",
        "    for blob in blobs_list:\n",
        "        if os.path.splitext(blob.name)[1] == '.json':\n",
        "            logger.info(f\"found a json file: {blob.name}\")\n",
        "            ais_remote_path = blob.name\n",
        "\n",
        "    ais_remote_path = '/' + ais_remote_path\n",
        "\n",
        "    try:\n",
        "        ais_sas_tkn = generate_blob_sas(account_name=blob_account_name, \n",
        "                                    container_name=output_container,\n",
        "                                    blob_name=ais_remote_path,\n",
        "                                    account_key=storage_account_key,\n",
        "                                    permission=BlobSasPermissions(read=True),\n",
        "                                    expiry=datetime.utcnow() + timedelta(hours=1))\n",
        "    except Exception as e:\n",
        "        logger.info(f\"unable to create ais blob sas: {e} \")\n",
        "    else:\n",
        "        logger.info(f\"created blob sas for file: https://{blob_account_name}.blob.{azure_storage_domain}/{output_container}/{ais_remote_path}\")\n",
        "\n",
        "    # - the output json file\n",
        "    out_json_remote_path = f'{output_path}/anomaly_detection_output.json'\n",
        "    try:\n",
        "        out_json_sas_tkn = generate_blob_sas(account_name=blob_account_name, \n",
        "                                    container_name=output_container,\n",
        "                                    blob_name=out_json_remote_path,\n",
        "                                    account_key=storage_account_key,\n",
        "                                    permission=BlobSasPermissions(\n",
        "                                        write=True),\n",
        "                                    expiry=datetime.utcnow() + timedelta(hours=1))\n",
        "    except Exception as e:\n",
        "        logger.info(f\"unable to create anomaly detection result json blob sas: {e} \")\n",
        "    else:\n",
        "        logger.info(f\"created blob sas for file: https://{blob_account_name}.blob.{azure_storage_domain}/{output_container}/{out_json_remote_path}\")\n",
        "\n",
        "    # Prepare the request body\n",
        "    anomaly_detection_metadata = {\n",
        "        \"img_meta\": {\n",
        "            \"blob_acct\": blob_account_name,\n",
        "            \"sas_token\": img_sas_tkn,\n",
        "            \"container\": input_container,\n",
        "            \"blob_path\": image_file_path\n",
        "        },\n",
        "        \"ais_meta\": {\n",
        "            \"blob_acct\": blob_account_name,\n",
        "            \"sas_token\": ais_sas_tkn,\n",
        "            \"container\": output_container,\n",
        "            \"blob_path\": ais_remote_path            \n",
        "        },\n",
        "        \"bb_meta\": {\n",
        "            \"blob_acct\": blob_account_name,\n",
        "            \"sas_token\": bb_sas_tkn,\n",
        "            \"container\": output_container,\n",
        "            \"blob_path\": bb_remote_json_path\n",
        "        },\n",
        "        \"anom_det_meta\": {\n",
        "            \"translate_options\": config['translate_options'],\n",
        "            \"visualization_config\": config['visualization_options'],\n",
        "            \"matching_config\": config['matching_options']\n",
        "        },\n",
        "        \"out_json_meta\": {\n",
        "            \"blob_acct\": blob_account_name,\n",
        "            \"sas_token\": out_json_sas_tkn,\n",
        "            \"container\": output_container,\n",
        "            \"blob_path\": out_json_remote_path\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(json.dumps(anomaly_detection_metadata, indent=4))\n",
        "\n",
        "    # Call the endpoint and \n",
        "    try:\n",
        "        #api auth key needs to match API_KEY in ship_anomaly_detection/gdal_server.py\n",
        "        anom_det_resp = call_detect_anomalies(\n",
        "            config['gdal_host']['app_url'],\n",
        "            anomaly_detection_metadata,\n",
        "            config['gdal_host']['key'])\n",
        "        anom_det_resp.raise_for_status()\n",
        "    except HTTPError as http_err:\n",
        "        logger.error(f'HTTP error occurred: {http_err}')\n",
        "        gdal_output = None\n",
        "    except Exception as err:\n",
        "        logger.error(f'Other error occurred: {err}')\n",
        "        gdal_output = None\n",
        "    else:\n",
        "        logger.info(f'Success. Response: {anom_det_resp.status_code}')\n",
        "        gdal_output = json.loads(anom_det_resp.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Generate three overlays\n",
        "## Helper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def call_make_overlay(gdal_host_url, make_overlay_metadata, key):\n",
        "    try:\n",
        "        resp = requests.post(\n",
        "            url=f\"{gdal_host_url}/make_overlay/\",\n",
        "            json=make_overlay_metadata,\n",
        "            headers={\n",
        "                'Content-Type': 'application/json',\n",
        "                'Gdal-Subscription-Key': key,\n",
        "                'KEY': key})\n",
        "        resp.raise_for_status()\n",
        "        resp_json = json.loads(resp.text)\n",
        "        return(resp_json)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Exception during call to make_overlay endpoint: {e}\")\n",
        "        return(None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## AIS points only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Get a SAS token for the output image\n",
        "out_ais_img_remote_path = ais_image\n",
        "out_ais_img_sas_tkn = generate_blob_sas(\n",
        "    account_name=blob_account_name,\n",
        "    container_name=output_container,\n",
        "    blob_name=out_ais_img_remote_path,\n",
        "    account_key=storage_account_key,\n",
        "    permission=BlobSasPermissions(write=True),\n",
        "    expiry=datetime.utcnow() + timedelta(hours=1))\n",
        "\n",
        "# Get all AIS points only\n",
        "features = {\n",
        "    'unpaired_ais_points': gdal_output['anomalies']['unpaired_ais_points'] + \\\n",
        "        [i[1] for i in gdal_output['anomalies']['bbox_ais_pairs']]\n",
        "}\n",
        "\n",
        "make_overlay_metadata = {\n",
        "    'in_img_meta': {\n",
        "        \"blob_acct\": blob_account_name,\n",
        "        \"sas_token\": img_sas_tkn,\n",
        "        \"container\": input_container,\n",
        "        \"blob_path\": image_file_path\n",
        "    },\n",
        "    'out_img_meta': {\n",
        "        'blob_acct': blob_account_name,\n",
        "        'sas_token': out_ais_img_sas_tkn,\n",
        "        'container': output_container,\n",
        "        'blob_path': out_ais_img_remote_path\n",
        "    },\n",
        "    'translate_options': config['translate_options'],\n",
        "    'visualization_config': config['visualization_options'],\n",
        "    'features_json_str': json.dumps(features)\n",
        "}\n",
        "\n",
        "resp_json = call_make_overlay(\n",
        "    config['gdal_host']['app_url'],\n",
        "    make_overlay_metadata,\n",
        "    config['gdal_host']['key'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## All Bounding Boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Get a SAS token for the output image\n",
        "out_bb_img_remote_path = ship_bb_image_high_res\n",
        "out_bb_img_sas_tkn = generate_blob_sas(\n",
        "    account_name=blob_account_name,\n",
        "    container_name=output_container,\n",
        "    blob_name=out_bb_img_remote_path,\n",
        "    account_key=storage_account_key,\n",
        "    permission=BlobSasPermissions(write=True),\n",
        "    expiry=datetime.utcnow() + timedelta(hours=1))\n",
        "\n",
        "# Get the bounding boxes only. Note that a confidence score threshold has\n",
        "# been applied (it's defined in the config file) so this is a subset of the\n",
        "# OD model's results.\n",
        "features = {\n",
        "    'unpaired_bboxes': gdal_output['anomalies']['unpaired_bboxes'] + [i[0] for i in gdal_output['anomalies']['bbox_ais_pairs']]\n",
        "}\n",
        "\n",
        "# The user can select a different color (by default, yellow) to show all\n",
        "# bounding boxes. Make sure it's being used:\n",
        "bbox_visualization_config = config['visualization_options'].copy()\n",
        "bbox_visualization_config['anom_bb_color'] = bbox_visualization_config['all_bb_color']\n",
        "\n",
        "make_overlay_metadata = {\n",
        "    'in_img_meta': {\n",
        "        \"blob_acct\": blob_account_name,\n",
        "        \"sas_token\": img_sas_tkn,\n",
        "        \"container\": input_container,\n",
        "        \"blob_path\": image_file_path\n",
        "    },\n",
        "    'out_img_meta': {\n",
        "        'blob_acct': blob_account_name,\n",
        "        'sas_token': out_bb_img_sas_tkn,\n",
        "        'container': output_container,\n",
        "        'blob_path': out_bb_img_remote_path\n",
        "    },\n",
        "    'translate_options': config['translate_options'],\n",
        "    'visualization_config': bbox_visualization_config,\n",
        "    'features_json_str': json.dumps(features)\n",
        "}\n",
        "\n",
        "resp_json = call_make_overlay(\n",
        "    config['gdal_host']['app_url'],\n",
        "    make_overlay_metadata,\n",
        "    config['gdal_host']['key'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Anomaly Bounding Boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Get a SAS token for the output image\n",
        "out_bb_img_remote_path = anomaly_image\n",
        "out_bb_img_sas_tkn = generate_blob_sas(\n",
        "    account_name=blob_account_name,\n",
        "    container_name=output_container,\n",
        "    blob_name=out_bb_img_remote_path,\n",
        "    account_key=storage_account_key,\n",
        "    permission=BlobSasPermissions(write=True),\n",
        "    expiry=datetime.utcnow() + timedelta(hours=1))\n",
        "\n",
        "# Get the bounding boxes only. Note that a confidence score threshold has\n",
        "# been applied (it's defined in the config file) so this is a subset of the\n",
        "# OD model's results.\n",
        "features = {\n",
        "    'unpaired_bboxes': gdal_output['anomalies']['unpaired_bboxes'] \n",
        "}\n",
        "\n",
        "# The user can select a different color (by default, yellow) to show all\n",
        "# bounding boxes. Make sure it's being used:\n",
        "bbox_visualization_config = config['visualization_options'].copy()\n",
        "bbox_visualization_config['anom_bb_color'] = bbox_visualization_config['all_bb_color']\n",
        "\n",
        "make_overlay_metadata = {\n",
        "    'in_img_meta': {\n",
        "        \"blob_acct\": blob_account_name,\n",
        "        \"sas_token\": img_sas_tkn,\n",
        "        \"container\": input_container,\n",
        "        \"blob_path\": image_file_path\n",
        "    },\n",
        "    'out_img_meta': {\n",
        "        'blob_acct': blob_account_name,\n",
        "        'sas_token': out_bb_img_sas_tkn,\n",
        "        'container': output_container,\n",
        "        'blob_path': out_bb_img_remote_path\n",
        "    },\n",
        "    'translate_options': config['translate_options'],\n",
        "    'visualization_config': bbox_visualization_config,\n",
        "    'features_json_str': json.dumps(features)\n",
        "}\n",
        "\n",
        "resp_json = call_make_overlay(\n",
        "    config['gdal_host']['app_url'],\n",
        "    make_overlay_metadata,\n",
        "    config['gdal_host']['key'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Bounding boxes and AIS points with pairs indicated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Get a SAS token for the output image\n",
        "out_anom_img_remote_path = f'{output_path}/bb_ais_pairs.png'\n",
        "out_anom_img_sas_tkn = generate_blob_sas(\n",
        "    account_name=blob_account_name,\n",
        "    container_name=output_container,\n",
        "    blob_name=out_anom_img_remote_path,\n",
        "    account_key=storage_account_key,\n",
        "    permission=BlobSasPermissions(write=True),\n",
        "    expiry=datetime.utcnow() + timedelta(hours=1))\n",
        "\n",
        "# Use all features\n",
        "features = {\n",
        "    'unpaired_ais_points': gdal_output['anomalies']['unpaired_ais_points'],\n",
        "    'unpaired_bboxes': gdal_output['anomalies']['unpaired_bboxes'],\n",
        "    'bbox_ais_pairs': gdal_output['anomalies']['bbox_ais_pairs']\n",
        "}\n",
        "\n",
        "make_overlay_metadata = {\n",
        "    'in_img_meta': {\n",
        "        \"blob_acct\": blob_account_name,\n",
        "        \"sas_token\": img_sas_tkn,\n",
        "        \"container\": input_container,\n",
        "        \"blob_path\": image_file_path\n",
        "    },\n",
        "    'out_img_meta': {\n",
        "        'blob_acct': blob_account_name,\n",
        "        'sas_token': out_anom_img_sas_tkn,\n",
        "        'container': output_container,\n",
        "        'blob_path': out_anom_img_remote_path\n",
        "    },\n",
        "    'translate_options': config['translate_options'],\n",
        "    'visualization_config': config['visualization_options'],\n",
        "    'features_json_str': json.dumps(features)\n",
        "}\n",
        "\n",
        "resp_json = call_make_overlay(\n",
        "    config['gdal_host']['app_url'],\n",
        "    make_overlay_metadata,\n",
        "    config['gdal_host']['key'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Wrap up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Extract datetime and location information needed for the cell's output\n",
        "my_datetime, location = '', []\n",
        "for line in gdal_output['img_metadata'].split('\\n'):\n",
        "    if 'TIFFTAG_DATETIME=' in line:\n",
        "        my_datetime = line.split('TIFFTAG_DATETIME=')[1].strip()\n",
        "    if ('Upper Left' in line) or ('Lower Left' in line) or \\\n",
        "        ('Upper Right' in line) or ('Lower Right' in line):\n",
        "        location.append(line.strip())\n",
        "location = '\\n'.join(location)\n",
        "\n",
        "output = {\n",
        "    'discrepancy': {\n",
        "        'anomaly_found': True if len(gdal_output['anomalies']['unpaired_bboxes']) > 0 else False,\n",
        "        'anomaly_location': location,\n",
        "        'anomaly_time': my_datetime,\n",
        "        'notebook_name': mssparkutils.runtime.context['notebookname']}}\n",
        "\n",
        "# Return the object to the pipeline\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\n",
        "mssparkutils.notebook.exit(json.dumps(output['discrepancy']))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
