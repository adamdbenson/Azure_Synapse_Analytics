{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multimodal Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "     \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "     \"spark.dynamicAllocation.enabled\": true,\n",
        "     \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "     \"spark.dynamicAllocation.maxExecutors\": 8\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "azure_storage_domain = ''\n",
        "batch_num = ''\n",
        "batch_root = ''\n",
        "blob_account_name = ''\n",
        "documents_contents_tbl_name = ''\n",
        "enriched_images_tbl_name = ''\n",
        "minted_tables_output_path = ''\n",
        "model_name = '' # This is the sentence-transformers model from Hugging Face that we will use from the Hugging Face repo - actual name on Hugging Face - \"sentence-transformers/all-mpnet-base-v2\"\n",
        "# This notebook depends on the main environment.yml file is updated with this notebook's necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyodbc\n",
        "database = 'minted'   \n",
        "driver= '{ODBC Driver 17 for SQL Server}'\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseServerlessSQLEndpoint\")\n",
        "display_dataframes = False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'documents_contents_tbl_name': documents_contents_tbl_name,\n",
        "    'enriched_images_tbl_name': enriched_images_tbl_name,\n",
        "    'batch_root': batch_root,\n",
        "    'batch_num': batch_num,\n",
        "    'model_name': model_name,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import json\n",
        "import os\n",
        "import ntpath\n",
        "import numpy as np\n",
        "from types import SimpleNamespace\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import ray\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, pandas_udf, lit, col\n",
        "from pyspark.sql.types import StringType, StructField, StructType, FloatType, IntegerType\n",
        "\n",
        "# Added to support possible BERTopic PickleError\n",
        "# https://github.com/MaartenGr/BERTopic/issues/517\n",
        "import pynndescent\n",
        "pynndescent.rp_trees.FlatTree.__module__  = \"pynndescent.rp_trees\"\n",
        "\n",
        "#batch_path = f'abfss://{manifest_container}@{blob_account_name}.dfs.{azure_storage_domain}'\n",
        "#batch_folder = os.path.dirname(manifest_file_path)\n",
        "#batch_root = f'{batch_path}{batch_folder}' \n",
        "\n",
        "with tracer.span(name=f\"Load config: {mssparkutils.runtime.context['notebookname']}\"):\n",
        "    # Initialise session, create (if necessary) and read batch config\n",
        "    sc = spark.sparkContext\n",
        "    spark = SparkSession.builder.appName(f\"TextProcessing {mssparkutils.runtime.context}\").getOrCreate()\n",
        "\n",
        "    def copy_global_config(config_path: str, global_config_path: str):\n",
        "        \"\"\"\n",
        "        This method makes sure that a config is availabile in the batch root.\n",
        "        If a config file isn't already there, it is copied over form global_config_path.\n",
        "        If there is no config under global_config_path, this function will crash (indicating an error in pipeline set up.)\n",
        "        \"\"\"\n",
        "        logger.info(\"Loading global config\")\n",
        "        try:\n",
        "            mssparkutils.fs.cp(global_config_path, config_path)    \n",
        "        except Py4JJavaError as e:\n",
        "            logger.exception(e)\n",
        "            raise e\n",
        "\n",
        "    def read_batch_config(batch_root: str, global_config_path: str):\n",
        "        \"\"\"\n",
        "        We read the config file using the Java File System API as we do not need to let multiple nodes read individual lines and join it\n",
        "        all back together again\n",
        "        \"\"\"\n",
        "        # Change our file system from 'synapse' to 'input'\n",
        "        sc._jsc.hadoopConfiguration().set(\"fs.defaultFS\", f'abfss://input@{blob_account_name}.dfs.{azure_storage_domain}')\n",
        "\n",
        "        fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
        "        config_path = sc._jvm.org.apache.hadoop.fs.Path(f'{batch_root}/config.json')\n",
        "\n",
        "        # If we don't have a batch config, copy the global one.\n",
        "        if fs.exists(config_path) != True:\n",
        "            copy_global_config(f'{batch_root}/config.json', global_config_path)\n",
        "\n",
        "        # Open our file directly rather than through spark\n",
        "        input_stream = fs.open(config_path)  # FSDataInputStream\n",
        "\n",
        "        config_string = sc._jvm.java.io.BufferedReader(\n",
        "            sc._jvm.java.io.InputStreamReader(input_stream, sc._jvm.java.nio.charset.StandardCharsets.UTF_8)\n",
        "            ).lines().collect(sc._jvm.java.util.stream.Collectors.joining(\"\\n\"))\n",
        "\n",
        "        # # Load it into json    \n",
        "        return json.loads(''.join(config_string), object_hook=lambda dictionary: SimpleNamespace(**dictionary))\n",
        "\n",
        "    # NOTE: this path should be in sync with Terraform configuration which uploads this file\n",
        "    config = read_batch_config(batch_root, global_config_path=f'abfss://configuration@{blob_account_name}.dfs.{azure_storage_domain}/config.global.json')\n",
        "\n",
        "    # Set log level\n",
        "    if config.log_level == \"INFO\":\n",
        "        logger.setLevel(logging.INFO)\n",
        "    else:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "        config.log_level = \"ERROR\"   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Model parameters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "batch_size = int(config.multimodal_clustering.batch_size) # Model dependent - Needs to be added to Synapse global config\r\n",
        "num_cpus = int(config.multimodal_clustering.num_cpus) # Resource dependent - Needs to be added to Synapse global config\r\n",
        "bert_topic_diversity = config.multimodal_clustering.diversity # BERTopic parameter - Needs to be added to Synapse global config\r\n",
        "nr_topics= config.multimodal_clustering.nr_topics # (UNUSED) BERTopic Topic reduction parameter - Needs to be added to Synapse global config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Read the documents contents table'):\n",
        "    df = spark.read.parquet(minted_tables_output_path + documents_contents_tbl_name) \\\n",
        "        .select('file_path', col('summarized_text_xsum')) \\\n",
        "        .withColumn('file_type', lit('text')) \\\n",
        "        .withColumnRenamed('summarized_text_xsum', 'summary_text')\n",
        "\n",
        "    # Convert to Pandas DataFrame\n",
        "    df_txt_pd = df.toPandas()\n",
        "\n",
        "    df_txt_pd['summary_text'] = df_txt_pd['summary_text'].apply(lambda x: x[1:-1]) # Removes and trailing chars, specifically '[]'\n",
        "    df_txt_pd['summary_text'] = df_txt_pd['summary_text'].str.strip() # Removes all leading and trailing     \n",
        "    filtered_df = df_txt_pd[df_txt_pd['summary_text'].str.len() >= 5] # Filtering out rows where the length of string in summary_text is less than n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create UDFs to extract caption text and clean it up\n",
        "get_caption = udf(lambda x: x['description']['captions'][0]['text'] if (x != None) and len(x['description']) > 0 and len(x['description']['captions'] ) > 0 else '', StringType())\n",
        "cap_caption = udf(lambda x: x.capitalize(), StringType())\n",
        "add_period_caption = udf(lambda x: x+'.' , StringType())\n",
        "\n",
        "with tracer.span(name='Read the enriched images table'):\n",
        "    df_img = spark.read.parquet(minted_tables_output_path + enriched_images_tbl_name) \\\n",
        "        .select('path','analysis_results', 'read_results') \\\n",
        "        .withColumn('file_type', lit('images')) \\\n",
        "        .withColumnRenamed('path', 'file_path') \\\n",
        "        .withColumn('summary_text', get_caption(col('analysis_results')))\n",
        "\n",
        "    # Images that have an empty caption\n",
        "    df_img_empty = df_img.where(df_img.summary_text == '') \\\n",
        "        .drop(col('analysis_results')) \\\n",
        "        .drop(col('read_results'))\n",
        "\n",
        "    # Convert to Pandas DataFrame\n",
        "    df_img_empty_pd = df_img_empty.toPandas()\n",
        "\n",
        "    df_img = df_img.where(df_img.summary_text != '') \\\n",
        "        .withColumn('summary_text', cap_caption(col('summary_text'))) \\\n",
        "        .withColumn('summary_text', add_period_caption(col('summary_text'))) \\\n",
        "        .drop(col('analysis_results')) \\\n",
        "        .drop(col('read_results'))\n",
        "\n",
        "    # Convert to Pandas DataFrame\n",
        "    df_img_pd = df_img.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "if display_dataframes:\n",
        "    print(df_img_pd.head())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "with tracer.span(name='Concatenate the text contents and enriched images captions'):\n",
        "    df_combined_pd = pd.concat([df_txt_pd, df_img_pd]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f'Mount {model_name} sentence-transformers model'):\n",
        "    try:\n",
        "        # Read the first character of the Model config.json to see if it's there. Otherwise download\n",
        "        mssparkutils.fs.head(f'abfss://synapse@{blob_account_name}.dfs.{azure_storage_domain}/models/{model_name}/config.json', 1)\n",
        "        mount_point = f'/mnt'\n",
        "        jobId = mssparkutils.env.getJobId()\n",
        "        linkedStorageName = f'{mssparkutils.env.getWorkspaceName()}-WorkspaceDefaultStorage'\n",
        "\n",
        "        # mssparkutils.fs.unmount(mount_point)\n",
        "        mssparkutils.fs.mount( \n",
        "            f'abfss://synapse@{blob_account_name}.dfs.{azure_storage_domain}/', \n",
        "            mount_point, \n",
        "            {'linkedService':f'{linkedStorageName}'} \n",
        "        )\n",
        "\n",
        "        # Please note the differences with the synfs protocol\n",
        "        # https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/synapse-file-mount-api#how-to-access-files-under-mount-point-via-local-file-system-api\n",
        "        model_location = f'/synfs/{jobId}{mount_point}/models/{model_name}/'\n",
        "        print(model_location)\n",
        "        transformers_cache_path = f'abfss://synapse@{blob_account_name}.dfs.{azure_storage_domain}/models/{model_name}/.cache/'\n",
        "        print(transformers_cache_path)\n",
        "        logger.info(f'Using {model_name} model from {model_location}.')\n",
        "        model_path = model_location\n",
        "    except Exception as e:\n",
        "        transformers_cache_path = f'abfss://synapse@{blob_account_name}.dfs.{azure_storage_domain}/models/{model_name}/.cache/'\n",
        "        print(transformers_cache_path)\n",
        "        model_path = f'{model_name}'\n",
        "        print(model_path)\n",
        "        logger.info(f'Using {model_path} model from HuggingFace.')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Get list of unique text summaries\n",
        "text_summs_list = df_combined_pd['summary_text'].unique().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f'Use Ray and run sentence-transformers {model_name} to get embeddings'):\n",
        "    import os\n",
        "    from sentence_transformers import SentenceTransformer, util\n",
        "    import ray\n",
        "\n",
        "    # Ray Actor (class) implementation\n",
        "    @ray.remote\n",
        "    class st_mpnet_text_model:\n",
        "        \n",
        "        # Want to avoid CPU contention.\n",
        "        os.environ[\"MKL_NUM_THREAD\"] = \"1\" \n",
        "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "        os.environ['TRANSFORMERS_CACHE'] = transformers_cache_path\n",
        "        \n",
        "        def __init__(self, model_dir: str):\n",
        "            from sentence_transformers import SentenceTransformer, util\n",
        "            import torch\n",
        "            \n",
        "            self.model = self.load_model(model_dir)\n",
        "            \n",
        "            # Quantized the model to speed up inference\n",
        "            # Resource: https://pytorch.org/docs/stable/quantization.html\n",
        "            # self.model = torch.quantization.quantize_dynamic(self.model, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "            \n",
        "        def load_model(self, model_dir: str):\n",
        "            model = SentenceTransformer(model_dir)\n",
        "            return model\n",
        "\n",
        "        def predict(self, txt_batch):\n",
        "            text_emb = self.model.encode(txt_batch)\n",
        "            text_emb_tbl = pd.DataFrame(text_emb)\n",
        "            text_emb_tbl['summarized_text_xsum'] = txt_batch\n",
        "            return text_emb_tbl\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Need an array to store the task_id\n",
        "task_ids = []\n",
        "\n",
        "# Actor index\n",
        "a_id = 0\n",
        "\n",
        "print('NUMBER OF CPUS', num_cpus)\n",
        "ray.init(num_cpus=num_cpus)\n",
        "\n",
        "model_actors = [st_mpnet_text_model.remote(model_path) for _ in range(num_cpus)]\n",
        "\n",
        "for i in range(0, len(text_summs_list), batch_size):\n",
        "    # Get the batch input\n",
        "    txt_batch = text_summs_list[i:i+batch_size]\n",
        "    \n",
        "    # Get which actor to use for this batch, round robin\n",
        "    a_id = (a_id + 1) % num_cpus\n",
        "    task_id = model_actors[a_id].predict.remote(txt_batch)\n",
        "    \n",
        "    # Add task id to the list\n",
        "    task_ids.append(task_id)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "print('Number of Task IDs: ', len(task_ids))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import datetime\n",
        "main_start_time = datetime.datetime.now()\n",
        "\n",
        "ready_results = []\n",
        "\n",
        "for i in range(len(task_ids)):\n",
        "    \n",
        "    start_time = datetime.datetime.now()\n",
        "    \n",
        "    # It is best to set a timeout if using this in production setting so that tasks that are taking too long don't continue to run and impact the production environment and resources.\n",
        "    ready, not_ready = ray.wait(task_ids, timeout=None)\n",
        "    ready_results.append(ray.get(ready))\n",
        "    \n",
        "    # print('Iteration: ', i) \n",
        "    # print('Ready Count: ', len(ready))\n",
        "    # print('Not Ready Count: ', len(not_ready))\n",
        "    # print('Iteration Time: ', (datetime.datetime.now()-start_time))\n",
        "    # print(\"Total Inference Time (Batches): \", (datetime.datetime.now()-main_start_time))\n",
        "    \n",
        "    task_ids = not_ready\n",
        "    if not task_ids:\n",
        "        break\n",
        "        \n",
        "print('Total inference time (batches): ', datetime.datetime.now()-main_start_time)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f'Shutdown Ray'):\n",
        "    # Shutdown Ray\n",
        "    ray.shutdown()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f'Return results from Ray into a pandas DataFrame'):\n",
        "    all_embs_tbl = pd.concat([y for x in ready_results for y in x])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f'Run BERTopic to perform topic modeling'):\n",
        "    import numpy as np\n",
        "    from bertopic import BERTopic\n",
        "    # from sentence_transformers import SentenceTransformer, util\n",
        "    # from umap import UMAP\n",
        "    # from hdbscan import HDBSCAN\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "    # Prepare embeddings\n",
        "    # sentence_model = SentenceTransformer(model_path)\n",
        "    # embeddings = sentence_model.encode(mytext, show_progress_bar=False)\n",
        "    embeddings = np.array(all_embs_tbl.iloc[:, :768])\n",
        "\n",
        "    # Get documents\n",
        "    #docs = text_summs_list\n",
        "    docs = all_embs_tbl['summarized_text_xsum'].tolist()\n",
        "\n",
        "    # Load vectorizer model\n",
        "    vectorizer_model = CountVectorizer(stop_words='english')\n",
        "\n",
        "    # Train BERTopic\n",
        "    # topic_model = BERTopic(embedding_model=model_path, vectorizer_model=vectorizer_model, diversity=0.2)\n",
        "    # topic_model = BERTopic(vectorizer_model=vectorizer_model, diversity=0.2, nr_topics=\"auto\") # Use for topic reduction\n",
        "    topic_model = BERTopic(vectorizer_model=vectorizer_model, diversity=bert_topic_diversity) # pre-computed embeddings; default is NOT \"auto\" but None\n",
        "    fitted_topic_model = topic_model.fit(docs, embeddings)\n",
        "\n",
        "    # Run the visualization with the original embeddings to visualize documents by topic\n",
        "    f1 = fitted_topic_model.visualize_documents(docs, embeddings=embeddings)\n",
        "\n",
        "    # Create topic model and calculate topics per class\n",
        "    #topics_main, probs_main = topic_model.fit_transform(docs, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f'Get the x and y values from the fitted topic model'):\n",
        "    all_outputs = []\n",
        "\n",
        "    for x in f1.data:\n",
        "        summarized_text_xsum = x['hovertext']\n",
        "        Topic_Name_Main = np.array([x['name']] * len(x['hovertext'])) \n",
        "        x_val = x['x']\n",
        "        y_val = x['y']\n",
        "        \n",
        "        all_outputs.extend([{\n",
        "            'summarized_text_xsum': summarized_text_xsum,\n",
        "            'Topic_Name_Main': Topic_Name_Main,\n",
        "            'X':x_val,\n",
        "            'Y':y_val} for summarized_text_xsum, Topic_Name_Main, x_val, y_val in zip(\n",
        "                summarized_text_xsum, Topic_Name_Main, x_val, y_val)])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f'Create a table with the text, topic, x and y values'):\n",
        "    res_data = pd.json_normalize(all_outputs)\n",
        "    res_data = res_data.fillna('')\n",
        "    res_data_final = res_data[res_data['summarized_text_xsum'] != '']\n",
        "    res_data_final = res_data_final.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f'Merge embeddings table with the table of x and y values'):\n",
        "    result_df = df_combined_pd.merge(res_data_final, left_on='summary_text', right_on='summarized_text_xsum',  how='left')\n",
        "    result_df = result_df[['file_path','file_type','summarized_text_xsum','Topic_Name_Main','X','Y']]\n",
        "    result_df['Topic_Name_Main'] = result_df['Topic_Name_Main'].apply(lambda x: str(x))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Helper function to avoid the Type Error if there are NaNs\n",
        "def cluster_util_func(topic_name):\n",
        "    try:\n",
        "        if topic_name != \"other\":\n",
        "            return int(topic_name.split(\"_\")[0])\n",
        "        else:\n",
        "            return int(-1)\n",
        "    except Exception: # Quick fix to avoid the Type Error if there are NaNs\n",
        "        return int(-1)\n",
        "\n",
        "def cluster_name__util_func(topic_name):\n",
        "    try:\n",
        "        if topic_name != \"other\":\n",
        "            return \"_\".join(x.split(\"_\")[1:])\n",
        "        else:\n",
        "            return 'other'\n",
        "    except Exception: # Quick fix to avoid the Type Error if there are NaNs\n",
        "        return 'other'\n",
        "\n",
        "# Replace Topic_Name_Main = 'nan' with 'other'\n",
        "result_df['Topic_Name_Main'] = result_df['Topic_Name_Main'].apply(lambda x: 'other' if x == 'nan' else x ) \n",
        "\n",
        "# A small table with two columns, the first being an integer ID for each cluster, \n",
        "# and the second being the auto-generated label for the cluster. \n",
        "result_df['cluster'] = result_df['Topic_Name_Main'].apply(lambda x: cluster_util_func(x) )\n",
        "result_df['cluster_name'] = result_df['Topic_Name_Main'].apply(lambda x: cluster_name__util_func(x))\n",
        "\n",
        "small_tbl = result_df[['cluster', 'cluster_name']]\n",
        "small_tbl = small_tbl.drop_duplicates()\n",
        "small_tbl = small_tbl.sort_values(by='cluster').reset_index(drop=True)\n",
        "\n",
        "if display_dataframes:\n",
        "    print(small_tbl.head())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Prepare empty entries for images that had no available caption\n",
        "df_img_empty_pd['summarized_text_xsum'] = ''\n",
        "df_img_empty_pd['Topic_Name_Main'] = 'other'\n",
        "df_img_empty_pd['X'] = None # was: np.nan\n",
        "df_img_empty_pd['Y'] = None # was: np.nan\n",
        "df_img_empty_pd['cluster'] = -1\n",
        "df_img_empty_pd['cluster_name'] = 'other'\n",
        "df_img_empty_pd = df_img_empty_pd.drop(['summary_text'], axis=1)\n",
        "\n",
        "if display_dataframes:\n",
        "    print(df_img_empty_pd.head())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "if display_dataframes:\n",
        "    print(len(result_df))\n",
        "    print(len(df_img_empty_pd))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# A table of all documents and images that could be clustered (e.g. had non-empty captions/summaries), \n",
        "# including their filenames, cluster assignments, summaries, thumbnails (for images), 2D \"coordinates,\" \n",
        "# and any other info that might be surfaced on a tab with multimodal clustering results\n",
        "output_df = pd.concat([result_df, df_img_empty_pd]).reset_index(drop=True)\n",
        "output_df['cluster'] = output_df['cluster'].astype('Int64')\n",
        "output_df.rename(columns={'Topic_Name_Main': 'topic_name', 'file_type': 'type_of_file'}, inplace=True)\n",
        "\n",
        "output_df = output_df[[\n",
        "    'file_path',\n",
        "    'type_of_file',\n",
        "    'summarized_text_xsum',\n",
        "    'topic_name',\n",
        "    'cluster',\n",
        "    'X',\n",
        "    'Y']]\n",
        "\n",
        "if display_dataframes:\n",
        "    print(len(output_df))\n",
        "    print(output_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# from pyspark.sql.functions import regexp_replace, length, col\n",
        "clustered_text_lookup_tbl_name = f'{batch_num}_clustered_multimodal_lookup'\n",
        "clustered_multimodal_tbl_name = f'{batch_num}_clustered_multimodal'\n",
        "\n",
        "# Change back to spark data frame\n",
        "df_spark_small = spark.createDataFrame(small_tbl)\n",
        "df_spark = spark.createDataFrame(output_df)\n",
        "\n",
        "if display_dataframes:\n",
        "    df_spark_small.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "if display_dataframes:\n",
        "    df_spark.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Persist to clustered multimodal lookup table'):\n",
        "    df_spark_small.write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{clustered_text_lookup_tbl_name}')\n",
        "    if display_dataframes:\n",
        "      df_spark_small.show()\n",
        "    df_spark_small.printSchema()\n",
        "\n",
        "    df_output_sql_command_lk = f\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{clustered_text_lookup_tbl_name}') CREATE EXTERNAL TABLE [{clustered_text_lookup_tbl_name}] ([cluster] bigint, [cluster_name] nvarchar(4000)) WITH (LOCATION = 'minted_tables/{clustered_text_lookup_tbl_name}/**', DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "    \n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "      with conn.cursor() as cursor:\n",
        "        cursor.execute(df_output_sql_command_lk)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Persist to clustered multimodal table'):\n",
        "    #Drop Nan values before writing out\n",
        "    df_spark = df_spark.na.drop()\n",
        "    df_spark.write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{clustered_multimodal_tbl_name}')\n",
        "    if display_dataframes:\n",
        "      df_spark.show()\n",
        "    df_spark.printSchema()\n",
        "\n",
        "    df_output_sql_command = f\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{clustered_multimodal_tbl_name}') CREATE EXTERNAL TABLE [{clustered_multimodal_tbl_name}] ([file_path] nvarchar(4000), [type_of_file] nvarchar(4000), [summarized_text_xsum] nvarchar(4000), [topic_name] nvarchar(4000), [cluster] bigint, [X] float, [Y] float ) WITH (LOCATION = 'minted_tables/{clustered_multimodal_tbl_name}/**', DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "    \n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "      with conn.cursor() as cursor:\n",
        "        cursor.execute(df_output_sql_command)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# return name of new table\n",
        "output = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'clustered_multimodal_tbl_name': clustered_multimodal_tbl_name,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "\n",
        "# Return the object to the pipeline\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\n",
        "mssparkutils.notebook.exit(output['custom_dimensions'])\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Raise clustering complete event"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# # Prepare the event contents\n",
        "# with tracer.span(name='preparing contents to send to event grid'):   \n",
        "#     from datetime import datetime\n",
        "#     now = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S%Z\")    \n",
        "#     web_app_uri = config.rule_sets.webapp_uri\n",
        "#     subscriber_uri = config.rule_sets.teams_webhook_uri\n",
        "#     alert_email = config.rule_sets.alert_email    \n",
        "#     df_cluster_count = df_spark.groupBy(\"cluster\").count()\n",
        "#     df_cluster_count = df_cluster_count.orderBy('cluster', ascending=True)\n",
        "#     cluster_json_list = df_cluster_count.toJSON().collect()\n",
        "#     num_of_clusters = df_cluster_count.distinct().count ()\n",
        "#     cluster_output = ''\n",
        "#     for x in range(len(cluster_json_list)): \n",
        "#         cluster_output = cluster_output + ', ' + cluster_json_list[x]   \n",
        "#     cluster_output = cluster_output[2:]\n",
        "#     cluster_output_str = ''.join(cluster_output)\n",
        "\n",
        "#     # generate the Event Grid json \n",
        "#     event_data = f'{{\"batch_id\": \"{batch_num}\",' \\\n",
        "#         f'\"batch_description\": \"{batch_description}\",' \\\n",
        "#         f'\"eventDate\": \"{now}\",' \\\n",
        "#         f'\"eventMetrics\": {{' \\\n",
        "#         f'  \"event_type\": \"text\",' \\\n",
        "#         f'  \"files_processed_count\": \"{text_file_count}\",' \\\n",
        "#         f'  \"event_detail_uri\": \"https://{web_app_uri}/reports\",' \\\n",
        "#         f'  \"num_of_clusters\": {num_of_clusters},' \\\n",
        "#         f'  \"clusters\": [' \\\n",
        "#         f'      {cluster_output_str}' \\\n",
        "#         f'  ]' \\\n",
        "#         f'}},' \\\n",
        "#         f'\"teams_webhook_endpoint\": \"{subscriber_uri}\",' \\\n",
        "#         f'\"alert_email\": \"{alert_email}\"' \\\n",
        "#         f'}}'\n",
        "\n",
        "#     event_data_obj = json.loads(event_data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# # Raise the event\n",
        "# with tracer.span(name='sending message to event grid'):    \n",
        "#     from azure.identity import ClientSecretCredential\n",
        "#     from azure.eventgrid import EventGridPublisherClient, EventGridEvent    \n",
        "\n",
        "#     # Get value from keyvault to build Event Grid Topic event\n",
        "#     subscription_id = TokenLibrary.getSecretWithLS(\"keyvault\", 'SubscriptionId')\n",
        "#     resource_group_name = TokenLibrary.getSecretWithLS(\"keyvault\", 'ResourceGroupName')\n",
        "#     event_grid_topic_name = TokenLibrary.getSecretWithLS(\"keyvault\", 'EventGridTopicName')\n",
        "#     event_grid_topic_endpoint = TokenLibrary.getSecretWithLS(\"keyvault\", 'EventGridTopicEndpointUri')\n",
        "#     tenant_id = TokenLibrary.getSecretWithLS(\"keyvault\", 'TenantID')\n",
        "#     client_id = TokenLibrary.getSecretWithLS(\"keyvault\", 'ADAppRegClientId')\n",
        "#     client_secret = TokenLibrary.getSecretWithLS(\"keyvault\", 'ADAppRegClientSecret')\n",
        "#     event_grid_topic = f'/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.EventGrid/topics/{event_grid_topic_name}'\n",
        "#     credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n",
        "#     client = EventGridPublisherClient(event_grid_topic_endpoint, credential)\n",
        "\n",
        "#     try:\n",
        "#         # queue event grid message\n",
        "#         event = EventGridEvent(data=event_data_obj, subject=\"MINTED/ClusterAlert\", event_type=\"MINTED.ruleTriggered\", data_version=\"1.0\", topic=event_grid_topic)\n",
        "#         client.send(event)\n",
        "#         print(\"done\")\n",
        "#     except Exception as e:\n",
        "#         logger.exception(e)\n",
        "#         raise e\n",
        "\n",
        "\n",
        "# # Return the object to the pipeline\n",
        "# logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\n",
        "# mssparkutils.notebook.exit(output['custom_dimensions'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
