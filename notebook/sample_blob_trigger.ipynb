{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sample notebook: process manifest.txt batch file\n",
        "\n",
        "This is a basic notebook illustrating how we can invoke via a pipeline which is triggered by a batch file being uploaded to blob storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "# This is a parameters cell where we define the batch_file details as params to be passed in by the pipeline\n",
        "manifest_file_path=''\n",
        "manifest_container=''\n",
        "blob_account_name = ''\n",
        "azure_storage_domain = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "tags": []
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "# Blob connection details\n",
        "path = f'abfss://{manifest_container}@{blob_account_name}.dfs.{azure_storage_domain}{manifest_file_path}'\n",
        "\n",
        "# Read as a dataframe\n",
        "df = spark.read.text(path)\n",
        "df.show()\n",
        "\n",
        "\n",
        "# Write the dataframe txt file to the output container (stripping manifest.txt from the end of the path)\n",
        "manifest_folder =  os.path.dirname(manifest_file_path)\n",
        "output_path = f'abfss://output@{blob_account_name}.dfs.{azure_storage_domain}{manifest_folder}/sample'\n",
        "df.write.mode('overwrite').text(output_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
