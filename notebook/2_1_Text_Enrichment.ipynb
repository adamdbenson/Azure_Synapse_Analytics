{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.1 Text Enrichment\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "     \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "     \"spark.dynamicAllocation.enabled\": true,\n",
        "     \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "     \"spark.dynamicAllocation.maxExecutors\": 4,\n",
        "     \"spark.jars.packages\": \"com.microsoft.azure:synapseml_2.12:0.10.0-19-c3a445c5-SNAPSHOT\",\n",
        "      \"spark.jars.repositories\": \"https://mmlspark.azureedge.net/maven\",\n",
        "      \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12,com.fasterxml.jackson.core:jackson-databind\",\n",
        "      \"spark.yarn.user.classpath.first\": \"true\"\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "documents_contents_tbl_name = ''\n",
        "batch_num = ''\n",
        "file_system = ''\n",
        "display_dataframes = False\n",
        "minted_tables_output_path = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyodbc\r\n",
        "database = 'minted'   \r\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\r\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\r\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\r\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseServerlessSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load keys, set defaults\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "text_analytics_keys = mssparkutils.credentials.getSecretWithLS(\"keyvault\", 'TextAnalyticsKeys').split(',')\n",
        "\n",
        "cog_svc_concurrency = 1\n",
        "cog_svc_batch_size = 15 # The /analyze endpoint that TextAnalyze uses is documented to allow batches of up to 25 documents\n",
        "cog_svc_intial_polling_delay = 15000 # Time (in ms) to wait before first poll for results\n",
        "cog_svc_polling_delay = 10000 # Time (in ms) to wait between repeated polling for results\n",
        "cog_svc_maximum_retry_count = 100 # Maximum number of retries. 60 => 60 * 10s + 15s = 615s ~= 10 mins allowed for a job to complete\n",
        "\n",
        "# Column names \n",
        "file_path_col = 'file_path'\n",
        "text_col = 'text_content_target_lang'\n",
        "text_split_col = 'text_content_target_lang_split'\n",
        "chunk_number_col = 'chunk_number'\n",
        "text_analysis_col = 'text_analysis'\n",
        "text_analysis_error_col = 'text_analysis_error'\n",
        "text_analytics_key_col = 'text_analytics_key'\n",
        "named_entities_col = 'named_entities'\n",
        "pii_col = 'pii'\n",
        "key_phrases_col = 'key_phrases'\n",
        "pii_redacted_text_col = 'pii_redacted_text'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'documents_contents_tbl_name': documents_contents_tbl_name,\n",
        "    'cog_svc_concurrency': cog_svc_concurrency,\n",
        "    'cog_svc_batch_size': cog_svc_batch_size,\n",
        "    'file_system': file_system,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import uuid\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import StringType, StructType, StructField\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from synapse.ml.cognitive import *\n",
        "from synapse.ml.featurize.text import PageSplitter\n",
        "from synapse.ml.stages import FixedMiniBatchTransformer\n",
        "from synapse.ml.stages import FlattenBatch\n",
        "\n",
        "# Initialise session and config\n",
        "sc = spark.sparkContext\n",
        "spark = SparkSession.builder.appName(f\"TextProcessing {mssparkutils.runtime.context}\").getOrCreate()\n",
        "\n",
        "def read_batch_config(batch_root: str):\n",
        "    \"\"\"\n",
        "    We read the config file using the Java File System API as we do not need to let multiple nodes read individual lines and join it\n",
        "    all back together again\n",
        "    \"\"\"\n",
        "    # Change our file system from 'synapse' to 'input'\n",
        "    sc._jsc.hadoopConfiguration().set(\"fs.defaultFS\", file_system)\n",
        "\n",
        "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
        "    config_path = sc._jvm.org.apache.hadoop.fs.Path(f'{batch_root}/config.json')\n",
        "\n",
        "    # If we don't have a batch config, copy the global one.\n",
        "    if fs.exists(config_path) != True:\n",
        "        logger.error(f'{config_path} not found.')\n",
        "\n",
        "    # Open our file directly rather than through spark\n",
        "    input_stream = fs.open(config_path)  # FSDataInputStream\n",
        "\n",
        "    config_string = sc._jvm.java.io.BufferedReader(\n",
        "        sc._jvm.java.io.InputStreamReader(input_stream, sc._jvm.java.nio.charset.StandardCharsets.UTF_8)\n",
        "        ).lines().collect(sc._jvm.java.util.stream.Collectors.joining(\"\\n\"))\n",
        "\n",
        "    # Load it into json    \n",
        "    return json.loads(''.join(config_string), object_hook=lambda dictionary: SimpleNamespace(**dictionary))\n",
        "\n",
        "with tracer.span(name=f\"Load config: {mssparkutils.runtime.context['notebookname']}\"):\n",
        "    try:\n",
        "        config = read_batch_config(batch_root)\n",
        "    except Exception as e:\n",
        "        logger.exception(e)\n",
        "        raise e\n",
        "\n",
        "    # Set log level\n",
        "    if config.log_level == \"INFO\":\n",
        "        logger.setLevel(logging.INFO)\n",
        "    else:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "        config.log_level = \"ERROR\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with tracer.span(name='Load documents contents table'):\n",
        "    df = spark.read.parquet(minted_tables_output_path + documents_contents_tbl_name).select('file_path',f'{text_col}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "with tracer.span(name='Split large documents'):\n",
        "    page_splitter = (PageSplitter()\n",
        "        .setInputCol(text_col)\n",
        "        .setMaximumPageLength(5000)\n",
        "        .setMinimumPageLength(4500)\n",
        "        .setOutputCol(text_split_col))\n",
        "\n",
        "    df_split = page_splitter.transform(df)\n",
        "\n",
        "    df_split = df_split.select(\"file_path\", text_col, F.posexplode(text_split_col).alias(chunk_number_col, text_split_col))\n",
        "\n",
        "    if display_dataframes:\n",
        "        df_split.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "with tracer.span(name='Group rows into batches'):\n",
        "      # This will reduce the number of API calls to Cognitive Services\n",
        "\n",
        "      fmbt = (FixedMiniBatchTransformer()\n",
        "            .setBatchSize(cog_svc_batch_size))\n",
        "\n",
        "      df_batched = fmbt.transform(df_split)\n",
        "\n",
        "      if display_dataframes:\n",
        "            df_batched.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "with tracer.span(name='Distribute cognitive service keys across rows'):\n",
        "    def rand_key() :\n",
        "        index = random.randint(0, len(text_analytics_keys)-1)\n",
        "        return text_analytics_keys[index]\n",
        "    udf_rand_key = F.udf(rand_key, StringType())\n",
        "\n",
        "    df_batched = df_batched.withColumn(\"text_analytics_key\", udf_rand_key())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "with tracer.span(name='Define TextAnalyze transform'):\n",
        "    # Text Analysis (key phrase extraction, named entity recognition, PII recognition + redaction)\n",
        "    text_analyze = (TextAnalyze()\n",
        "        .setTextCol(text_split_col)\n",
        "        .setLocation(config.location)\n",
        "        .setSubscriptionKeyCol(text_analytics_key_col)\n",
        "        .setOutputCol(text_analysis_col)\n",
        "        .setErrorCol(text_analysis_error_col)\n",
        "        .setConcurrency(cog_svc_concurrency)\n",
        "        .setInitialPollingDelay(cog_svc_intial_polling_delay)\n",
        "        .setPollingDelay(cog_svc_polling_delay)\n",
        "        .setMaxPollingRetries(cog_svc_maximum_retry_count)\n",
        "        .setSuppressMaxRetriesExceededException(True)\n",
        "        .setLanguage(config.prep.target_language)\n",
        "        .setEntityRecognitionTasks([{\"parameters\": { \"model-version\": \"latest\"}}])\n",
        "        .setKeyPhraseExtractionTasks([{\"parameters\": { \"model-version\": \"latest\"}}])\n",
        "        .setEntityRecognitionPiiTasks([{\"parameters\": { \"model-version\": \"latest\"}}])\n",
        "        <<SYNAPSE_ML_TEXT_ANALYZE_ENDPOINT_CMD>>\n",
        "        )\n",
        "\n",
        "    df_results_batched = text_analyze.transform(df_batched)\n",
        "\n",
        "    if display_dataframes:\n",
        "        df_results_batched.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "flattener = FlattenBatch()\n",
        "\n",
        "df_results = flattener.transform(df_results_batched)\n",
        "\n",
        "if display_dataframes:\n",
        "    df_results.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# split out text analysis results\n",
        "df_results = df_results.select(\n",
        "        file_path_col,\n",
        "        f\"{text_analysis_col}.*\",\n",
        "        text_analysis_error_col,\n",
        "        chunk_number_col\n",
        "    )\n",
        "    \n",
        "if display_dataframes:\n",
        "    df_results.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# collate split rows back to single row per document\n",
        "error_response_schema = StructType(\n",
        "    [StructField(\"error\", StructType(\n",
        "        [StructField(\"code\", StringType()), StructField(\"message\", StringType())]\n",
        "    ))]\n",
        ")\n",
        "\n",
        "sorted_text_col = \"sorted_text\"\n",
        "df_output = df_results.select(\n",
        "                file_path_col,\n",
        "                # we only have a single task for each task type, so unpack it\n",
        "                col(\"entityRecognition\")[0][\"result\"][\"entities\"].alias(named_entities_col),\n",
        "                col(\"entityRecognitionPii\")[0][\"result\"][\"entities\"].alias(pii_col),\n",
        "                col(\"keyPhraseExtraction\")[0][\"result\"][\"keyPhrases\"].alias(key_phrases_col),\n",
        "                col(\"entityRecognitionPii\")[0][\"result\"][\"redactedText\"].alias(pii_redacted_text_col),\n",
        "                 # set up as an array for the grouping step\n",
        "                F.from_json(df_results[text_analysis_error_col][\"response\"], error_response_schema)[\"error\"][\"message\"].alias(text_analysis_error_col),\n",
        "                chunk_number_col\n",
        "            )\\\n",
        "            .groupby(file_path_col)\\\n",
        "            .agg(\n",
        "                F.flatten(F.collect_list(\"named_entities\")).alias(named_entities_col),\n",
        "                F.flatten(F.collect_list(\"pii\")).alias(pii_col),\n",
        "                F.flatten(F.collect_list(\"key_phrases\")).alias(key_phrases_col),\n",
        "                F.sort_array(F.collect_list(F.struct(chunk_number_col, pii_redacted_text_col))).alias(sorted_text_col),\n",
        "                F.max(col(text_analysis_error_col)).alias(text_analysis_error_col)\n",
        "            )\\\n",
        "            .withColumn(\n",
        "                pii_redacted_text_col,\n",
        "                F.concat_ws(\"\", col(f\"{sorted_text_col}.{pii_redacted_text_col}\"))\n",
        "            )\\\n",
        "            .drop(sorted_text_col)\n",
        "\n",
        "if display_dataframes:\n",
        "    df_output.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with tracer.span(name='Persist enriched text as table'):\n",
        "    enriched_text_tbl_name = f'{batch_num}_enriched_text'\n",
        "    df_output.show()\n",
        "    df_output.printSchema()\n",
        "    print(enriched_text_tbl_name)\n",
        "    df_output.write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{enriched_text_tbl_name}')\n",
        "\n",
        "    df_output_sql_command = f\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{enriched_text_tbl_name}') CREATE EXTERNAL TABLE [{enriched_text_tbl_name}] ([file_path] nvarchar(4000), [named_entities] varchar(MAX), [pii] varchar(MAX), [key_phrases] varchar(MAX), [text_analysis_error] varchar(MAX), [pii_redacted_text] varchar(MAX)) WITH (LOCATION = 'minted_tables/{enriched_text_tbl_name}/**', DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "    \n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "      with conn.cursor() as cursor:\n",
        "        cursor.execute(df_output_sql_command)\n",
        "\n",
        "# return name of new table\n",
        "output = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'enriched_text_tbl_name': enriched_text_tbl_name,\n",
        "    'file_system': file_system,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "\n",
        "# Return the object to the pipeline\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\n",
        "mssparkutils.notebook.exit(output['custom_dimensions'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
