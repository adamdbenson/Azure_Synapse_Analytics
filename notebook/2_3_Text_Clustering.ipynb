{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "     \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "     \"spark.dynamicAllocation.enabled\": true,\n",
        "     \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "     \"spark.dynamicAllocation.maxExecutors\": 4\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "batch_num = ''\n",
        "batch_root = ''\n",
        "documents_contents_tbl_name = ''\n",
        "file_system = ''\n",
        "batch_description = ''\n",
        "text_file_count = 0\n",
        "minted_tables_output_path = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyodbc\r\n",
        "database = 'minted'   \r\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\r\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\r\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\r\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseServerlessSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'documents_contents_tbl_name': documents_contents_tbl_name,\n",
        "    'batch_root': batch_root,\n",
        "    'batch_num': batch_num,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import json\n",
        "import ntpath\n",
        "import numpy as np\n",
        "from types import SimpleNamespace\n",
        "\n",
        "from pyspark.ml.feature import HashingTF, IDF, CountVectorizer, StopWordsRemover, PCA, RegexTokenizer\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from sklearn.manifold import TSNE\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
        "\n",
        "# Initialise session and config\n",
        "sc = spark.sparkContext\n",
        "spark = SparkSession.builder.appName(f\"TextProcessing {mssparkutils.runtime.context}\").getOrCreate()\n",
        "\n",
        "config = json.loads(''.join(sc.textFile(f'{batch_root}/config.json').collect()), object_hook=lambda dictionary: SimpleNamespace(**dictionary))\n",
        "job_config = config.clustering\n",
        "\n",
        "# Set log level\n",
        "if config.log_level == \"INFO\":\n",
        "    logger.setLevel(logging.INFO)\n",
        "else:\n",
        "    logger.setLevel(logging.ERROR)\n",
        "    config.log_level = \"ERROR\"\n",
        "\n",
        "job_config_parameters = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'minTokenLength': job_config.min_token_length,\n",
        "    'minDF': job_config.min_DF,\n",
        "    'maxDF': job_config.max_DF,\n",
        "    'numFeatures': job_config.num_features,\n",
        "    'minDocFreq': job_config.min_doc_freq,\n",
        "    'k': job_config.k,\n",
        "    'pca1': job_config.pca1,\n",
        "    'perplexity': job_config.perplexity,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: JOB_CONFIG\", extra=job_config_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def get_x(row):\n",
        "    return str(row.values[0])\n",
        "\n",
        "def get_y(row):\n",
        "    return str(row.values[1])\n",
        "\n",
        "def get_joined_text(row):\n",
        "    return \" \".join(row)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "tags": []
      },
      "source": [
        "from pyspark.sql.functions import regexp_replace, length, col\n",
        "clustered_text_tbl_name = f'{batch_num}_clustered_text'\n",
        "\n",
        "with tracer.span(name='Load documents table'):\n",
        "    df = spark.read.parquet(minted_tables_output_path + documents_contents_tbl_name).select('text_content_target_lang','file_name','file_type','file_path')\n",
        "\n",
        "def run_clustering(df): \n",
        "    # use a regex to clear out other characters, and if there are less than 100 chars left, filter out the row\n",
        "    if df.count() < 2: \n",
        "        empty_RDD = spark.sparkContext.emptyRDD()\n",
        "        columns = StructType([\n",
        "            StructField('file_path', StringType()),\n",
        "            StructField('processed_text',StringType()),\n",
        "            StructField('cluster', IntegerType()),\n",
        "            StructField('X', FloatType()),\n",
        "            StructField('Y', FloatType())\n",
        "        ])\n",
        "        empty_df = spark.createDataFrame(data = empty_RDD, schema = columns)        \n",
        "        return empty_df\n",
        "\n",
        "\n",
        "    df = df.filter(length(regexp_replace(col('text_content_target_lang'), '[^a-z]', '')) > 100)\n",
        "\n",
        "    # Now we will determine the distance measure based on the dataset\n",
        "    # For small functional tests we want to use the euclidean distance else cosine\n",
        "    distance_measure = \"cosine\"\n",
        "\n",
        "    if int(df.count()) < 100: # Threshold until we find a better value\n",
        "        distance_measure = \"euclidean\"\n",
        "\n",
        "    with tracer.span(name='Create clustering pipeline'):\n",
        "        k = int(job_config.k) # We will need to calculate k to determine the optimum cluster number for any new dataset\n",
        "        tokenizer = RegexTokenizer(inputCol=\"text_content_target_lang\", outputCol=\"tokens\", gaps=False, minTokenLength=int(job_config.min_token_length), toLowercase=True, pattern=\"[a-zA-Z\\-][a-zA-Z\\-]{2,}\")\n",
        "        remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"stopWordsRemovedTokens\")\n",
        "        vectorizer = CountVectorizer(inputCol=\"stopWordsRemovedTokens\", outputCol=\"word_count_vector\", minDF=float(job_config.min_DF), maxDF=float(job_config.max_DF))\n",
        "        hashingTF = HashingTF(inputCol=\"stopWordsRemovedTokens\", outputCol=\"rawFeatures\", numFeatures=int(job_config.num_features))\n",
        "        idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=int(job_config.min_doc_freq))\n",
        "        pca_1 = PCA(k=int(job_config.pca1), inputCol=\"features\")\n",
        "        pca_1.setOutputCol(\"pca_features\")\n",
        "        kmeans = KMeans(k=k, seed=42, initMode=\"k-means||\", distanceMeasure=distance_measure)\n",
        "        pipeline = Pipeline(stages=[tokenizer, remover, vectorizer, hashingTF, idf, pca_1, kmeans])\n",
        "\n",
        "    with tracer.span(name='Fit pipeline'):\n",
        "        model = pipeline.fit(df)\n",
        "\n",
        "    with tracer.span(name='Transform pipeline'):\n",
        "        results = model.transform(df)\n",
        "\n",
        "    with tracer.span(name='Convert to pandas dataframe'):\n",
        "        pandas_df = results.toPandas()\n",
        "\n",
        "    with tracer.span(name='T-SNE dimensionality reduction'):\n",
        "        series = pandas_df['features'].apply(lambda x : np.array(x.toArray())).to_numpy().reshape(-1,1)\n",
        "        features = np.apply_along_axis(lambda x : x[0], 1, series)\n",
        "\n",
        "        tsne = TSNE(verbose=1, perplexity=int(job_config.perplexity)) \n",
        "        X_embedded = tsne.fit_transform(features)\n",
        "\n",
        "    with tracer.span(name='Clean up dataframe'):\n",
        "        pandas_df.rename(columns = {'file':'file_name', 'prediction':'cluster', 'stopWordsRemovedTokens': 'processed_text'}, inplace = True)\n",
        "        pandas_df['X'] = X_embedded[:,0]\n",
        "        pandas_df['Y'] = X_embedded[:,1]\n",
        "        pandas_df['processed_text'] = pandas_df.apply(lambda x: get_joined_text(x['processed_text']), axis=1)\n",
        "        pandas_df.drop(columns = [\"tokens\", \"word_count_vector\", \"pca_features\", \"features\", \"rawFeatures\", \"file_type\", \"file_name\", \"text_content_target_lang\"], inplace = True)\n",
        "\n",
        "    with tracer.span(name='Move back to spark RDD'):\n",
        "        df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "        return df \n",
        "\n",
        "with tracer.span(name='Run clustering'): \n",
        "    df = run_clustering(df)\n",
        "\n",
        "with tracer.span(name='Perist to clustered text table'):\n",
        "    df.write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{clustered_text_tbl_name}')\n",
        "    df.show()\n",
        "    df.printSchema()\n",
        "\n",
        "    df_output_sql_command = f\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{clustered_text_tbl_name}') CREATE EXTERNAL TABLE [{clustered_text_tbl_name}] ([file_path] nvarchar(4000), [processed_text] nvarchar(4000), [cluster] bigint, [X] float, [Y] float) WITH (LOCATION = 'minted_tables/{clustered_text_tbl_name}/**', DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "      with conn.cursor() as cursor:\n",
        "        cursor.execute(df_output_sql_command)\n",
        "\n",
        "# return name of new table\n",
        "output = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'clustered_text_tbl_name': clustered_text_tbl_name,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Raise clustering complete event"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Prepare the event contents\n",
        "with tracer.span(name='preparing contents to send to event grid'):   \n",
        "    from datetime import datetime\n",
        "    now = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S%Z\")    \n",
        "    web_app_uri = config.rule_sets.webapp_uri\n",
        "    subscriber_uri = config.rule_sets.teams_webhook_uri\n",
        "    alert_email = config.rule_sets.alert_email    \n",
        "    df_cluster_count = df.groupBy(\"cluster\").count()\n",
        "    df_cluster_count = df_cluster_count.orderBy('cluster', ascending=True)\n",
        "    cluster_json_list = df_cluster_count.toJSON().collect()\n",
        "    num_of_clusters = df_cluster_count.distinct().count ()\n",
        "    cluster_output = ''\n",
        "    for x in range(len(cluster_json_list)): \n",
        "        cluster_output = cluster_output + ', ' + cluster_json_list[x]   \n",
        "    cluster_output = cluster_output[2:]\n",
        "    cluster_output_str = ''.join(cluster_output)\n",
        "\n",
        "    # generate the Event Grid json \n",
        "    event_data = f'{{\"batch_id\": \"{batch_num}\",' \\\n",
        "        f'\"batch_description\": \"{batch_description}\",' \\\n",
        "        f'\"eventDate\": \"{now}\",' \\\n",
        "        f'\"eventMetrics\": {{' \\\n",
        "        f'  \"event_type\": \"text\",' \\\n",
        "        f'  \"files_processed_count\": \"{text_file_count}\",' \\\n",
        "        f'  \"event_detail_uri\": \"https://{web_app_uri}/reports\",' \\\n",
        "        f'  \"num_of_clusters\": {num_of_clusters},' \\\n",
        "        f'  \"clusters\": [' \\\n",
        "        f'      {cluster_output_str}' \\\n",
        "        f'  ]' \\\n",
        "        f'}},' \\\n",
        "        f'\"teams_webhook_endpoint\": \"{subscriber_uri}\",' \\\n",
        "        f'\"alert_email\": \"{alert_email}\"' \\\n",
        "        f'}}'\n",
        "\n",
        "    event_data_obj = json.loads(event_data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Raise the event\n",
        "with tracer.span(name='sending message to event grid'):    \n",
        "    from azure.identity import ClientSecretCredential\n",
        "    from azure.eventgrid import EventGridPublisherClient, EventGridEvent    \n",
        "\n",
        "    # Get value from keyvault to build Event Grid Topic event\n",
        "    subscription_id = TokenLibrary.getSecretWithLS(\"keyvault\", 'SubscriptionId')\n",
        "    resource_group_name = TokenLibrary.getSecretWithLS(\"keyvault\", 'ResourceGroupName')\n",
        "    event_grid_topic_name = TokenLibrary.getSecretWithLS(\"keyvault\", 'EventGridTopicName')\n",
        "    event_grid_topic_endpoint = TokenLibrary.getSecretWithLS(\"keyvault\", 'EventGridTopicEndpointUri')\n",
        "    tenant_id = TokenLibrary.getSecretWithLS(\"keyvault\", 'TenantID')\n",
        "    client_id = TokenLibrary.getSecretWithLS(\"keyvault\", 'ADAppRegClientId')\n",
        "    client_secret = TokenLibrary.getSecretWithLS(\"keyvault\", 'ADAppRegClientSecret')\n",
        "    event_grid_topic = f'/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.EventGrid/topics/{event_grid_topic_name}'\n",
        "    credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n",
        "    client = EventGridPublisherClient(event_grid_topic_endpoint, credential)\n",
        "\n",
        "    try:\n",
        "        # queue event grid message\n",
        "        event = EventGridEvent(data=event_data_obj, subject=\"MINTED/ClusterAlert\", event_type=\"MINTED.ruleTriggered\", data_version=\"1.0\", topic=event_grid_topic)\n",
        "        client.send(event)\n",
        "        print(\"done\")\n",
        "    except Exception as e:\n",
        "        logger.exception(e)\n",
        "        raise e\n",
        "\n",
        "\n",
        "# Return the object to the pipeline\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\n",
        "mssparkutils.notebook.exit(output['custom_dimensions'])        "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
