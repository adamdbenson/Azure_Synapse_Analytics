{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "azure_storage_domain = ''\n",
        "input_container = ''\n",
        "blob_account_name = ''\n",
        "image_file_path = ''\n",
        "output_container = ''\n",
        "key_vault_name = ''\n",
        "ship_bb_image_high_res = ''\n",
        "input_image_low_res = ''\n",
        "config_path = ''\n",
        "output_path = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "import base64\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# NOTE: this path should be in sync with Terraform configuration which uploads this file\n",
        "global_config_path = f'abfss://configuration@{blob_account_name}.dfs.{azure_storage_domain}/anomdet.config.global.json'\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'input_container': input_container,\n",
        "    'image_file_path': image_file_path,\n",
        "    'blob_account_name': blob_account_name,\n",
        "    'global_config_path': global_config_path,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        " \n",
        "logger.info(f\"INITIALISED: {mssparkutils.runtime.context['notebookname']}\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import os, io, sys, math\n",
        "import json\n",
        "import glob\n",
        "import requests\n",
        "import copy\n",
        "\n",
        "from requests.exceptions import HTTPError\n",
        "from pathlib import Path\n",
        "from pyspark.sql import SparkSession\n",
        "from py4j.protocol import Py4JJavaError\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from azure.storage.blob import  ResourceTypes, ContainerClient, BlobServiceClient, generate_account_sas, generate_container_sas, generate_blob_sas, AccountSasPermissions, ContainerSasPermissions, BlobSasPermissions\n",
        "from azure.identity import ClientSecretCredential\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initialise paths\n",
        "image_path = f'https://{blob_account_name}.blob.{azure_storage_domain}/{input_container}/'\n",
        "image_path_abfss = f'abfss://{input_container}@{blob_account_name}.dfs.{azure_storage_domain}/'\n",
        "image_folder = os.path.dirname(image_file_path)\n",
        "image_root = f\"{image_path}{image_folder}\"\n",
        "image_root_abfss = f'{image_path_abfss}{image_folder}'\n",
        "image_full_path = f\"{image_path}{image_file_path}\"\n",
        "output_dir = f'https://{blob_account_name}.blob.{azure_storage_domain}/{output_container}/{output_path}'\n",
        "output_dir_abfss = f'abfss://{output_container}@{blob_account_name}.dfs.{azure_storage_domain}/{output_path}'\n",
        "output_root = f'{output_dir}/{image_folder}'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f'Preparing config from global config and loading into memory'):\n",
        "    # Initialise session, create (if necessary) and read config\n",
        "    sc = spark.sparkContext\n",
        "    spark = SparkSession.builder.appName(f\"AnomalyDetection {mssparkutils.runtime.context}\").getOrCreate()\n",
        "\n",
        "    def prepare_config(image_root: str, global_config_path: str):\n",
        "        \"\"\"\n",
        "        This method makes sure that a config is availabile in the batch root.\n",
        "        If a config file isn't already there, it is copied over form global_config_path.\n",
        "        If there is no config under global_config_path, this function will crash (indicating an error in pipeline set up.)\n",
        "        \"\"\"\n",
        "        image_config_path = f'{image_root_abfss}/anomdet.config.json'\n",
        "        try: \n",
        "            mssparkutils.fs.head(image_config_path)\n",
        "        except Py4JJavaError as e:\n",
        "            if 'java.io.FileNotFoundException' in str(e):\n",
        "                # File doesn't exist, copying over the global config path\n",
        "                mssparkutils.fs.cp(global_config_path, image_config_path)    \n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "\n",
        "    # prepare_config(image_root=image_root, global_config_path=global_config_path)\n",
        "\n",
        "    config = json.loads(''.join(sc.textFile(f'{image_path_abfss}/{config_path}').collect()))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f'Getting Credentials, creating BlobServiceClient and sas_token'):\n",
        "    tenant_id = mssparkutils.credentials.getSecretWithLS('keyvault', 'TenantID')\n",
        "    client_id = mssparkutils.credentials.getSecretWithLS('keyvault', 'ADAppRegClientId')\n",
        "    client_secret = mssparkutils.credentials.getSecretWithLS('keyvault', 'ADAppRegClientSecret')\n",
        "    storage_account_key = mssparkutils.credentials.getSecretWithLS('keyvault', 'StorageAccountKey')\n",
        "    credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n",
        "    service = BlobServiceClient(account_url=f'https://{blob_account_name}.blob.{azure_storage_domain}/', credential=credential)\n",
        "    sas_token = generate_account_sas(\n",
        "        account_name=f'{blob_account_name}',\n",
        "        account_key=f'{storage_account_key}',\n",
        "        resource_types=ResourceTypes(service=False,container=True, object=True),\n",
        "        permission=AccountSasPermissions(read=True, list=True, write=True, add=True, create=True, update=True),\n",
        "        expiry=datetime.utcnow() + timedelta(hours=1)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f'Process geoTIFF through GDAL.Transform'):\n",
        "  \n",
        "  def call_img_prep(gdal_endpoint,img_prep_meta,api_key):\n",
        "    resp = \"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            # Request headers\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"Gdal-Subscription-Key\": api_key,\n",
        "            \"KEY\": api_key\n",
        "        }\n",
        "        body = img_prep_meta\n",
        "        url = f\"{gdal_endpoint}/img_prep/\"\n",
        "        resp = requests.post(url=url, json=body, headers=headers)\n",
        "        #result_response = resp.json()\n",
        "        #print(json.dumps(result_response, indent=4, sort_keys=True))\n",
        "    except Exception as e:\n",
        "        logger.error('Exception', e)\n",
        "    return resp\n",
        "\n",
        "  def call_info(gdal_endpoint,info_metadata,api_key):\n",
        "    resp = \"\"\n",
        "    try:\n",
        "      headers = {\n",
        "          # Request headers\n",
        "          \"Content-Type\": \"application/json\",\n",
        "          \"Gdal-Subscription-Key\": api_key,\n",
        "          \"KEY\": api_key\n",
        "      }\n",
        "      body = info_metadata\n",
        "      url = f\"{gdal_endpoint}/img_info/\"\n",
        "      #print(url, body, headers)\n",
        "      resp = requests.post(url=url, json=body, headers=headers)\n",
        "      result_response = resp.json()\n",
        "      print(json.dumps(result_response, indent=4, sort_keys=True))\n",
        "    except Exception as e:\n",
        "      logger.error('Exception', e)\n",
        "    #return result_response\n",
        "    return resp\n",
        "\n",
        "  gdal_host_url = config['gdal_host']['app_url']\n",
        "  out_img_blob_path = input_image_low_res\n",
        "\n",
        "  in_blob_sas_tkn = generate_blob_sas(account_name=blob_account_name, \n",
        "                              container_name=input_container,\n",
        "                              blob_name=image_file_path,\n",
        "                              account_key=storage_account_key,\n",
        "                              permission=BlobSasPermissions(read=True),\n",
        "                              expiry=datetime.utcnow() + timedelta(hours=1))\n",
        "\n",
        "  out_cont_sas_tkn = generate_container_sas(account_name=blob_account_name, \n",
        "                              container_name=output_container,\n",
        "                              account_key=storage_account_key,\n",
        "                              permission=ContainerSasPermissions(read=True, list=True, write=True, add=True, create=True, update=True),\n",
        "                              expiry=datetime.utcnow() + timedelta(hours=1))\n",
        "\n",
        "  translate_opts = config[\"translate_options\"]\n",
        "  tile_opts = config[\"tile_options\"]\n",
        "  in_img_meta = {\n",
        "      \"blob_acct\": blob_account_name,\n",
        "      \"sas_token\": in_blob_sas_tkn,\n",
        "      \"container\": input_container,\n",
        "      \"blob_path\": image_file_path,\n",
        "  }\n",
        "  out_cont_meta = {\n",
        "      \"blob_acct\": blob_account_name,\n",
        "      \"sas_token\": out_cont_sas_tkn,\n",
        "      \"container\": output_container\n",
        "  }\n",
        "  img_prep_meta = {\n",
        "      \"in_image\":in_img_meta,\n",
        "      \"out_container\":out_cont_meta,\n",
        "      \"translate_options\":translate_opts,\n",
        "      \"tile_options\":tile_opts\n",
        "  }\n",
        "    \n",
        "  #get img info\n",
        "  info_config = { \"format\": \"json\"}\n",
        "  gdal_info = { \n",
        "      \"info_options\": info_config,\n",
        "      \"in_img\": in_img_meta\n",
        "  }\n",
        "  \n",
        "  try:\n",
        "    \n",
        "    ########################################################################################\n",
        "    #   get img info\n",
        "    #\n",
        "    #    * this is used to get the metadata of the input satellite image file\n",
        "    #    * this is very useful for debugging future semantics involving the image metadata\n",
        "    #    \n",
        "    #   info_resp = call_info(gdal_host_url,gdal_info, config['gdal_host']['key'])\n",
        "    #   logger.info(json.dumps(info_resp, indent=4, sort_keys=True))\n",
        "    ########################################################################################\n",
        "\n",
        "    #prepare image for sending off to inference api\n",
        "    #api auth key needs to match API_KEY in ship_anomaly_detection/gdal_server.py\n",
        "    img_prep_resp = call_img_prep(gdal_host_url, img_prep_meta,config['gdal_host']['key'])\n",
        "    \n",
        "    img_prep_resp.raise_for_status()\n",
        "  except HTTPError as http_err:\n",
        "    logger.error(f'HTTP error occurred: {http_err}')\n",
        "  except Exception as err:\n",
        "    logger.error(f'Other error occurred: {err}')\n",
        "  else:\n",
        "    logger.info(f'Success. Response: {img_prep_resp.status_code} - {img_prep_resp.text}')\n",
        "    gdal_output = json.loads(img_prep_resp.text)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f'Run Custom Vision labeling'):\n",
        "    \n",
        "    def get_base64_encoded_image(image_path):\n",
        "        \"\"\"\n",
        "        Converts an image to base64\n",
        "        :param image_path: the filename and path\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        with open(image_path, \"rb\") as img_file:\n",
        "            return base64.b64encode(img_file.read()).decode('utf-8')\n",
        "\n",
        "    \n",
        "    def predict(img_data):\n",
        "        result_response = \"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                # Request headers\n",
        "                \"Content-Type\": \"application/json\",\n",
        "                \"Ocp-Apim-Subscription-Key\": API_KEY,\n",
        "                \"KEY\": API_KEY\n",
        "            }\n",
        "            body = {\n",
        "                \"values\": [\n",
        "                    {\n",
        "                        \"recordId\": \"0\",\n",
        "                        \"data\": {\n",
        "                            \"images\": {\n",
        "                                \"data\": img_data\n",
        "                            }\n",
        "                        }\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "            url = f\"{WEB_APP_URL}/api/extraction\"\n",
        "            print(url, headers)\n",
        "\n",
        "            resp = requests.post(url=url, json=body, headers=headers)\n",
        "            print(resp)\n",
        "            result_response = resp.json()\n",
        "            \n",
        "            #return result_response\n",
        "        except Exception as e:\n",
        "            logger.error('Exception', e)\n",
        "            raise Exception(e)\n",
        "\n",
        "        return result_response\n",
        "\n",
        "\n",
        "    def download(client, source, dest):\n",
        "        '''\n",
        "        Download a file or directory to a path on the local filesystem\n",
        "        '''\n",
        "        if not dest:\n",
        "            raise Exception('A destination must be provided')\n",
        "\n",
        "        blobs = ls_files(client, source, recursive=True)\n",
        "        if blobs:\n",
        "        # if source is a directory, dest must also be a directory\n",
        "            if not source == '' and not source.endswith('/'):\n",
        "                source += '/'\n",
        "            if not dest.endswith('/'):\n",
        "                dest += '/'\n",
        "        # append the directory name from source to the destination\n",
        "            dest += os.path.basename(os.path.normpath(source)) + '/'\n",
        "\n",
        "            blobs = [source + blob for blob in blobs]\n",
        "            for blob in blobs:\n",
        "                blob_dest = dest + os.path.relpath(blob, source)\n",
        "                download_file(client, blob, blob_dest)\n",
        "        else:\n",
        "            download_file(client, source, dest)\n",
        "\n",
        "    def download_file(client, source, dest):\n",
        "        '''\n",
        "        Download a single file to a path on the local filesystem\n",
        "        '''\n",
        "        # dest is a directory if ending with '/' or '.', otherwise it's a file\n",
        "        if dest.endswith('.'):\n",
        "            dest += '/'\n",
        "        blob_dest = dest + os.path.basename(source) if dest.endswith('/') else dest\n",
        "\n",
        "        print(f'Downloading {source} to {blob_dest}')\n",
        "        os.makedirs(os.path.dirname(blob_dest), exist_ok=True)\n",
        "        bc = client.get_blob_client(blob=source)\n",
        "        with open(blob_dest, 'wb') as file:\n",
        "            data = bc.download_blob()\n",
        "            file.write(data.readall())\n",
        "\n",
        "    def ls_files(client, path, recursive=False):\n",
        "        '''\n",
        "        List files under a path, optionally recursively\n",
        "        '''\n",
        "        if not path == '' and not path.endswith('/'):\n",
        "            path += '/'\n",
        "\n",
        "        blob_iter = client.list_blobs(name_starts_with=path)\n",
        "        files = []\n",
        "        for blob in blob_iter:\n",
        "            relative_path = os.path.relpath(blob.name, path)\n",
        "            if recursive or not '/' in relative_path:\n",
        "                files.append(relative_path)\n",
        "        return files\n",
        "\n",
        "    def ls_dirs(client, path, recursive=False):\n",
        "        '''\n",
        "        List directories under a path, optionally recursively\n",
        "        '''\n",
        "        if not path == '' and not path.endswith('/'):\n",
        "            path += '/'\n",
        "\n",
        "        blob_iter = client.list_blobs(name_starts_with=path)\n",
        "        dirs = []\n",
        "        for blob in blob_iter:\n",
        "            relative_dir = os.path.dirname(os.path.relpath(blob.name, path))\n",
        "            if relative_dir and (recursive or not '/' in relative_dir) and not relative_dir in dirs:\n",
        "                dirs.append(relative_dir)\n",
        "\n",
        "        return dirs\n",
        "\n",
        "    blob_service_client = BlobServiceClient(account_url=f'https://{blob_account_name}.blob.{azure_storage_domain}/', credential=storage_account_key)\n",
        "    container_client = blob_service_client.get_container_client(output_container)\n",
        "    remote_container_files = ls_files(container_client, f'{output_path}/tiles', recursive=True)\n",
        "    remote_tile_paths = []\n",
        "    for possible_tile_path in remote_container_files:\n",
        "        if os.path.splitext(possible_tile_path)[1][-3:] == config[\"tile_options\"][\"out_type\"]:\n",
        "            remote_tile_paths.append(possible_tile_path)\n",
        "\n",
        "    API_KEY = config['computer_vision']['key'] # This is a secret key on your app service - only requests with this key will be allowed\n",
        "    WEB_APP_URL = config['computer_vision']['app_url'] # # \"http://0.0.0.0:6000\" #\n",
        "\n",
        "    chips_and_chip_bounding_boxes = {}\n",
        "    for chip_file in remote_tile_paths:\n",
        "        try:\n",
        "            download_file(container_client, f'{output_path}/tiles/{chip_file}', f'tiles/{chip_file}')\n",
        "        except Exception as err:\n",
        "            logger.error(f'Other error occurred: {err}')\n",
        "        else:\n",
        "            logger.info(f'Success. Downloaded: {chip_file}')\n",
        "\n",
        "        try:\n",
        "            chips_and_chip_bounding_boxes[chip_file] = predict(get_base64_encoded_image(f'tiles/{chip_file}'))\n",
        "        except Exception as err:\n",
        "            logger.error(f'Other error occurred: {err}')\n",
        "        else:\n",
        "            logger.info(f'Success. Response: {chips_and_chip_bounding_boxes[chip_file]}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f'get vision model to work with aoai geolocate'):\n",
        "    class NormalAoaiGeolocateInput:\n",
        "        \"\"\"\n",
        "        The Target defines the domain-specific interface used by the client code.\n",
        "        \"\"\"\n",
        "        def request(self) -> str:\n",
        "            return \"Target: The default target's behavior.\"\n",
        "\n",
        "    class VisionModelReturn:\n",
        "        \"\"\"\n",
        "        The Adaptee contains some useful behavior, but its interface is incompatible\n",
        "        with the existing client code. The Adaptee needs some adaptation before the\n",
        "        client code can use it.\n",
        "        \"\"\"\n",
        "        def __init__(self, chip_name, data) -> None:\n",
        "            self.chip_name = chip_name\n",
        "            self.data = data\n",
        "\n",
        "        def specific_request(self) -> str:\n",
        "            return self.data\n",
        "\n",
        "    class Adapter(NormalAoaiGeolocateInput):\n",
        "        \"\"\"\n",
        "        The Adapter makes the Adaptee's interface compatible with the Target's\n",
        "        interface via composition.\n",
        "        \"\"\"\n",
        "        def __init__(self, visionmodelreturn: VisionModelReturn) -> None:\n",
        "            self.visionmodelreturn = visionmodelreturn\n",
        "            #print('Input \\n%s' %  json.dumps(self.visionmodelreturn.data, indent = 4, sort_keys=True))\n",
        "        \n",
        "        @staticmethod\n",
        "        def get_full_image_cs(img_tile_size, tile_overlap, chip_annot, offset):\n",
        "            full_img_annot = {}\n",
        "            full_img_annot['bottomX'] = math.floor(float(chip_annot['bottomX']) * float(img_tile_size)) + ((int(offset['x']) - 1) * (int(img_tile_size) - int(tile_overlap)))\n",
        "            full_img_annot['topX'] = math.floor(float(chip_annot['topX']) * float(img_tile_size)) + ((int(offset['x']) - 1) * (int(img_tile_size) - int(tile_overlap)))\n",
        "            full_img_annot['bottomY'] = math.floor(float(chip_annot['bottomY']) * float(img_tile_size)) + ((int(offset['y']) - 1) * (int(img_tile_size) - int(tile_overlap)))\n",
        "            full_img_annot['topY'] = math.floor(float(chip_annot['topY']) * float(img_tile_size)) + ((int(offset['y']) - 1) * (int(img_tile_size) - int(tile_overlap)))\n",
        "            #print(full_img_annot)\n",
        "            return full_img_annot\n",
        "\n",
        "        def request(self) -> str:\n",
        "            \"\"\"\n",
        "            multiply box values coordinates that are in percentage image cs by size of image chip to get box values coordinates that are in pixels image cs \n",
        "            add chip offset from origin to each \n",
        "            \"\"\"\n",
        "            tile_size = config[\"tile_options\"][\"tile_size\"]\n",
        "            #get the chip offset from origin that's written in the chip filename\n",
        "            chip_offset = {}\n",
        "            chip_offset['y'], chip_offset['x'] = (self.visionmodelreturn.chip_name.split('_'))[-2:]\n",
        "            chip_offset['x'] = os.path.splitext(chip_offset['x'])[0]\n",
        "            print(f\"tile size: {tile_size}  chip offsets: {int(chip_offset['x'])}, {int(chip_offset['y'])}\")\n",
        "            self.adapted_returns = copy.deepcopy(self.visionmodelreturn.data)\n",
        "\n",
        "            for idx, detection in enumerate(self.visionmodelreturn.data['values'][0]['data']['boxes']):\n",
        "                print('Detection %d before: %s' %  (idx, json.dumps(self.visionmodelreturn.data['values'][0]['data']['boxes'][idx], indent = 4, sort_keys=True)))\n",
        "                full_image_cs_annot = Adapter.get_full_image_cs(img_tile_size=tile_size, tile_overlap=config['tile_options']['overlap'], chip_annot=detection['box'], offset=chip_offset)\n",
        "                self.adapted_returns['values'][0]['data']['boxes'][idx]['box']['bottomX'] = full_image_cs_annot['bottomX']\n",
        "                self.adapted_returns['values'][0]['data']['boxes'][idx]['box']['topX'] = full_image_cs_annot['topX']\n",
        "                self.adapted_returns['values'][0]['data']['boxes'][idx]['box']['bottomY'] = full_image_cs_annot['bottomY']\n",
        "                self.adapted_returns['values'][0]['data']['boxes'][idx]['box']['topY'] = full_image_cs_annot['topY']\n",
        "                print('Detection %d after : %s' %  (idx, json.dumps(self.adapted_returns['values'][0]['data']['boxes'][idx], indent = 4, sort_keys=True)))\n",
        "\n",
        "            return self.adapted_returns\n",
        "\n",
        "    def download_file(client, source, dest):\n",
        "        '''\n",
        "        Download a single file to a path on the local filesystem\n",
        "        '''\n",
        "        # dest is a directory if ending with '/' or '.', otherwise it's a file\n",
        "        if dest.endswith('.'):\n",
        "            dest += '/'\n",
        "        blob_dest = dest + os.path.basename(source) if dest.endswith('/') else dest\n",
        "\n",
        "        os.makedirs(os.path.dirname(blob_dest), exist_ok=True)\n",
        "        bc = client.get_blob_client(blob=source)\n",
        "        with open(blob_dest, 'wb') as file:\n",
        "            data = bc.download_blob()\n",
        "            file.write(data.readall())\n",
        "    \n",
        "    def upload_file(client, source, dest):\n",
        "        '''\n",
        "        Upload a single file to a path inside the container\n",
        "        '''\n",
        "        with open(source, 'rb') as data:\n",
        "            client.upload_blob(name=dest, data=data, overwrite=True)\n",
        "\n",
        "    #init access and files\n",
        "    blob_service_client = BlobServiceClient(account_url=f'https://{blob_account_name}.blob.{azure_storage_domain}/', credential=storage_account_key)\n",
        "    container_client = blob_service_client.get_container_client(output_container)\n",
        "\n",
        "    #using converted_bounding_boxes for big picture image \n",
        "    converted_bounding_boxes = {}\n",
        "    #using big_list_of_bbs to write to blob and use for image cs to lat long cs\n",
        "    big_list_of_bbs = []\n",
        "\n",
        "    #for each chip that we ran ship inference\n",
        "    for chip_name, data in chips_and_chip_bounding_boxes.items():\n",
        "        vision_return = VisionModelReturn(chip_name,data)\n",
        "        converted_bounding_boxes[chip_name] = Adapter(vision_return)\n",
        "        converted_bounding_boxes[chip_name].request()\n",
        "        logger.debug(f\"chip:%s \\ndata:\\n%s\" % (chip_name,json.dumps(converted_bounding_boxes[chip_name].adapted_returns['values'][0]['data']['boxes'], indent = 4, sort_keys=True)))\n",
        "\n",
        "        big_list_of_bbs += converted_bounding_boxes[chip_name].adapted_returns['values'][0]['data']['boxes']\n",
        "        #print(f\"box data:\\n%s\" % (json.dumps(big_list_of_bbs, indent = 4, sort_keys=True)))\n",
        "\n",
        "    #create a file to write full image cs bounding boxes as json\n",
        "    bb_local_json_name = 'bb_' + os.path.splitext(os.path.split(image_file_path)[1])[0]+'.json'\n",
        "    bb_remote_json_path = f'{output_path}/{bb_local_json_name}'\n",
        "    with open(bb_local_json_name, 'w+',encoding=\"utf-8\") as file:\n",
        "        json.dump(big_list_of_bbs, file)\n",
        "    try:\n",
        "        upload_file(container_client,bb_local_json_name, bb_remote_json_path)\n",
        "    except Exception as e:\n",
        "        logger.error(f'Other error occurred: {e}')\n",
        "    else:\n",
        "         logger.info(f'Success. Upload to blob: {bb_remote_json_path}')\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f'get bounding boxes in format that easily compares to AIS'):\n",
        "\n",
        "    def call_bb_translate(gdal_endpoint,bb_translate_metadata,api_key):\n",
        "        resp = \"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                # Request headers\n",
        "                \"Content-Type\": \"application/json\",\n",
        "                \"Gdal-Subscription-Key\": api_key,\n",
        "                \"KEY\": api_key\n",
        "            }\n",
        "            body = bb_translate_metadata\n",
        "            url = f\"{gdal_endpoint}/bb_translate/\"\n",
        "            #print(url, body, headers)\n",
        "            resp = requests.post(url=url, json=body, headers=headers)\n",
        "            #result_response = resp.json()\n",
        "            #print(json.dumps(result_response, indent=4, sort_keys=True))\n",
        "        except Exception as e:\n",
        "            logger.error('Exception', e)\n",
        "        return resp\n",
        "\n",
        "    #init access and files\n",
        "    blob_service_client = BlobServiceClient(account_url=f'https://{blob_account_name}.blob.{azure_storage_domain}/', credential=storage_account_key)\n",
        "    container_client = blob_service_client.get_container_client(output_container)\n",
        "    gdal_host_url = config['gdal_host']['app_url']\n",
        "    \n",
        "    bb_local_json_name = 'bb_' + os.path.splitext(image_file_path)[0]+'.json'\n",
        "    bb_remote_json_path = os.path.splitext(image_file_path)[0] + f'/{bb_local_json_name}'\n",
        "    bb_local_json_path = f'full_img/{bb_local_json_name}'\n",
        "    proj_bb_local_json_name = 'proj_' + bb_local_json_name\n",
        "    proj_bb_remote_json_path = os.path.splitext(image_file_path)[0] + f'/{proj_bb_local_json_name}'\n",
        "\n",
        "    in_img_blob_sas_tkn = generate_blob_sas(account_name=blob_account_name, \n",
        "                                container_name=input_container,\n",
        "                                blob_name=image_file_path,\n",
        "                                account_key=storage_account_key,\n",
        "                                permission=BlobSasPermissions(read=True),\n",
        "                                expiry=datetime.utcnow() + timedelta(hours=1))\n",
        "\n",
        "    in_bb_blob_sas_tkn = generate_blob_sas(account_name=blob_account_name, \n",
        "                              container_name=output_container,\n",
        "                              blob_name=bb_remote_json_path,\n",
        "                              account_key=storage_account_key,\n",
        "                              permission=BlobSasPermissions(read=True),\n",
        "                              expiry=datetime.utcnow() + timedelta(hours=1))\n",
        "    out_bb_blob_sas_tkn = generate_blob_sas(account_name=blob_account_name, \n",
        "                                container_name=output_container,\n",
        "                                blob_name=proj_bb_remote_json_path,\n",
        "                                account_key=storage_account_key,\n",
        "                                permission=BlobSasPermissions(write=True),\n",
        "                                expiry=datetime.utcnow() + timedelta(hours=1))\n",
        "\n",
        "    in_img_metadata = {\n",
        "        \"blob_acct\": blob_account_name,\n",
        "        \"sas_token\": in_blob_sas_tkn,\n",
        "        \"container\": input_container,\n",
        "        \"blob_path\": image_file_path\n",
        "    }\n",
        "    in_bb_metadata = {\n",
        "        \"blob_acct\": blob_account_name,\n",
        "        \"sas_token\": in_bb_blob_sas_tkn,\n",
        "        \"container\": output_container,\n",
        "        \"blob_path\": bb_remote_json_path\n",
        "    }\n",
        "    out_bb_metadata = {\n",
        "        \"blob_acct\": blob_account_name,\n",
        "        \"sas_token\": out_bb_blob_sas_tkn,\n",
        "        \"container\": output_container,\n",
        "        \"blob_path\": proj_bb_remote_json_path\n",
        "    }\n",
        "\n",
        "    gdal_bb_translate = {\n",
        "        \"in_img\":in_img_metadata,\n",
        "        \"in_bbs\":in_bb_metadata,\n",
        "        \"out_bbs\":out_bb_metadata\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        #api auth key needs to match API_KEY in ship_anomaly_detection/gdal_server.py\n",
        "        #translate bbs\n",
        "        bb_translate_resp = call_bb_translate(gdal_host_url, gdal_bb_translate, config['gdal_host']['key'])\n",
        "        \n",
        "        bb_translate_resp.raise_for_status()\n",
        "    except HTTPError as http_err:\n",
        "        logger.error(f'HTTP error occurred: {http_err}')\n",
        "    except Exception as err:\n",
        "        logger.error(f'Other error occurred: {err}')\n",
        "    else:\n",
        "        logger.info(f'Success. Response: {bb_translate_resp.status_code} - {bb_translate_resp.text}')\n",
        "        gdal_output = json.loads(bb_translate_resp.text)\n",
        "    \n",
        "\n",
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
