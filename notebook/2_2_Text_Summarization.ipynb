{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "     \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "     \"spark.dynamicAllocation.enabled\": true,\n",
        "     \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "     \"spark.dynamicAllocation.maxExecutors\": 4\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "documents_contents_tbl_name = ''\n",
        "batch_root = ''\n",
        "batch_num = ''\n",
        "model_name = ''  # This is the text summarisation model we will use: currently either \"google/pegasus-xsum\" || \"google/pegasus-cnn_dailymail\"\n",
        "output_col_name = ''\n",
        "error_col_name = ''\n",
        "output_tbl_name = ''\n",
        "file_system = ''\n",
        "blob_account_name = ''\n",
        "azure_storage_domain = ''\n",
        "minted_tables_output_path = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyodbc\r\n",
        "database = 'minted'   \r\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\r\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\r\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\r\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseServerlessSQLEndpoint\")\r\n",
        "display_dataframes = False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'documents_contents_tbl_name': documents_contents_tbl_name,\n",
        "    'batch_root': batch_root,\n",
        "    'batch_num': batch_num,\n",
        "    'model_name': model_name,\n",
        "    'output_col_name': output_col_name,\n",
        "    'error_col_name': error_col_name,\n",
        "    'output_tbl_name': output_tbl_name,\n",
        "    'file_system': file_system,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import json\n",
        "import os\n",
        "from types import SimpleNamespace\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType, StructField, StructType\n",
        "\n",
        "# Initialise session and config\n",
        "sc = spark.sparkContext\n",
        "spark = SparkSession.builder.appName(f\"TextProcessing {mssparkutils.runtime.context}\").getOrCreate()\n",
        "\n",
        "def read_batch_config(batch_root: str):\n",
        "    \"\"\"\n",
        "    We read the config file using the Java File System API as we do not need to let multiple nodes read individual lines and join it\n",
        "    all back together again\n",
        "    \"\"\"\n",
        "    # Change our file system from 'synapse' to 'input'\n",
        "    sc._jsc.hadoopConfiguration().set(\"fs.defaultFS\", file_system)\n",
        "\n",
        "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
        "    config_path = sc._jvm.org.apache.hadoop.fs.Path(f'{batch_root}/config.json')\n",
        "\n",
        "    # If we don't have a batch config, copy the global one.\n",
        "    if fs.exists(config_path) != True:\n",
        "        logger.error(f'{config_path} not found.')\n",
        "\n",
        "    # Open our file directly rather than through spark\n",
        "    input_stream = fs.open(config_path)  # FSDataInputStream\n",
        "\n",
        "    config_string = sc._jvm.java.io.BufferedReader(\n",
        "        sc._jvm.java.io.InputStreamReader(input_stream, sc._jvm.java.nio.charset.StandardCharsets.UTF_8)\n",
        "        ).lines().collect(sc._jvm.java.util.stream.Collectors.joining(\"\\n\"))\n",
        "\n",
        "    # Load it into json    \n",
        "    return json.loads(''.join(config_string), object_hook=lambda dictionary: SimpleNamespace(**dictionary))\n",
        "\n",
        "with tracer.span(name=f\"Load config: {mssparkutils.runtime.context['notebookname']}\"):\n",
        "    try:\n",
        "        config = read_batch_config(batch_root)\n",
        "    except Exception as e:\n",
        "        logger.exception(e)\n",
        "        raise e\n",
        "\n",
        "    # Set log level\n",
        "    if config.log_level == \"INFO\":\n",
        "        logger.setLevel(logging.INFO)\n",
        "    else:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "        config.log_level = \"ERROR\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "max_length = int(config.summarization.max_length) # Max length for summary\n",
        "num_beams = int(config.summarization.num_beams) # Number of beams to use for beam search - read from global config and passed in\n",
        "skip_special_tokens = bool(config.summarization.skip_special_tokens) # HF Skip special tokens - read from global config and passed in\n",
        "clean_up_tokenization_spaces = bool(config.summarization.clean_up_tokenization_spaces)  # Use HF to clean tokenization - read from global config and passed in\n",
        "\n",
        "summarisation_config = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'max_length': max_length,\n",
        "    'num_beams': num_beams,\n",
        "    'skip_special_tokens': skip_special_tokens,\n",
        "    'clean_up_tokenization_spaces': clean_up_tokenization_spaces,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: RUN_CONFIG\", extra=summarisation_config)\n",
        "\n",
        "class Models:\n",
        "\n",
        "    def __init__(self, summarizer_model, tokenizer=None, model=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "\n",
        "    def load_summarisation_model(self, summarizer_model):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(summarizer_model) \n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(summarizer_model)\n",
        "\n",
        "\n",
        "def summarize(text) -> (str, str):\n",
        "    try:\n",
        "        # Creates a summary from the input text\n",
        "        token_inputs = summarizer_model.tokenizer([text], max_length=max_length, return_tensors='pt', truncation=True)\n",
        "        summary_ids = summarizer_model.model.generate(token_inputs['input_ids'], num_beams=num_beams,\n",
        "                                                early_stopping=True) # Not using early stopping\n",
        "        summary = [summarizer_model.tokenizer.decode(g, skip_special_tokens=skip_special_tokens, clean_up_tokenization_spaces=clean_up_tokenization_spaces, truncation=True)\n",
        "                for g in summary_ids]\n",
        "        return summary, \"\"\n",
        "    except Exception as e:\n",
        "        logger.exception(e)\n",
        "        return \"\", str(e)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Read the documents contents table'):\n",
        "    df = spark.read.parquet(minted_tables_output_path + documents_contents_tbl_name).select('file_path','text_content_target_lang').repartition(320)\n",
        "\n",
        "with tracer.span(name=f'Mount {model_name} summarisation model'):\n",
        "    try:\n",
        "        # Read the first character of the Model config.json to see if it's there. Otherwise download\n",
        "        mssparkutils.fs.head(f'abfss://synapse@{blob_account_name}.dfs.{azure_storage_domain}/models/{model_name}/config.json', 1)\n",
        "        mount_point = f'/mnt'\n",
        "        jobId = mssparkutils.env.getJobId()\n",
        "        linkedStorageName = f'{mssparkutils.env.getWorkspaceName()}-WorkspaceDefaultStorage'\n",
        "\n",
        "        # mssparkutils.fs.unmount(mount_point)\n",
        "        mssparkutils.fs.mount( \n",
        "            f'abfss://synapse@{blob_account_name}.dfs.{azure_storage_domain}/', \n",
        "            mount_point, \n",
        "            {'linkedService':f'{linkedStorageName}'} \n",
        "        )\n",
        "\n",
        "        # Please note the differences with the synfs protocol\n",
        "        # https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/synapse-file-mount-api#how-to-access-files-under-mount-point-via-local-file-system-api\n",
        "        model_location = f'/synfs/{jobId}{mount_point}/models/{model_name}/'\n",
        "        logger.info(f'Using {model_name} model from {model_location}.')\n",
        "        model_name = model_location\n",
        "    except Exception as e:\n",
        "        logger.info(f'Using {model_name} model from HuggingFace.')\n",
        "\n",
        "with tracer.span(name=f'Download and instantiate model {model_name}'):\n",
        "    summarizer_model = Models(summarizer_model=None)\n",
        "    summarizer_model.load_summarisation_model(model_name)\n",
        "\n",
        "with tracer.span(name=f'Run summarisation {model_name}'):\n",
        "    udf_summarize = udf(summarize, StructType([StructField(\"summary_text\", StringType()), StructField(\"error\", StringType())]))\n",
        "    summarized_result = udf_summarize(df.text_content_target_lang)\n",
        "\n",
        "    df = (df\n",
        "        .withColumn(output_col_name, summarized_result.summary_text)\n",
        "        .withColumn(error_col_name, summarized_result.error)\n",
        "        .drop(df.text_content_target_lang)\n",
        "    )\n",
        "\n",
        "if display_dataframes:\n",
        "    df.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f'Persist summarisation as table'):\n",
        "    summarized_text_tbl_name = f'{batch_num}_{output_tbl_name}'\n",
        "    \n",
        "    df.write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{summarized_text_tbl_name}')\n",
        "\n",
        "    df_sql_command = f\"\"\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{summarized_text_tbl_name}') \n",
        "    CREATE EXTERNAL TABLE [{summarized_text_tbl_name}] \n",
        "    (\n",
        "        [file_path] nvarchar(4000), \n",
        "        [{output_col_name}] nvarchar(4000), \n",
        "        [{error_col_name}] nvarchar(4000)\n",
        "    ) WITH (\n",
        "            LOCATION = 'minted_tables/{summarized_text_tbl_name}/**', \n",
        "            DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], \n",
        "            FILE_FORMAT = [SynapseParquetFormat]\n",
        "            )\"\"\"\n",
        "\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "        with conn.cursor() as cursor:\n",
        "            cursor.execute(df_sql_command)\n",
        "\n",
        "# return name of new table\n",
        "output = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'summarized_text_tbl_name': summarized_text_tbl_name,\n",
        "    'file_system': file_system,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "\n",
        "# Return the object to the pipeline\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\n",
        "mssparkutils.notebook.exit(output['custom_dimensions'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
