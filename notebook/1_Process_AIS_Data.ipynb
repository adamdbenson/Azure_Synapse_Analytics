{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "blob_account_name = ''\n",
        "input_container = ''\n",
        "output_container = ''\n",
        "image_file_path = ''\n",
        "ais_file_path = ''\n",
        "azure_storage_domain = ''\n",
        "config_path = ''\n",
        "kml_path = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from azure.storage.blob import generate_blob_sas, BlobSasPermissions, generate_container_sas, ContainerSasPermissions, BlobClient\n",
        "from datetime import datetime, timedelta\n",
        "import fsspec\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "from py4j.protocol import Py4JJavaError\n",
        "from pyspark.sql.functions import abs\n",
        "from pyspark.sql.functions import col, date_trunc\n",
        "from pyspark.sql.functions import date_format\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "import requests\n",
        "import xml.dom.minidom\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Setup Logger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Set paths for satellite image and configuration\n",
        "    Used for retrieving the satellite image timestamp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "image_path = f'https://{blob_account_name}.blob.{azure_storage_domain}/{input_container}/'\n",
        "image_path_abfss = f'abfss://{input_container}@{blob_account_name}.dfs.{azure_storage_domain}/'\n",
        "image_folder = os.path.dirname(image_file_path)\n",
        "image_root = f\"{image_path}{image_folder}\"\n",
        "image_root_abfss = f'{image_path_abfss}{image_folder}'\n",
        "global_config_path = f'abfss://configuration@{blob_account_name}.dfs.{azure_storage_domain}/anomdet.config.global.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Load the satellite configuration file\n",
        "    Used for retrieving the satellite image timestamp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create a spark session\n",
        "spark = SparkSession.builder.appName(f\"AnomalyDetection {mssparkutils.runtime.context}\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Function to enssure config file exists\n",
        "def prepare_config(image_root: str, global_config_path: str):\n",
        "    \"\"\"\n",
        "    This method makes sure that a config is availabile in the batch root.\n",
        "    If a config file isn't already there, it is copied over form global_config_path.\n",
        "    If there is no config under global_config_path, this function will crash (indicating an error in pipeline set up.)\n",
        "    \"\"\"\n",
        "    image_config_path = f'{image_root_abfss}/anomdet.config.json'\n",
        "    try: \n",
        "        mssparkutils.fs.head(image_config_path)\n",
        "    except Py4JJavaError as e:\n",
        "        if 'java.io.FileNotFoundException' in str(e):\n",
        "            # File doesn't exist, copying over the global config path\n",
        "            mssparkutils.fs.cp(global_config_path, image_config_path)    \n",
        "        else:\n",
        "            raise e\n",
        "\n",
        "# Prepare and load the configuration file\n",
        "prepare_config(image_root=image_root, global_config_path=global_config_path)\n",
        "config = json.loads(''.join(sc.textFile(f'{image_path_abfss}/{config_path}').collect()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Setup the POST body and URL request to the gdal_container img_info endpoint \n",
        "    Used for retrieving the satellite image timestamp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "storage_account_key = mssparkutils.credentials.getSecretWithLS('keyvault', 'StorageAccountKey')\n",
        "in_blob_sas_tkn = generate_blob_sas(account_name=blob_account_name, \n",
        "                            container_name=input_container,\n",
        "                            blob_name=image_file_path,\n",
        "                            account_key=storage_account_key,\n",
        "                            permission=BlobSasPermissions(read=True),\n",
        "                            expiry=datetime.utcnow() + timedelta(hours=1))\n",
        "\n",
        "\n",
        "in_img_metadata = {\n",
        "    \"blob_acct\": blob_account_name,\n",
        "    \"container\": input_container,\n",
        "    \"blob_path\": image_file_path,\n",
        "    \"sas_token\": in_blob_sas_tkn\n",
        "}\n",
        "info_config = { \"format\": \"json\"}\n",
        "\n",
        "# POST Body\n",
        "gdal_info = { \n",
        "    \"info_options\": info_config,\n",
        "    \"in_img\": in_img_metadata\n",
        "}\n",
        "\n",
        "#Host URL\n",
        "gdal_host_url = config['gdal_host']['app_url']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Call the gdal_container img_info HTTP POST endpoint to retrieve the satellite metadata\n",
        "    Used for retrieving the satellite image timestamp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Function to call the img_info endpoint\n",
        "def call_info(gdal_endpoint,info_metadata,api_key):\n",
        "    resp = \"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            # Request headers\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"Gdal-Subscription-Key\": api_key,\n",
        "            \"KEY\": api_key\n",
        "        }\n",
        "        body = info_metadata\n",
        "        url = f\"{gdal_endpoint}/img_info/\"\n",
        "        resp = requests.post(url=url, json=body, headers=headers)\n",
        "        result_response = resp.json()\n",
        "        print(json.dumps(result_response, indent=4, sort_keys=True))\n",
        "    except Exception as e:\n",
        "        logger.error('Exception', e)\n",
        "    return resp\n",
        "\n",
        "# Get the satellite image metadata\n",
        "info_resp = call_info(gdal_host_url,gdal_info, config['gdal_host']['key'])\n",
        "logger.info(json.dumps(info_resp.json(), indent=4, sort_keys=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Get the timestamp from the Sattelite Image and convert to a PySpark Timestamp type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "info_json = info_resp.json()\n",
        "image_timestamp = info_json['img_metadata']['metadata']['']['TIFFTAG_DATETIME']\n",
        "\n",
        "# Convert the string time to a spark dataframe \n",
        "image_time_df = spark.createDataFrame([image_timestamp], \"string\").toDF('image_timestamp')\n",
        "image_time_df.show(truncate=False)\n",
        "\n",
        "# Convert the string time to a timestamp data type \n",
        "image_time_df = image_time_df.withColumn(\"image_timestamp\", to_timestamp(col(\"image_timestamp\"),\"yyyy-MM-dd'T'HH:mm:ss.SSSSSSSSS'Z'\"))\n",
        "image_time_df.show(truncate=False)\n",
        "# Get the timestamp object\n",
        "image_time = image_time_df.collect()[0].image_timestamp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Process and filter the AIS data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# abfss URL\n",
        "ais_csv_file_url = f'abfss://{input_container}@{blob_account_name}.dfs.{azure_storage_domain}/{ais_file_path}'\n",
        "\n",
        "# Read AIS data from input container\n",
        "ais_df = spark.read.options(inferSchema='False',header=True,delimiter=';').csv(ais_csv_file_url)\n",
        "\n",
        "# Remove duplicates\n",
        "ais_df = ais_df.distinct()\n",
        "\n",
        "# # Find the time difference in minutes from the Satellite image\n",
        "# ais_df = ais_df.withColumn('TIMESTAMP', to_timestamp(col('TIMESTAMP'))) \\\n",
        "#                .withColumn('DIFFINMIN', abs((col('TIMESTAMP').cast('long') - lit(image_time).cast('long'))/60))\n",
        "\n",
        "# # Filter the AIS data so that it includes times with +-10 min of sattelite image\n",
        "# ais_df = ais_df.filter((ais_df['DIFFINMIN'] <= 10))\n",
        "# ais_df = ais_df.sort('SHIPNAME')\n",
        "\n",
        "# # Filter the AIS data with time closest to time of sattelite image\n",
        "# ais_df = ais_df.groupBy(['SHIPNAME']).min('DIFFINMIN').withColumnRenamed('min(DIFFINMIN)', 'DIFFINMIN').join(ais_df, ['SHIPNAME','DIFFINMIN'])\n",
        "# ais_df = ais_df.sort('SHIPNAME')\n",
        "# ais_df = ais_df.drop('DIFFINMIN')\n",
        "# ais_df = ais_df.fillna('None')\n",
        "\n",
        "ais_df = ais_df.withColumn('TIMESTAMP', date_format(col('TIMESTAMP'), \"yyyy-MM-dd HH:mm:ss\"))\n",
        "#ais_df = ais_df.withColumn('date', date_trunc(\"yyyy-MM-dd HH:mm:ss\", col('TIMESTAMP')))\n",
        "ais_df.show(10,False)\n",
        "ais_df.printSchema() # used to check the datatype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Functions to convert AIS data to KML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def create_placemark(klm_document, row, name_description, extended_data, lat_long):\n",
        "\n",
        "    # <PlaceMark> \n",
        "    placemark_element = klm_document.createElement('Placemark')\n",
        "\n",
        "    # <name>\n",
        "    name_element = klm_document.createElement('name')\n",
        "    try:\n",
        "        name_element.appendChild(klm_document.createTextNode(row[name_description[0]]))\n",
        "    except Exception as e:\n",
        "        logger.info(\"failed creating placemark\")\n",
        "\n",
        "    placemark_element.appendChild(name_element)\n",
        "\n",
        "    # <description> \n",
        "    description_element = klm_document.createElement('description')\n",
        "    try:\n",
        "        description_element.appendChild(klm_document.createTextNode(row[name_description[1]]))\n",
        "    except Exception as e:\n",
        "        logger.info(\"\\t\\t\\t 111111\")\n",
        "    placemark_element.appendChild(description_element)\n",
        "\n",
        "    # <ExtendedData> \n",
        "    ext_element = klm_document.createElement('ExtendedData')\n",
        "    placemark_element.appendChild(ext_element)\n",
        "    \n",
        "    # <Data name=\"NAME\"> <value> VALUE </value> </Data>\n",
        "    for key in extended_data:\n",
        "        data_element = klm_document.createElement('Data')  \n",
        "        data_element.setAttribute('name', key)      \n",
        "        value_element = klm_document.createElement('value')\n",
        "        data_element.appendChild(value_element)\n",
        "        try:\n",
        "            value_text = klm_document.createTextNode(str(row[key]))\n",
        "        except Exception as e:\n",
        "            logger.info(\"failed creating key in extended_data\")\n",
        "        value_element.appendChild(value_text)\n",
        "        ext_element.appendChild(data_element)\n",
        "\n",
        "    # <Point> <coordinates> lat,long </coordinates> <Point>\n",
        "    point_element = klm_document.createElement('Point')\n",
        "    placemark_element.appendChild(point_element)\n",
        "    coor_element = klm_document.createElement('coordinates')\n",
        "    try:\n",
        "        coor_element.appendChild(klm_document.createTextNode(row[lat_long[0]]+','+row[lat_long[1]]))\n",
        "    except Exception as e:\n",
        "        logger.info(\"failed appending child to document\")\n",
        "    point_element.appendChild(coor_element)\n",
        "    \n",
        "    return placemark_element\n",
        "\n",
        "def convert_ais_csv_to_kml(ais_df, file_name, kml_path, name_description, extended_data, lat_long):\n",
        "\n",
        "    # Create a kml document\n",
        "    klm_document = xml.dom.minidom.Document()\n",
        "\n",
        "    # Set the XML name space values unders the <kml> tag\n",
        "    kml_element = klm_document.createElementNS('http://www.opengis.net/kml/2.2','kml') \n",
        "    kml_element.setAttribute('xmlns','http://www.opengis.net/kml/2.2')\n",
        "    kml_element = klm_document.appendChild(kml_element)\n",
        "    # Google name space\n",
        "    kml_element.setAttribute('xmlns:gx','http://www.google.com/kml/ext/2.2')\n",
        "    kml_element = klm_document.appendChild(kml_element)\n",
        "    # Open GIS name space\n",
        "    kml_element.setAttribute('xmlns:kml','http://www.opengis.net/kml/2.2')\n",
        "    kml_element = klm_document.appendChild(kml_element)\n",
        "    # Atom name space\n",
        "    kml_element.setAttribute('xmlns:atom','http://www.w3.org/2005/Atom')\n",
        "    kml_element = klm_document.appendChild(kml_element)\n",
        "\n",
        "    # Add the <Document> tag\n",
        "    document_element = klm_document.createElement('Document')\n",
        "    document_element = kml_element.appendChild(document_element)\n",
        "\n",
        "    # Create a <PlaceMark> tag for each row\n",
        "    for row in ais_df.rdd.collect():\n",
        "        placemark_element = create_placemark(klm_document, row, name_description, extended_data, lat_long)\n",
        "        document_element.appendChild(placemark_element)\n",
        "\n",
        "    out_cont_sas_tkn = generate_container_sas(account_name=blob_account_name, \n",
        "                            container_name=output_container,\n",
        "                            account_key=storage_account_key,\n",
        "                            permission=ContainerSasPermissions(read=True, list=True, write=True, add=True, create=True, update=True),\n",
        "                            expiry=datetime.utcnow() + timedelta(hours=1))\n",
        "    connection_string = f'DefaultEndpointsProtocol=https;AccountName={blob_account_name};AccountKey={storage_account_key};EndpointSuffix={azure_storage_domain}'\n",
        "    blob = BlobClient.from_connection_string(conn_str=connection_string, container_name=f'{output_container}', blob_name=f'{kml_path}', credential=out_cont_sas_tkn)\n",
        "\n",
        "    #Write XML to file \n",
        "    with open(file_name, mode='w') as f:\n",
        "        f.write(klm_document.toxml())\n",
        "    \n",
        "    # Write xml to output_container\n",
        "    with open(file_name, \"rb\") as data:\n",
        "        blob.upload_blob(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Convert AIS data to KML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Theses are headers from the AIS CSV file\n",
        "name_description = ['SHIPNAME','TYPE_NAME']\n",
        "extended_data = ['IMO','MMSI', 'LENGTH', 'WIDTH', 'SPEED', 'STATUS', 'COURSE', 'HEADING', 'TIMESTAMP'] \n",
        "lat_long = ['LAT','LON']\n",
        "\n",
        "# Convert AIS csv data to KML\n",
        "convert_ais_csv_to_kml(ais_df, ais_file_path.split('.csv')[0], kml_path, name_description, extended_data, lat_long)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
