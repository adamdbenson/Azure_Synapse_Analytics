{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "     \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "     \"spark.dynamicAllocation.enabled\": true,\n",
        "     \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "     \"spark.dynamicAllocation.maxExecutors\": 8\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "enriched_images_tbl_name = ''\n",
        "clustered_images_tbl_name_1 = ''\n",
        "clustered_images_tbl_name_2 = ''\n",
        "clustered_images_tbl_name_3 = ''\n",
        "clustered_multimodal_tbl_name = ''\n",
        "image_contents_tbl_name = ''\n",
        "batch_root = ''\n",
        "batch_num = ''\n",
        "batch_description = ''\n",
        "input_container=''\n",
        "output_container=''\n",
        "blob_account_name = ''\n",
        "file_system = \"\"\n",
        "azure_storage_domain = ''\n",
        "image_file_count = 0\n",
        "minted_tables_output_path = \"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'image_contents_tbl_name': image_contents_tbl_name,\n",
        "    'enriched_images_tbl_name': enriched_images_tbl_name,\n",
        "    'clustered_images_tbl_name_1': clustered_images_tbl_name_1,\n",
        "    'clustered_images_tbl_name_2': clustered_images_tbl_name_2,\n",
        "    'clustered_images_tbl_name_3': clustered_images_tbl_name_3,\n",
        "    'batch_description': batch_description,\n",
        "    'batch_root': batch_root,\n",
        "    'batch_num': batch_num,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "file_path_col = \"path\"\n",
        "file_name_col = \"file_name\"\n",
        "file_type_col = \"file_type\"\n",
        "image_analysis_col = \"analysis_results\"\n",
        "image_analysis_error_col = \"image_analysis_error\"\n",
        "read_col = \"read_results\"\n",
        "read_error_col = \"read_error\"\n",
        "\n",
        "cluster_path_col = \"original_uri\"\n",
        "cluster_col = \"cluster\"\n",
        "image_expl_col = \"Explanations\"\n",
        "x_col = \"X\"\n",
        "y_col = \"Y\"\n",
        "\n",
        "cluster_col_1 = \"cluster_1\"\n",
        "image_expl_col_1 = \"Explanations_1\"\n",
        "x_col_1 = \"X_1\"\n",
        "y_col_1 = \"Y_1\"\n",
        "\n",
        "cluster_col_2 = \"cluster_2\"\n",
        "image_expl_col_2 = \"Explanations_2\"\n",
        "x_col_2 = \"X_2\"\n",
        "y_col_2 = \"Y_2\"\n",
        "\n",
        "cluster_col_3 = \"cluster_3\"\n",
        "image_expl_col_3 = \"Explanations_3\"\n",
        "x_col_3 = \"X_3\"\n",
        "y_col_3 = \"Y_3\"\n",
        "\n",
        "# For multimodal clustering results\n",
        "cluster_path_col_4 = \"file_path\"\n",
        "summarized_text_xsum_col = \"summarized_text_xsum\"\n",
        "summarized_text_xsum_col_4 = \"summarized_text_xsum_4\"\n",
        "topic_name_col = \"topic_name\"\n",
        "topic_name_col_4 = \"topic_name_4\"\n",
        "cluster_col_4 = \"cluster_4\"\n",
        "x_col_4 = \"X_4\"\n",
        "y_col_4 = \"Y_4\"\n",
        "\n",
        "output_cols = [ \n",
        "    file_name_col, \n",
        "    file_type_col\n",
        "]\n",
        "\n",
        "error_cols = [\n",
        "    image_analysis_error_col\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import uuid\n",
        "import csv\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import StringType, StructType, StructField, MapType\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialise session and config\n",
        "sc = spark.sparkContext\n",
        "spark = SparkSession.builder.appName(f\"ImageProcessing {mssparkutils.runtime.context}\").getOrCreate()\n",
        "\n",
        "def read_batch_config(batch_root: str):\n",
        "    \"\"\"\n",
        "    We read the config file using the Java File System API as we do not need to let multiple nodes read individual lines and join it\n",
        "    all back together again\n",
        "    \"\"\"\n",
        "    # Change our file system from 'synapse' to 'input'\n",
        "    sc._jsc.hadoopConfiguration().set(\"fs.defaultFS\", file_system)\n",
        "\n",
        "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
        "    config_path = sc._jvm.org.apache.hadoop.fs.Path(f'{batch_root}/config.json')\n",
        "\n",
        "    # If we don't have a batch config, copy the global one.\n",
        "    if fs.exists(config_path) != True:\n",
        "        logger.error(f'{config_path} not found.')\n",
        "\n",
        "    # Open our file directly rather than through spark\n",
        "    input_stream = fs.open(config_path)  # FSDataInputStream\n",
        "\n",
        "    config_string = sc._jvm.java.io.BufferedReader(\n",
        "        sc._jvm.java.io.InputStreamReader(input_stream, sc._jvm.java.nio.charset.StandardCharsets.UTF_8)\n",
        "        ).lines().collect(sc._jvm.java.util.stream.Collectors.joining(\"\\n\"))\n",
        "\n",
        "    # Load it into json    \n",
        "    return json.loads(''.join(config_string), object_hook=lambda dictionary: SimpleNamespace(**dictionary))\n",
        "\n",
        "with tracer.span(name=f\"Load config: {mssparkutils.runtime.context['notebookname']}\"):\n",
        "    try:\n",
        "        config = read_batch_config(batch_root)\n",
        "    except Exception as e:\n",
        "        logger.exception(e)\n",
        "        raise e\n",
        "\n",
        "    # Set log level\n",
        "    if config.log_level == \"INFO\":\n",
        "        logger.setLevel(logging.INFO)\n",
        "    else:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "        config.log_level = \"ERROR\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyodbc\r\n",
        "from pyspark.sql.functions import col, current_timestamp, from_json\r\n",
        "# Dedicated and serverless SQL config\r\n",
        "dedicated_database = 'dedicated'\r\n",
        "database = 'minted'   \r\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\r\n",
        "\r\n",
        "# secrets\r\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\r\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\r\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseServerlessSQLEndpoint\")\r\n",
        "dedicated_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseDedicatedSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name=f'Read the dataframe from the given table {image_contents_tbl_name}'):\n",
        "    # Load Dataframes and rename columns \n",
        "    image_contents_df = spark.read.parquet(f'{minted_tables_output_path}{image_contents_tbl_name}')\n",
        "    enriched_images_df = spark.read.parquet(f'{minted_tables_output_path}{enriched_images_tbl_name}')\n",
        "    clustered_images_df_1 = spark.read.parquet(f'{minted_tables_output_path}{clustered_images_tbl_name_1}')\n",
        "    clustered_images_df_2 = (spark.read.parquet(f'{minted_tables_output_path}{clustered_images_tbl_name_2}')\n",
        "        .withColumnRenamed(image_expl_col, image_expl_col_2)\n",
        "        .withColumnRenamed(cluster_col, cluster_col_2)\n",
        "        .withColumnRenamed(x_col, x_col_2)\n",
        "        .withColumnRenamed(y_col, y_col_2)\n",
        "    )\n",
        "    clustered_images_df_3 = (spark.read.parquet(f'{minted_tables_output_path}{clustered_images_tbl_name_3}')\n",
        "        .withColumnRenamed(image_expl_col, image_expl_col_3)\n",
        "        .withColumnRenamed(cluster_col, cluster_col_3)\n",
        "        .withColumnRenamed(x_col, x_col_3)\n",
        "        .withColumnRenamed(y_col, y_col_3)\n",
        "    )\n",
        "    # For multimodal clustering results\n",
        "    clustered_multimodal_df = (spark.read.parquet(f'{minted_tables_output_path}{clustered_multimodal_tbl_name}')\n",
        "        .withColumnRenamed(summarized_text_xsum_col, summarized_text_xsum_col_4)\n",
        "        .withColumnRenamed(topic_name_col, topic_name_col_4)\n",
        "        .withColumnRenamed(cluster_col, cluster_col_4)\n",
        "        .withColumnRenamed(x_col, x_col_4)\n",
        "        .withColumnRenamed(y_col, y_col_4)\n",
        "    )\n",
        "    clustered_multimodal_df = (clustered_multimodal_df\n",
        "        .where(clustered_multimodal_df.type_of_file == 'images')\n",
        "    )\t\n",
        "    clustered_multimodal_df = (clustered_multimodal_df\n",
        "        .drop(clustered_multimodal_df.type_of_file)\n",
        "    )\n",
        "    # Join Dataframes\n",
        "    images_df = (image_contents_df\n",
        "        .join(enriched_images_df, file_path_col, 'left_outer')\n",
        "        .join(clustered_images_df_1, [image_contents_df[file_path_col] == clustered_images_df_1[cluster_path_col]], 'left_outer')\n",
        "        .join(clustered_images_df_2, [image_contents_df[file_path_col] == clustered_images_df_2[cluster_path_col]], 'left_outer')\n",
        "        .join(clustered_images_df_3, [image_contents_df[file_path_col] == clustered_images_df_3[cluster_path_col]], 'left_outer')\n",
        "        .join(clustered_multimodal_df, [image_contents_df[file_path_col] == clustered_multimodal_df[cluster_path_col_4]], 'left_outer')    \n",
        "        # Select specific columns\n",
        "        .select(\n",
        "            image_contents_df.path,\n",
        "            image_contents_df.file_name,\n",
        "            image_contents_df.file_type,\n",
        "            enriched_images_df[image_analysis_col],\n",
        "            enriched_images_df[image_analysis_error_col],\n",
        "            enriched_images_df[read_col],\n",
        "            enriched_images_df[read_error_col],\n",
        "            clustered_images_df_1[image_expl_col],\n",
        "            clustered_images_df_1[cluster_col],\n",
        "            clustered_images_df_1[x_col],\n",
        "            clustered_images_df_1[y_col],\n",
        "            clustered_images_df_2[image_expl_col_2],\n",
        "            clustered_images_df_2[cluster_col_2],\n",
        "            clustered_images_df_2[x_col_2],\n",
        "            clustered_images_df_2[y_col_2],\n",
        "            clustered_images_df_3[image_expl_col_3],\n",
        "            clustered_images_df_3[cluster_col_3],\n",
        "            clustered_images_df_3[x_col_3],\n",
        "            clustered_images_df_3[y_col_3],\n",
        "            clustered_multimodal_df[summarized_text_xsum_col_4], # For multimodal clustering results\n",
        "            clustered_multimodal_df[topic_name_col_4], # For multimodal clustering results\n",
        "            clustered_multimodal_df[cluster_col_4], # For multimodal clustering results\n",
        "            clustered_multimodal_df[x_col_4], # For multimodal clustering results\n",
        "            clustered_multimodal_df[y_col_4] # For multimodal clustering results\n",
        "        )\n",
        "        # Rename file path column\n",
        "        .withColumnRenamed(file_path_col, 'file_path')\n",
        "        .withColumn(\"batch_num\", F.lit(batch_num))\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Persist processed images as SQL table'):\n",
        "    processed_images_tbl_name = f'{batch_num}_processed_images'\n",
        "    images_df.write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{processed_images_tbl_name}')\n",
        "    sql_command_processed_images = f'''\n",
        "        IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{processed_images_tbl_name}') \n",
        "        CREATE EXTERNAL TABLE [{processed_images_tbl_name}] (\n",
        "            [file_path] nvarchar(1000), \n",
        "            [file_name] nvarchar(1000), \n",
        "            [file_type] nvarchar(1000), \n",
        "            [analysis_results] varchar(max),\n",
        "            [image_analysis_error] varchar(max),\n",
        "            [read_results] varchar(max),\n",
        "            [read_error] varchar(max),\n",
        "            [Explanations] nvarchar(4000),\n",
        "            [cluster] bigint,\n",
        "            [X] float,\n",
        "            [Y] float,\n",
        "            [Explanations_2] nvarchar(4000),\n",
        "            [cluster_2] bigint,\n",
        "            [X_2] float,\n",
        "            [Y_2] float,\n",
        "            [Explanations_3] nvarchar(4000),\n",
        "            [cluster_3] bigint,\n",
        "            [X_3] float,\n",
        "            [Y_3] float,\n",
        "            [summarized_text_xsum_4] nvarchar(4000),\n",
        "            [topic_name_4] nvarchar(4000),\n",
        "            [cluster_4] bigint,\n",
        "            [X_4] float,\n",
        "            [Y_4] float,\n",
        "            [batch_num] nvarchar(4000)\n",
        "            \n",
        "        )\n",
        "        WITH (\n",
        "            LOCATION = 'minted_tables/{processed_images_tbl_name}/**', \n",
        "            DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], \n",
        "            FILE_FORMAT = [SynapseParquetFormat]\n",
        "        )\n",
        "    '''\n",
        "\n",
        "    # The batch_num_image_read_errors dataframe is used as a simplified query in the powerBI query\n",
        "    schema = StructType([ \n",
        "        StructField(\"error\",MapType(StringType(),StringType()),True)\n",
        "    ])\n",
        "    batch_num_image_read_errors = images_df.select(from_json(f\"{read_error_col}.response\", schema).alias(\"data\")) \\\n",
        "            .select(\"data.error.message\").groupBy(\"message\").count()\n",
        "    batch_num_image_read_errors = batch_num_image_read_errors.selectExpr(\"message as Errors\", \"count as Count\")\n",
        "    batch_num_image_read_errors = batch_num_image_read_errors.select(\"Errors\", \"Count\")\n",
        "\n",
        "    batch_num_image_read_errors_tbl_name = f'{batch_num}_image_read_errors'\n",
        "    batch_num_image_read_errors.write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{batch_num_image_read_errors_tbl_name}')\n",
        "    \n",
        "    # Create the external table for batch_num_image_read_errors to be used in powerBI\n",
        "    sql_command_batch_num_image_read_errors = f'''\n",
        "        IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{batch_num_image_read_errors_tbl_name}') \n",
        "        CREATE EXTERNAL TABLE [{batch_num_image_read_errors_tbl_name}] (\n",
        "            [Errors] nvarchar(1000), \n",
        "            [Count] bigint, \n",
        "        )\n",
        "        WITH (\n",
        "            LOCATION = 'minted_tables/{batch_num_image_read_errors_tbl_name}/**', \n",
        "            DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], \n",
        "            FILE_FORMAT = [SynapseParquetFormat]\n",
        "        )\n",
        "    '''\n",
        "\n",
        "    # The batch_num_image_analyze_errors dataframe is used as a simplified query in the powerBI query\n",
        "    schema = StructType([ \n",
        "        StructField(\"code\",StringType(),True), \n",
        "        StructField(\"requestId\",StringType(),True), \n",
        "        StructField(\"message\",StringType(),True)\n",
        "    ])\n",
        "    batch_num_image_analyze_errors = images_df.select(from_json(f\"{image_analysis_error_col}.response\", schema).alias(\"data\")) \\\n",
        "            .select(\"data.message\").groupBy(\"message\").count()\n",
        "    batch_num_image_analyze_errors = batch_num_image_analyze_errors.selectExpr(\"message as Errors\", \"count as Count\")\n",
        "    batch_num_image_analyze_errors = batch_num_image_analyze_errors.select(\"Errors\", \"Count\")\n",
        "\n",
        "    batch_num_image_analyze_errors_tbl_name = f'{batch_num}_image_analyze_errors'\n",
        "    batch_num_image_analyze_errors.write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{batch_num_image_analyze_errors_tbl_name}')\n",
        "    \n",
        "    # Create the external table for batch_num_image_analyze_errors to be used in powerBI\n",
        "    sql_command_image_analysis_errors = f'''\n",
        "        IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{batch_num_image_analyze_errors_tbl_name}') \n",
        "        CREATE EXTERNAL TABLE [{batch_num_image_analyze_errors_tbl_name}] (\n",
        "            [Errors] nvarchar(1000), \n",
        "            [Count] bigint, \n",
        "        )\n",
        "        WITH (\n",
        "            LOCATION = 'minted_tables/{batch_num_image_analyze_errors_tbl_name}/**', \n",
        "            DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], \n",
        "            FILE_FORMAT = [SynapseParquetFormat]\n",
        "        )\n",
        "    '''\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "        with conn.cursor() as cursor:\n",
        "            cursor.execute(sql_command_processed_images)\n",
        "            cursor.execute(sql_command_image_analysis_errors)\n",
        "            cursor.execute(sql_command_batch_num_image_read_errors)\n",
        "\n",
        "with tracer.span(name='Persist processed images as json'):\n",
        "    output_path = f'abfss://{output_container}@{blob_account_name}.dfs.{azure_storage_domain}/{batch_num}'\n",
        "\n",
        "    images_df = images_df \\\n",
        "        .withColumn(\"json\", F.to_json(F.struct(col(\"*\"))))\n",
        "\n",
        "    out_lst = images_df.collect()\n",
        "\n",
        "    for row in out_lst:\n",
        "        json_path = f'{output_path}/image_processing_json/{row.file_name}.output.json'\n",
        "        mssparkutils.fs.put(json_path, row.json, overwrite=True)\n",
        "\n",
        "with tracer.span(name='Persist clustering results as CSV and a table'):\n",
        "    clustered_df_1 = images_df.select(col('file_path').alias(cluster_path_col), \\\n",
        "        col(image_expl_col), \\\n",
        "        col(cluster_col), \\\n",
        "        col(x_col), \\\n",
        "        col(y_col))\n",
        "    clustered_df_2 = images_df.select(col('file_path').alias(cluster_path_col), \\\n",
        "        col(image_expl_col_2).alias(image_expl_col), \\\n",
        "        col(cluster_col_2).alias(cluster_col), \\\n",
        "        col(x_col_2).alias(x_col), \\\n",
        "        col(y_col_2).alias(y_col))\n",
        "    clustered_df_3 = images_df.select(col('file_path').alias(cluster_path_col), \\\n",
        "        col(image_expl_col_3).alias(image_expl_col), \\\n",
        "        col(cluster_col_3).alias(cluster_col), \\\n",
        "        col(x_col_3).alias(x_col), \\\n",
        "        col(y_col_3).alias(y_col))\n",
        "    # For multimodal clustering results\n",
        "    clustered_df_4 = images_df.select(col('file_path'), \\\n",
        "        col(summarized_text_xsum_col_4).alias(summarized_text_xsum_col), \\\n",
        "\t\tcol(topic_name_col_4).alias(topic_name_col), \\\n",
        "        col(cluster_col_4).alias(cluster_col), \\\n",
        "        col(x_col_4).alias(x_col), \\\n",
        "        col(y_col_4).alias(y_col))\n",
        "\n",
        "    clustered_df_1 = clustered_df_1 \\\n",
        "        .withColumn(\"batch_num\", F.lit(batch_num))\n",
        "    clustered_df_2 = clustered_df_2 \\\n",
        "        .withColumn(\"batch_num\", F.lit(batch_num))\n",
        "    clustered_df_3 = clustered_df_3 \\\n",
        "        .withColumn(\"batch_num\", F.lit(batch_num))\n",
        "    # For multimodal clustering results\n",
        "    clustered_df_4 = clustered_df_4 \\\n",
        "        .withColumn(\"batch_num\", F.lit(batch_num))\n",
        "\n",
        "    # Save the clustering results type 1 as a single\n",
        "    clustered_df_1.write.mode(\"overwrite\").parquet(f\"{minted_tables_output_path}{batch_num}_clustered_images_report_1\")\n",
        "    clustered_lst_1 = clustered_df_1.collect()\n",
        "\n",
        "    # Save the clustering results type 2 as a single\n",
        "    clustered_df_2.write.mode(\"overwrite\").parquet(f\"{minted_tables_output_path}{batch_num}_clustered_images_report_2\")\n",
        "    clustered_lst_2 = clustered_df_2.collect()\n",
        "\n",
        "    # Save the clustering results type 3 as a single\n",
        "    clustered_df_3.write.mode(\"overwrite\").parquet(f\"{minted_tables_output_path}{batch_num}_clustered_images_report_3\")\n",
        "    clustered_lst_3 = clustered_df_3.collect()\n",
        "\n",
        "    # Save the clustering results type 4 as a single\n",
        "    # Drop NaN values\n",
        "    clustered_df_4 = clustered_df_4.na.drop() \n",
        "    clustered_df_4.write.mode(\"overwrite\").parquet(f\"{minted_tables_output_path}{batch_num}_clustered_images_report_4\")\n",
        "    clustered_lst_4 = clustered_df_4.collect()\n",
        "    \n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "        with conn.cursor() as cursor:\n",
        "            for tbl_name in [f'{batch_num}_clustered_images_report_{i}' for i in [1,2,3]]:\n",
        "                sql_cmd = f'''\n",
        "                    IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{tbl_name}') \n",
        "                    CREATE EXTERNAL TABLE [{tbl_name}] (\n",
        "                        [original_uri] nvarchar(4000),\n",
        "                        [Explanations] nvarchar(max),\n",
        "                        [cluster] bigint,\n",
        "                        [X] float,\n",
        "                        [Y] float,\n",
        "                        [batch_num] nvarchar(4000)\n",
        "                    )\n",
        "                    WITH (\n",
        "                        LOCATION = 'minted_tables/{tbl_name}/**', \n",
        "                        DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], \n",
        "                        FILE_FORMAT = [SynapseParquetFormat]\n",
        "                    )\n",
        "                '''  \n",
        "                            \n",
        "                cursor.execute(sql_cmd)\n",
        "\n",
        "    # For multimodal clustering results\n",
        "    # Save the clustering results type 4 as a single\n",
        "    clustered_images_report_tbl_name_4 = f'{batch_num}_clustered_images_report_4'\n",
        "\n",
        "    clustered_df_sql_command_4 = f\"\"\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{clustered_images_report_tbl_name_4}') \n",
        "    CREATE EXTERNAL TABLE [{clustered_images_report_tbl_name_4}] \n",
        "    (\n",
        "        [file_path] nvarchar(4000), \n",
        "        [summarized_text_xsum] nvarchar(max), \n",
        "\t\t[topic_name] nvarchar(max), \n",
        "        [cluster] bigint,\n",
        "        [X] float,\n",
        "        [Y] float,\n",
        "        [batch_num] nvarchar(4000)\n",
        "    ) WITH (\n",
        "            LOCATION = 'minted_tables/{clustered_images_report_tbl_name_4}/**', \n",
        "            DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], \n",
        "            FILE_FORMAT = [SynapseParquetFormat]\n",
        "            )\"\"\"\n",
        "\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "      with conn.cursor() as cursor:\n",
        "        cursor.execute(clustered_df_sql_command_4)\n",
        "\n",
        "    # Output as CSV\n",
        "    # Saving to a 'local' file first as we can't save directly to the outut container via open()\n",
        "    with open('clusters_1.csv', 'w') as f:\n",
        "        write = csv.writer(f)\n",
        "        write.writerow(clustered_df_1.columns)\n",
        "        write.writerows(clustered_lst_1)\n",
        "\n",
        "    with open('clusters_1.csv', 'r') as f:\n",
        "        contents = f.read() \n",
        "        mssparkutils.fs.put(f'{output_path}/image_processing_clustering/clusters_1.csv', contents, overwrite=True)\n",
        "    \n",
        "    with open('clusters_2.csv', 'w') as f:\n",
        "        write = csv.writer(f)\n",
        "        write.writerow(clustered_df_2.columns)\n",
        "        write.writerows(clustered_lst_2)\n",
        "\n",
        "    with open('clusters_2.csv', 'r') as f:\n",
        "        contents = f.read() \n",
        "        mssparkutils.fs.put(f'{output_path}/image_processing_clustering/clusters_2.csv', contents, overwrite=True)\n",
        "    \n",
        "    with open('clusters_3.csv', 'w') as f:\n",
        "        write = csv.writer(f)\n",
        "        write.writerow(clustered_df_3.columns)\n",
        "        write.writerows(clustered_lst_3)\n",
        "\n",
        "    with open('clusters_3.csv', 'r') as f:\n",
        "        contents = f.read() \n",
        "        mssparkutils.fs.put(f'{output_path}/image_processing_clustering/clusters_3.csv', contents, overwrite=True)\n",
        "\n",
        "    # For multimodal clustering results\n",
        "    with open('clusters_4.csv', 'w') as f:\n",
        "        write = csv.writer(f)\n",
        "        write.writerow(clustered_df_4.columns)\n",
        "        write.writerows(clustered_lst_4)\n",
        "\n",
        "    with open('clusters_4.csv', 'r') as f:\n",
        "        contents = f.read() \n",
        "        mssparkutils.fs.put(f'{output_path}/image_processing_clustering/clusters_4.csv', contents, overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# return name of new table\n",
        "output = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'processed_images_tbl_name': processed_images_tbl_name,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Raise clustering complete event\n",
        "This is also dependent on image enrichments completing, but this is accepted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Prepare the event contents\n",
        "df = images_df\n",
        "\n",
        "with tracer.span(name='preparing contents to send to event grid'):   \n",
        "    from datetime import datetime\n",
        "    now = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S%Z\")    \n",
        "    web_app_uri = config.rule_sets.webapp_uri\n",
        "    subscriber_uri = config.rule_sets.teams_webhook_uri\n",
        "    alert_email = config.rule_sets.alert_email\n",
        "    files_count = df.count()\n",
        "    df_cluster_count = df.groupBy(\"cluster\").count()\n",
        "    df_cluster_count = df_cluster_count.orderBy('cluster', ascending=True)\n",
        "    cluster_json_list = df_cluster_count.toJSON().collect()\n",
        "    num_of_clusters = df_cluster_count.distinct().count ()\n",
        "    cluster_output = ''\n",
        "    for x in range(len(cluster_json_list)): \n",
        "        cluster_output = cluster_output + ', ' + cluster_json_list[x]   \n",
        "    cluster_output = cluster_output[2:]\n",
        "    cluster_output_str = ''.join(cluster_output)\n",
        "\n",
        "    # generate the Event Grid json \n",
        "    event_data = f'{{\"batch_id\": \"{batch_num}\",' \\\n",
        "        f'\"batch_description\": \"{batch_description}\",' \\\n",
        "        f'\"eventDate\": \"{now}\",' \\\n",
        "        f'\"eventMetrics\": {{' \\\n",
        "        f'  \"event_type\": \"image\",' \\\n",
        "        f'  \"files_processed_count\": \"{image_file_count}\",' \\\n",
        "        f'  \"event_detail_uri\": \"https://{web_app_uri}/reports\",' \\\n",
        "        f'  \"num_of_clusters\": {num_of_clusters},' \\\n",
        "        f'  \"clusters\": [' \\\n",
        "        f'      {cluster_output_str}' \\\n",
        "        f'  ]' \\\n",
        "        f'}},' \\\n",
        "        f'\"teams_webhook_endpoint\": \"{subscriber_uri}\",' \\\n",
        "        f'\"alert_email\": \"{alert_email}\"' \\\n",
        "        f'}}'\n",
        "\n",
        "    print(event_data)\n",
        "\n",
        "    event_data_obj = json.loads(event_data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Raise the event\n",
        "with tracer.span(name='sending message to event grid'):    \n",
        "    from azure.identity import ClientSecretCredential\n",
        "    from azure.eventgrid import EventGridPublisherClient, EventGridEvent    \n",
        "\n",
        "    # Get value from keyvault to build Event Grid Topic event\n",
        "    subscription_id = TokenLibrary.getSecretWithLS(\"keyvault\", 'SubscriptionId')\n",
        "    resource_group_name = TokenLibrary.getSecretWithLS(\"keyvault\", 'ResourceGroupName')\n",
        "    event_grid_topic_name = TokenLibrary.getSecretWithLS(\"keyvault\", 'EventGridTopicName')\n",
        "    event_grid_topic_endpoint = TokenLibrary.getSecretWithLS(\"keyvault\", 'EventGridTopicEndpointUri')\n",
        "    tenant_id = TokenLibrary.getSecretWithLS(\"keyvault\", 'TenantID')\n",
        "    client_id = TokenLibrary.getSecretWithLS(\"keyvault\", 'ADAppRegClientId')\n",
        "    client_secret = TokenLibrary.getSecretWithLS(\"keyvault\", 'ADAppRegClientSecret')\n",
        "    event_grid_topic = f'/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.EventGrid/topics/{event_grid_topic_name}'\n",
        "    credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n",
        "    client = EventGridPublisherClient(event_grid_topic_endpoint, credential)\n",
        "\n",
        "    try:\n",
        "        # queue event grid message\n",
        "        event = EventGridEvent(data=event_data_obj, subject=\"MINTED/ClusterAlert\", event_type=\"MINTED.ruleTriggered\", data_version=\"1.0\", topic=event_grid_topic)\n",
        "        client.send(event)\n",
        "        print(\"done\")\n",
        "    except Exception as e:\n",
        "        logger.exception(e)\n",
        "        raise e\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from time import sleep\r\n",
        "import datetime\r\n",
        "# Update Status Table\r\n",
        "def get_recent_status(batch_num, driver, dedicated_sql_endpoint, dedicated_database, sql_user_name, sql_user_pwd):\r\n",
        "    query = f\"\"\"\r\n",
        "        SELECT TOP (1) \r\n",
        "        [num_stages_complete], [description]\r\n",
        "        FROM [dbo].[batch_status] \r\n",
        "        WHERE [batch_id] = ?\r\n",
        "        ORDER BY [num_stages_complete] DESC;\r\n",
        "    \"\"\"\r\n",
        "    with pyodbc.connect(f'DRIVER={driver};SERVER=tcp:{dedicated_sql_endpoint};PORT=1433;DATABASE={dedicated_database};UID={sql_user_name};PWD={sql_user_pwd}',autocommit=True) as conn:\r\n",
        "        with conn.cursor() as cursor:\r\n",
        "            cursor.execute(query, batch_num)\r\n",
        "            num_stages_complete, description = cursor.fetchone()\r\n",
        "            return num_stages_complete, description\r\n",
        "\r\n",
        "def update_status_table(status_text, minted_tables_path, batch_num, driver, dedicated_sql_endpoint, sql_user_name, sql_user_pwd):\r\n",
        "    retries = 0 \r\n",
        "    exc = ''\r\n",
        "    while retries < 10:\r\n",
        "        try:\r\n",
        "            stages_complete, description = get_recent_status(batch_num, driver, dedicated_sql_endpoint, dedicated_database, sql_user_name, sql_user_pwd)\r\n",
        "            stages_complete += 1\r\n",
        "            status = f'[{stages_complete}/10] {status_text}'\r\n",
        "            x = datetime.datetime.now()\r\n",
        "            time_stamp = x.strftime(\"%Y-%m-%d %H:%M:%S\")\r\n",
        "\r\n",
        "            sql_command = f\"UPDATE batch_status SET status = ?, update_time_stamp = ?, num_stages_complete = ? WHERE batch_id = ?\"\r\n",
        "            with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+dedicated_sql_endpoint+';PORT=1433;DATABASE='+dedicated_database+';UID='+sql_user_name+';PWD='+ sql_user_pwd+'',autocommit=True) as conn:\r\n",
        "                with conn.cursor() as cursor:\r\n",
        "                    cursor.execute(sql_command, status, time_stamp, stages_complete, batch_num)\r\n",
        "                    cursor.commit()\r\n",
        "            return \r\n",
        "        except Exception as e:\r\n",
        "            exc_str = str(e)\r\n",
        "            exc = e \r\n",
        "            logger.warning(f'Failed to update status table: {exc_str}, retrying . . .')\r\n",
        "            retries += 1\r\n",
        "            sleep(3)\r\n",
        "\r\n",
        "    raise exc\r\n",
        "\r\n",
        "update_status_table('Image Processing Complete', minted_tables_output_path, batch_num, driver, dedicated_sql_endpoint, sql_user_name, sql_user_pwd)\r\n",
        "\r\n",
        "\r\n",
        "# Return the object to the pipeline\r\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\r\n",
        "mssparkutils.notebook.exit(output['custom_dimensions'])        "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
