{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "     \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "     \"spark.dynamicAllocation.enabled\": true,\n",
        "     \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "     \"spark.dynamicAllocation.maxExecutors\": 8\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "image_file_path = ''\n",
        "ais_file_path = ''\n",
        "input_container = ''\n",
        "output_container = ''\n",
        "blob_account_name = ''\n",
        "azure_storage_domain = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# used to get the config \r\n",
        "from pyspark.sql import SparkSession\r\n",
        "import json\r\n",
        "sc = spark.sparkContext\r\n",
        "spark = SparkSession.builder.appName(f'Anomaly Prep {mssparkutils.runtime.context}').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "# Load secrets\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "  'image_file_path': image_file_path,\n",
        "  'ais_file_path': ais_file_path,  \n",
        "  'input_container': input_container, \n",
        "  'output_container': output_container,\n",
        "  'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import os \n",
        "from py4j.protocol import Py4JJavaError\n",
        "\n",
        "# Setup blob paths \n",
        "output_abfss_path = f'abfss://{input_container}@{blob_account_name}.dfs.{azure_storage_domain}/'\n",
        "global_config_abfss_path = f'abfss://configuration@{blob_account_name}.dfs.{azure_storage_domain}/anomdet.config.global.json'\n",
        "\n",
        "# Get components to build output locations \n",
        "base_dir, filename = os.path.split(image_file_path) \n",
        "file_base, _ext = filename.split('.')\n",
        "tgt_dir = f'{base_dir}/{file_base}'.lstrip('/')\n",
        "\n",
        "config_path = f'{tgt_dir}/anomdet.config.json'\n",
        "config_abfss_path = f\"{output_abfss_path}/{config_path}\"\n",
        "\n",
        "# Copy configuration from configuration/anomdet.config.global.json if it doesn't already exist \n",
        "try: \n",
        "    mssparkutils.fs.head(config_abfss_path)\n",
        "except Py4JJavaError as e: \n",
        "    if 'java.io.FileNotFoundException' in str(e): \n",
        "        mssparkutils.fs.cp(global_config_abfss_path, config_abfss_path)\n",
        "    else: \n",
        "        raise e \n",
        "\n",
        "config_n = json.loads(''.join(sc.textFile(config_abfss_path).collect()))\n",
        "low_res_name = str(config_n[\"translate_options\"][\"widthPct\"][1]) + \"_pct_\"\n",
        "\n",
        "# Setup notebook outputs \n",
        "output = {\n",
        "    'custom_dimensions': {\n",
        "        'input_image_low_res': f'{tgt_dir}/{low_res_name}{file_base}.png',\n",
        "        'ship_bb_image_low_res': f'{tgt_dir}/{file_base}_ship_bb_low_res.png',\n",
        "        'ship_bb_image_high_res': f'{tgt_dir}/{file_base}_ship_bb_high_res.png',\n",
        "        'ais_image': f'{tgt_dir}/{file_base}_ais.png',\n",
        "        'anomaly_image': f'{tgt_dir}/{file_base}_anomaly.png',\n",
        "        'config_path': config_path,\n",
        "        'output_path': tgt_dir,\n",
        "        'kml_path': f'{tgt_dir}/{file_base}.xml'\n",
        "    }\n",
        "}\n",
        "\n",
        "# return the object to the pipeline\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\n",
        "mssparkutils.notebook.exit(output['custom_dimensions'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
