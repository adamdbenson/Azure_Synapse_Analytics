{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 2.8 Relation Extraction\n",
        "\n",
        "Notebook processes the english-translated documents from batch and outputs a dataframe of relation predictions that can be used to construct a knowledge graph.\n",
        "Note the addition of the session-scoped java package needed to get Spark NLP to work: spark-nlp-assembly-4.0.2.jar. Spark NLP is used to perform sentence splitting and may eventually be used to replace the API-based coreference resolution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "     \"spark.jars.packages\": \"com.microsoft.azure:synapseml_2.12:0.10.0-19-c3a445c5-SNAPSHOT\",\n",
        "      \"spark.jars.repositories\": \"https://mmlspark.azureedge.net/maven\",\n",
        "      \"spark.jars\": \"abfss://synapse@<<STORAGE_ACCOUNT_NAME>>.dfs.<<AZURE_STORAGE_DOMAIN>>/jars/spark-nlp.jar\",\n",
        "      \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12,com.fasterxml.jackson.core:jackson-databind\",\n",
        "      \"spark.yarn.user.classpath.first\": \"true\"\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "manifest_file_path = ''\n",
        "manifest_container = ''\n",
        "summarized_text_dailymail_tbl_name = ''\n",
        "batch_num = ''\n",
        "file_system = ''\n",
        "batch_root = ''\n",
        "blob_account_name = ''\n",
        "azure_storage_domain = ''\n",
        "minted_tables_output_path = ''\n",
        "input_container = ''\n",
        "output_container= ''\n",
        "\n",
        "display_dataframes = False # set to True for debugging (Note, this triggers execution multiple times, only use on small runs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyodbc\r\n",
        "from azure.storage.blob import generate_blob_sas, BlobSasPermissions, generate_container_sas, ContainerSasPermissions, BlobClient\r\n",
        "from datetime import datetime, timedelta\r\n",
        "dedicated_database = \"dedicated\"\r\n",
        "database = 'minted'   \r\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\r\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\r\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\r\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseServerlessSQLEndpoint\")\r\n",
        "dedicated_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseDedicatedSQLEndpoint\")\r\n",
        "storage_account_key = mssparkutils.credentials.getSecretWithLS('keyvault', 'StorageAccountKey')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import os\r\n",
        "import ast\r\n",
        "import json\r\n",
        "from types import SimpleNamespace\r\n",
        "\r\n",
        "# Initialise paths and batch root\r\n",
        "batch_path = f'abfss://{manifest_container}@{blob_account_name}.dfs.{azure_storage_domain}'\r\n",
        "batch_folder = os.path.dirname(manifest_file_path)\r\n",
        "batch_root = f'{batch_path}{batch_folder}'\r\n",
        "\r\n",
        "def copy_global_config(config_path: str, global_config_path: str):\r\n",
        "    \"\"\"\r\n",
        "    This method makes sure that a config is availabile in the batch root.\r\n",
        "    If a config file isn't already there, it is copied over form global_config_path.\r\n",
        "    If there is no config under global_config_path, this function will crash (indicating an error in pipeline set up.)\r\n",
        "    \"\"\"\r\n",
        "    logger.info(\"Loading global config\")\r\n",
        "    try:\r\n",
        "        mssparkutils.fs.cp(global_config_path, config_path)    \r\n",
        "    except Py4JJavaError as e:\r\n",
        "        logger.exception(e)\r\n",
        "        raise e\r\n",
        "\r\n",
        "def read_batch_config(batch_root: str, global_config_path: str):\r\n",
        "    \"\"\"\r\n",
        "    We read the config file using the Java File System API as we do not need to let multiple nodes read individual lines and join it\r\n",
        "    all back together again\r\n",
        "    \"\"\"\r\n",
        "    # Change our file system from 'synapse' to 'input'\r\n",
        "    sc._jsc.hadoopConfiguration().set(\"fs.defaultFS\", batch_path)\r\n",
        "\r\n",
        "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\r\n",
        "    config_path = sc._jvm.org.apache.hadoop.fs.Path(f'{batch_root}/config.json')\r\n",
        "\r\n",
        "    # If we don't have a batch config, copy the global one.\r\n",
        "    if fs.exists(config_path) != True:\r\n",
        "        copy_global_config(f'{batch_root}/config.json', global_config_path)\r\n",
        "\r\n",
        "    # Open our file directly rather than through spark\r\n",
        "    input_stream = fs.open(config_path)  # FSDataInputStream\r\n",
        "\r\n",
        "    config_string = sc._jvm.java.io.BufferedReader(\r\n",
        "        sc._jvm.java.io.InputStreamReader(input_stream, sc._jvm.java.nio.charset.StandardCharsets.UTF_8)\r\n",
        "        ).lines().collect(sc._jvm.java.util.stream.Collectors.joining(\"\\n\"))\r\n",
        "\r\n",
        "    # # Load it into json    \r\n",
        "    return json.loads(''.join(config_string), object_hook=lambda dictionary: SimpleNamespace(**dictionary))\r\n",
        "\r\n",
        "# NOTE: this path should be in sync with Terraform configuration which uploads this file\r\n",
        "config = read_batch_config(batch_root, global_config_path=f'abfss://configuration@{blob_account_name}.dfs.{azure_storage_domain}/config.global.json')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Load Cog Services text analytics keys, set defaults\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "text_analytics_keys = mssparkutils.credentials.getSecretWithLS(\"keyvault\", 'TextAnalyticsKeys').split(',')\n",
        "\n",
        "# Get config values \n",
        "web_app_uri = config.relex.relex_webapp_uri\n",
        "relex_endpoint_api_key = config.relex.key\n",
        "\n",
        "# Coreference resolution and relation extraction model endpoints\n",
        "relex_endpoint_healthcheck_url = f'{web_app_uri}/api/healthcheck'\n",
        "coref_endpoint_url = f'{web_app_uri}/api/coref_batch_pred'\n",
        "relex_endpoint_url = f'{web_app_uri}/api/relex_batch_pred'\n",
        "\n",
        "# Pipeline parameters (chunking large documents/summaries and batching the chunks for fewer API calls)\n",
        "max_coref_doc_char_length = 3500\n",
        "min_coref_doc_char_length = 2000\n",
        "coref_batch_size = 30 # number of document chunks to perform coreference resolution per API call\n",
        "\n",
        "max_entity_link_doc_char_length = 3000\n",
        "min_entity_link_doc_char_length = 2000\n",
        "entity_link_min_confidence_score = float(config.relex.el_confidence_score) # (0)\n",
        "\n",
        "relex_batch_size = 1000 # number of relation predictions per call to API\n",
        "\n",
        "# Graph Visualization Filters\n",
        "filter_graph_viz = ast.literal_eval(config.relex.filter_graph_viz) # defaults to False\n",
        "relex_min_probability = float(config.relex.relex_min_probability) # filters predictions displayed in graph visualizations, (0.7)\n",
        "exclude_relations = config.relex.exclude_relations # [\"main subject\", \"followed by\", \"follows\", \"said to be the same as\", \"instance of\"]\n",
        "clique_min_nodes = int(config.relex.clique_min_nodes) # plots only connected cliques with at least min # nodes per clique, (3)\n",
        "\n",
        "# Cognitive Services Entity Linker API Config\n",
        "cog_svc_concurrency = 1\n",
        "cog_svc_batch_size = 15 # The /analyze endpoint that TextAnalyze uses is documented to allow batches of up to 25 documents\n",
        "cog_svc_intial_polling_delay = 15000 # Time (in ms) to wait before first poll for results\n",
        "cog_svc_polling_delay = 10000 # Time (in ms) to wait between repeated polling for results\n",
        "cog_svc_maximum_retry_count = 100 # Maximum number of retries. 60 => 60 * 10s + 15s = 615s ~= 10 mins allowed for a job to complete\n",
        "\n",
        "# Column names \n",
        "file_path_col = 'file_path'\n",
        "text_col = 'summarized_text_dailymail'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate Logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'summarized_text_dailymail_tbl_name': summarized_text_dailymail_tbl_name,\n",
        "    'batch_root': batch_root,\n",
        "    'batch_num': batch_num,\n",
        "    'cog_svc_concurrency': cog_svc_concurrency,\n",
        "    'cog_svc_batch_size': cog_svc_batch_size,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from collections import defaultdict\n",
        "import itertools\n",
        "import re\n",
        "import random\n",
        "import requests\n",
        "import fsspec\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyvis\n",
        "from pyvis.network import Network\n",
        "import networkx as nx\n",
        "from networkx.algorithms.community import k_clique_communities\n",
        "\n",
        "import sparknlp\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "import synapse.ml\n",
        "from synapse.ml.io import *\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from synapse.ml.cognitive import *\n",
        "from synapse.ml.featurize.text import PageSplitter\n",
        "from synapse.ml.stages import FixedMiniBatchTransformer\n",
        "from synapse.ml.stages import FlattenBatch\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import udf, col, length, size, posexplode, explode, trim\n",
        "from pyspark.sql.types import ArrayType, MapType, StructType, StructField, StringType, IntegerType, FloatType\n",
        "\n",
        "# Initialise session and config\n",
        "sc = spark.sparkContext\n",
        "spark = SparkSession.builder.appName(f\"RelationExtraction {mssparkutils.runtime.context}\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Load documents table from 2_2_Text_Summarization_dailymail notebook output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#with tracer.span(name='Load documents contents table'):\n",
        "#\n",
        "#    prefix_length = len(f'abfss://{input_container}@{blob_account_name}.dfs.{azure_storage_domain}/')\n",
        "#\n",
        "#    df = spark.read.parquet(f'{minted_tables_output_path}{summarized_text_dailymail_tbl_name}') \\\n",
        "#                .withColumn('file_name', F.expr(f\"substring(file_path, {prefix_length + 1}, length(file_path))\"))\n",
        "#    \n",
        "#    if display_dataframes:\n",
        "#        df.show(n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Resolve document coreferences\n",
        "Find all expressions that refer to the same entity and replace with entity name.\n",
        "Chunk up documents into smaller more manageable portions to avoid model OOM errors and then batch the API call to perform coreference resolution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#with tracer.span(name='Split large documents into chunks for coreference resolution'):\n",
        "#    page_splitter = (PageSplitter()\n",
        "#        .setInputCol(text_col)\n",
        "#        .setMinimumPageLength(min_coref_doc_char_length)\n",
        "#        .setMaximumPageLength(max_coref_doc_char_length)\n",
        "#        .setOutputCol(\"extracted_doc_split\"))\n",
        "#\n",
        "#    df_split = page_splitter.transform(df)\n",
        "#\n",
        "#    df_split = df_split.select(\"file_name\", posexplode(\"extracted_doc_split\") \\\n",
        "#                        .alias(\"chunk_number\", \"extracted_doc_split\")) \\\n",
        "#                        .withColumn(\"split_doc_char_length\", length(\"extracted_doc_split\"))\n",
        "#\n",
        "#with tracer.span(name='Group rows into batches for coreference resolution'):\n",
        "#      fmbt = (FixedMiniBatchTransformer()\n",
        "#            .setBatchSize(coref_batch_size))\n",
        "#\n",
        "#      df_batched = fmbt.transform(df_split)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#with tracer.span(name='Perform coference resolution on document batches'):\n",
        "#    @udf(returnType=ArrayType(StringType()))\n",
        "#    def get_coref(docs_list):\n",
        "#        try:\n",
        "#            print(coref_endpoint_url)\n",
        "#            headers = {'Content-Type': 'application/json',\n",
        "#            'Ocp-Apim-Subscription-Key': relex_endpoint_api_key}\n",
        "#            response = requests.post(url=coref_endpoint_url, headers=headers, json={\"doc_corefs_request\":docs_list})\n",
        "#            return response.json()\n",
        "#        except Exception as e:\n",
        "#            logger.error(f\"Exception during call to batch coref resolution endpoint: {e}\")\n",
        "#            return docs_list # fall back to non-coreffed text if exception\n",
        "#\n",
        "#    health_check = requests.get(relex_endpoint_healthcheck_url)\n",
        "#\n",
        "#    if health_check.json()==\"Ready\":\n",
        "#        coreffed_batch_df = df_batched.withColumn(\"coref_output\", get_coref(col(\"extracted_doc_split\")))\n",
        "#    else:\n",
        "#        logger.error(\"Relex endpoint inactive\")\n",
        "#\n",
        "#with tracer.span(name='Unroll batched coreffed documents and re-combine chunks'):\n",
        "#    flattener = FlattenBatch()\n",
        "#    coreffed_df = flattener.transform(coreffed_batch_df)\n",
        "#    coreffed_df = coreffed_df.select(\"file_name\", \"chunk_number\", \"coref_output\") \\\n",
        "#                                .groupby(\"file_name\") \\\n",
        "#                                .agg(F.sort_array(F.collect_list(F.struct(\"chunk_number\", \"coref_output\"))) \\\n",
        "#                                .alias(\"sorted_coref_text_col\")) \\\n",
        "#                                .withColumn(\"coreffed_text\", F.concat_ws(\"\", col(f\"sorted_coref_text_col.coref_output\")))\\\n",
        "#                                .drop(\"sorted_coref_text_col\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Extract named entities and link them to a knowledge base\n",
        "\n",
        "First split and batch the coreferenced text documents before calling the NER and EL Cog Services endpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#with tracer.span(name='Split coreffed text into chunks'):\n",
        "#    page_splitter = (PageSplitter()\n",
        "#        .setInputCol(\"coreffed_text\")\n",
        "#        .setMaximumPageLength(max_entity_link_doc_char_length)\n",
        "#        .setMinimumPageLength(min_entity_link_doc_char_length)\n",
        "#        .setOutputCol(\"coreffed_text_split\"))\n",
        "#\n",
        "#    df_split = page_splitter.transform(coreffed_df)\n",
        "#\n",
        "#    df_split = df_split.select(\"file_name\", posexplode(\"coreffed_text_split\") \\\n",
        "#                        .alias(\"chunk_number\", \"coreffed_text_split\")) \\\n",
        "#                        .withColumn(\"split_doc_char_length\", length(\"coreffed_text_split\"))\n",
        "#\n",
        "#with tracer.span(name='Group coreffed text chunks into batches'):\n",
        "#      fmbt = (FixedMiniBatchTransformer()\n",
        "#            .setBatchSize(cog_svc_batch_size))\n",
        "#\n",
        "#      df_batched = fmbt.transform(df_split)\n",
        "#\n",
        "#with tracer.span(name='Distribute cognitive service keys across rows'):\n",
        "#    @udf(returnType=StringType())\n",
        "#    def rand_key() :\n",
        "#        index = random.randint(0, len(text_analytics_keys)-1)\n",
        "#        return text_analytics_keys[index]\n",
        "#\n",
        "#    df_batched = df_batched.withColumn(\"text_analytics_key\", rand_key())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#with tracer.span(name='Call cognitive services NER and entity linker'):\n",
        "#    text_analyze = (TextAnalyze()\n",
        "#        .setTextCol(\"coreffed_text_split\")\n",
        "#        .setLocation(config.location)\n",
        "#        .setLanguage(config.prep.target_language)\n",
        "#        .setSubscriptionKeyCol(\"text_analytics_key\")\n",
        "#        .setOutputCol(\"batch_entity_link_results\")\n",
        "#        .setErrorCol(\"text_analysis_error\")\n",
        "#        .setConcurrency(cog_svc_concurrency)\n",
        "#        .setInitialPollingDelay(cog_svc_intial_polling_delay)\n",
        "#        .setPollingDelay(cog_svc_polling_delay)\n",
        "#        .setMaxPollingRetries(cog_svc_maximum_retry_count)\n",
        "#        .setSuppressMaxRetriesExceededException(True)\n",
        "#        .setEntityRecognitionTasks([{\"parameters\": {\"model-version\": \"latest\"}}])\n",
        "#        .setEntityLinkingTasks([{\"parameters\": { \"model-version\": \"latest\"}}])\n",
        "#        <<SYNAPSE_ML_TEXT_ANALYZE_ENDPOINT_CMD>>\n",
        "#        )\n",
        "#\n",
        "#    df_results_batched = text_analyze.transform(df_batched)\n",
        "#\n",
        "#with tracer.span(name='Unroll batched entity linking API responses'):\n",
        "#    flattener = FlattenBatch()\n",
        "#    entity_results_df = flattener.transform(df_results_batched)\n",
        "#\n",
        "#    # split out text analysis results\n",
        "#    el_cols_to_drop = (\"entityRecognitionPii\",\"keyPhraseExtraction\",\"sentimentAnalysis\")\n",
        "#    entity_results_df = entity_results_df.select(\"file_name\",\"chunk_number\",\"coreffed_text_split\",\n",
        "#                                                \"batch_entity_link_results.*\",\"text_analysis_error\")\\\n",
        "#                            .drop(*el_cols_to_drop)\n",
        "#\n",
        "#with tracer.span(name='Collate split rows back to single row per document'):\n",
        "#    error_response_schema = StructType(\n",
        "#        [StructField(\"error\", StructType(\n",
        "#            [StructField(\"code\", StringType()), StructField(\"message\", StringType())]\n",
        "#        ))]\n",
        "#    )\n",
        "#\n",
        "#    entity_results_df = entity_results_df.select(\n",
        "#                    \"file_name\",\n",
        "#                    \"chunk_number\",\n",
        "#                    \"coreffed_text_split\",\n",
        "#                    # we only have a single task for each task type, so unpack it\n",
        "#                    col(\"entityRecognition\")[0][\"result\"][\"entities\"].alias(\"named_entities\"),\n",
        "#                    col(\"entityLinking\")[0][\"result\"][\"entities\"].alias(\"linked_entities\"),\n",
        "#                    # set up as an array for the grouping step\n",
        "#                    F.from_json(entity_results_df[\"text_analysis_error\"][\"response\"], error_response_schema)[\"error\"][\"message\"].alias(\"text_analysis_error\"),\n",
        "#                )\\\n",
        "#                .groupby(\"file_name\")\\\n",
        "#                .agg(\n",
        "#                    F.flatten(F.collect_list(\"named_entities\")).alias(\"named_entities\"),\n",
        "#                    F.flatten(F.collect_list(\"linked_entities\")).alias(\"linked_entities\"),\n",
        "#                    F.sort_array(F.collect_list(F.struct(\"chunk_number\", \"coreffed_text_split\"))).alias(\"sorted_text\"),\n",
        "#                    F.max(col(\"text_analysis_error\")).alias(\"text_analysis_error\"))\\\n",
        "#                .withColumn(\n",
        "#                    \"coreffed_text\",\n",
        "#                    F.concat_ws(\"\", col(\"sorted_text.coreffed_text_split\"))\n",
        "#                )\\\n",
        "#                .drop(\"sorted_text\")\n",
        "#\n",
        "#    if display_dataframes:\n",
        "#        entity_results_df.show(n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Replace coreferenced text with linked-entity names from cognitive services."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#with tracer.span(name='Replace entity mentions with linked entities'):\n",
        "#\n",
        "#    def link_entities(coreffed_text, linked_entities, min_confidence_score=entity_link_min_confidence_score):\n",
        "#        \"\"\"\n",
        "#        Finds and links entities within a document to a knowledge base (KB) using MS Cog Service Entity Linking API\n",
        "#        :param min_confidence_score: Limit's the API's matching entities base\n",
        "#        :return: Document text replaced with KB entities (str), lookup of nicknames mapped to KB entities (dict)\n",
        "#        \"\"\"\n",
        "#        coreffed_text = coreffed_text.strip() # we trimmed the doc earlier in spark but just in case\n",
        "#\n",
        "#        # store a mapping of the entity mention to kb name\n",
        "#        entity_mentions = defaultdict(None) # for replacing mentions with wiki names\n",
        "#\n",
        "#        for entity in linked_entities:\n",
        "#            for match in entity[\"matches\"]:\n",
        "#                if match[\"confidenceScore\"] >= min_confidence_score:\n",
        "#                    entity_mentions[match[\"text\"]] = entity[\"name\"]\n",
        "#                    \n",
        "#        # store kb entity names\n",
        "#        _linked_entity_names = list(set(entity_mentions.values()))\n",
        "#\n",
        "#        # need underscore instead of ws to do proper multi-name entity replacement\n",
        "#        string_to_modify = coreffed_text.replace(\" \", \"_\")\n",
        "#        _ent_mentions = defaultdict(None)\n",
        "#\n",
        "#        for k, v in entity_mentions.items():\n",
        "#            _ent_mentions[k.replace(\" \", \"_\")] = v.replace(\" \", \"_\")\n",
        "#            \n",
        "#        # substitute all mentions with the wiki name\n",
        "#        escaped_key_names = [re.escape(key) for key in _ent_mentions.keys()] # handles special chars in wiki names\n",
        "#        uncompiled_pattern = \"|\".join(escaped_key_names)\n",
        "#        pattern = re.compile(uncompiled_pattern)\n",
        "#\n",
        "#        output_text = pattern.sub(lambda x: _ent_mentions.get(x.group(0),x.group(0)), string_to_modify)\n",
        "#        output_text = output_text.replace(\"_\",\" \")\n",
        "#\n",
        "#        return output_text, _linked_entity_names\n",
        "#\n",
        "#    el_schema = StructType([StructField(\"entity_linked_text\", StringType()), StructField(\"linked_entity_names\", ArrayType(StringType()))])\n",
        "#    el_udf = udf(link_entities, el_schema)\n",
        "#\n",
        "#    entity_linked_text_df = entity_results_df.withColumn('el_results', el_udf(col('coreffed_text'),col('linked_entities'))).select(col('file_name'),col('coreffed_text'), col(\"el_results.*\"))\n",
        "#\n",
        "#    if display_dataframes:\n",
        "#        entity_linked_text_df.show(n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Prepare pre-processed documents for sentence-level relation extraction\n",
        "Split documents up into sentences and preserve only sentences containing pairs of identified entities from previous steps.\n",
        "Note: we cap extremely long sentence lengths at 512 tokens to avoid spark OOM issues. Abnormally long sentences can occur because texts may be missing punctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#with tracer.span(name='Split documents into sentences for relation extraction'):\n",
        "#    max_sent_token_length = 512\n",
        "#\n",
        "#    document_assembler = DocumentAssembler().setInputCol(\"entity_linked_text\").setOutputCol(\"document\")\n",
        "#    sentence_detector = SentenceDetectorDLModel.pretrained(\"sentence_detector_dl\", \"en\")\\\n",
        "#                                                .setSplitLength(max_sent_token_length)\\\n",
        "#                                                .setInputCols([\"document\"])\\\n",
        "#                                                .setOutputCol(\"sentences\")\n",
        "#\n",
        "#    sent_pipeline = Pipeline(stages=[document_assembler, sentence_detector])\n",
        "#\n",
        "#    sent_model = sent_pipeline.fit(entity_linked_text_df)\n",
        "#    sent_df = sent_model.transform(entity_linked_text_df)\n",
        "#    sent_df = sent_df.withColumn(\"sent_list\",col(\"sentences.result\"))\\\n",
        "#                        .drop(\"coreffed_text\",\"entity_linked_text\",\"document\",\"sentences\")\\\n",
        "#                        .select(\"file_name\", \"linked_entity_names\", explode(col(\"sent_list\")).alias(\"sentence\"))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#with tracer.span(name='Generate all pair spans per sentence'):\n",
        "#\n",
        "#    @udf(returnType=ArrayType(MapType(StringType(), StringType())))\n",
        "#    def get_sent_pairs(sentence, linked_entity_names):\n",
        "#        \"\"\"Returns all entity permutation information per sentence.\n",
        "#        The entities are the set of knowledge base entities returned by link_entities() above\"\"\"\n",
        "#        \n",
        "#        sent_pair_spans = [] # store the entity pair info for all sentences in the doc\n",
        "#\n",
        "#        # keep track of linked entity names and their spans in the sentence\n",
        "#        entity_spans = defaultdict(None)\n",
        "#        \n",
        "#        # if entity name is in the sentence, store it along with its start and end spans\n",
        "#        for linked_name in linked_entity_names:\n",
        "#            for match in re.finditer(re.escape(linked_name), sentence):\n",
        "#                entity_spans[(match.start(), match.end())] = linked_name\n",
        "#        \n",
        "#        # only extract entity spans if more than one unique entity detected in sentence (else can't predict a relation)\n",
        "#        if len(set(entity_spans.values())) > 1:\n",
        "#            \n",
        "#            # generate all pair permutations excluding same-name pairs\n",
        "#            perms_list = list(itertools.permutations(entity_spans.items(), 2))\n",
        "#            pair_span_list = list(perm for perm in perms_list if perm[0][1] != perm[1][1])\n",
        "#\n",
        "#            for pair in pair_span_list:\n",
        "#                head_ent, head_span = pair[0][1], pair[0][0]\n",
        "#                tail_ent, tail_span = pair[1][1], pair[1][0]\n",
        "#\n",
        "#                # convert char spans to string to avoid mixed value types and be able to use MapType() schema\n",
        "#                # also, the list arraytype(structtype()) representation does not guarantee order of the inputs\n",
        "#                head_span_start, head_span_end = str(head_span[0]), str(head_span[1])\n",
        "#                tail_span_start, tail_span_end = str(tail_span[0]), str(tail_span[1])\n",
        "#\n",
        "#                sent_pair_dict = {\n",
        "#                    \"sentence\":sentence,\n",
        "#                    \"head_ent\":head_ent,\"head_span_start\":head_span_start,\"head_span_end\":head_span_end,\n",
        "#                    \"tail_ent\":tail_ent,\"tail_span_start\":tail_span_start,\"tail_span_end\":tail_span_end\n",
        "#                    }\n",
        "#\n",
        "#                sent_pair_spans.append(sent_pair_dict)\n",
        "#        \n",
        "#        return sent_pair_spans\n",
        "# \n",
        "#    sent_df = sent_df.withColumn(\"pair_spans\", get_sent_pairs(col(\"sentence\"),col(\"linked_entity_names\")))\\\n",
        "#                    .filter(size(\"pair_spans\") > 0)\\\n",
        "#                    .select(\"file_name\",explode(\"pair_spans\").alias(\"pair_spans\"))\n",
        "#\n",
        "#    if display_dataframes:\n",
        "#        sent_df.show(n=5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#with tracer.span(name='Batch pair spans to reduce API calls to relex endpoint'):\n",
        "#    fmbt = (FixedMiniBatchTransformer()\n",
        "#        .setBatchSize(relex_batch_size))\n",
        "#\n",
        "#    sent_df = fmbt.transform(sent_df)\n",
        "#\n",
        "#    if display_dataframes:\n",
        "#        sent_df.show(n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Predict relation for each pair span in the doc.\n",
        "Relation extraction endpoint designed to accept a list of all pair spans for a single document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#with tracer.span(name='Generate relation predictions per entity pair and unbatch results'):\n",
        "#    def get_relation(spans_list):\n",
        "#        try:\n",
        "#            headers = {'Content-Type': 'application/json', 'Ocp-Apim-Subscription-Key': relex_endpoint_api_key}\n",
        "#            response = requests.post(url=relex_endpoint_url, headers=headers, json={\"doc_spans_request\":spans_list})\n",
        "#            return response.json()\n",
        "#        except Exception as e:\n",
        "#            logger.error(f\"Exception during call to batch relation extraction endpoint: {e}\")\n",
        "#            return None\n",
        "#\n",
        "#    relex_output_schema = ArrayType(StructType([\n",
        "#        StructField(\"sentence\", StringType(), True),\n",
        "#        StructField(\"head_ent\", StringType(), True),\n",
        "#        StructField(\"tail_ent\", StringType(), True),\n",
        "#        StructField(\"relation\", StringType(), True),\n",
        "#        StructField(\"probability\", FloatType(), True)\n",
        "#    ]))\n",
        "#\n",
        "#    gr_udf = udf(get_relation, relex_output_schema)\n",
        "#\n",
        "#    health_check = requests.get(relex_endpoint_healthcheck_url)\n",
        "#\n",
        "#    if health_check.json()==\"Ready\":\n",
        "#        sent_df = sent_df.withColumn(\"relex_output\", gr_udf(col(\"pair_spans\")))\n",
        "#    else:\n",
        "#        logger.error(\"Relex endpoint inactive\")\n",
        "#\n",
        "#    # unbatch the response\n",
        "#    flattener = FlattenBatch()\n",
        "#    sent_df = flattener.transform(sent_df)\n",
        "#    relex_results_df = sent_df.select(\"file_name\",\"relex_output.*\")\n",
        "#\n",
        "#    if display_dataframes:\n",
        "#        relex_results_df.show(n=10)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#with tracer.span(name='Trigger Relex Pipeline Execution'):\n",
        "#    # sort by descending probability and preserve only highest probability relation among duplicate relations\n",
        "#    relex_results_df = relex_results_df.sort(col(\"probability\").desc()) \\\n",
        "#                                        .dropDuplicates([\"head_ent\",\"tail_ent\",\"relation\"]) \\\n",
        "#                                        .sort(col(\"probability\").desc())\n",
        "#\n",
        "#    # save relex_results_df to parquet and create external table\n",
        "#    edgelist_tbl_name = f'{batch_num}_relex_edgelist'\n",
        "#    edgelist_df = relex_results_df.toPandas()\n",
        "#    relex_results_df.write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{edgelist_tbl_name}')\n",
        "#\n",
        "#    relex_sql_command = f\"\"\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{edgelist_tbl_name}') \n",
        "#    CREATE EXTERNAL TABLE [{edgelist_tbl_name}] \n",
        "#    (\n",
        "#        [file_name] nvarchar(4000), \n",
        "#        [sentence] nvarchar(4000), \n",
        "#        [head_ent] nvarchar(4000),\n",
        "#        [tail_ent] nvarchar(4000),\n",
        "#        [relation] nvarchar(4000),\n",
        "#        [probability] float\n",
        "#    ) WITH (\n",
        "#            LOCATION = 'minted_tables/{edgelist_tbl_name}/**', \n",
        "#            DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], \n",
        "#            FILE_FORMAT = [SynapseParquetFormat]\n",
        "#            )\"\"\"\n",
        "#\n",
        "#    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "#        with conn.cursor() as cursor:\n",
        "#            cursor.execute(relex_sql_command)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Visualize the relations in a knowledge graph.\n",
        "\n",
        "If user sets \"filter_graph_viz=True\" in the global config file, a graph preserving only cliques with min_clique_nodes will be generated. Else, all the unfiltered predictions will be graphed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#with tracer.span(name='Filter graph for plotting'):\n",
        "#\n",
        "#    # Curate edgelist for visualization\n",
        "#    edgelist_df.dropna(inplace=True) # drop nulls\n",
        "#\n",
        "#    # grab head and tail entity document information for graph tooltips\n",
        "#    head_docs_df = edgelist_df.groupby(\"head_ent\").agg(head_found_in=('file_name','unique')).reset_index()\n",
        "#    tail_docs_df = edgelist_df.groupby(\"tail_ent\").agg(tail_found_in=('file_name','unique')).reset_index()\n",
        "#\n",
        "#    # preserve only unique documents of a given node (entity) whether it's a head or tail entity\n",
        "#    lookup_df = pd.merge(head_docs_df, tail_docs_df, left_on=\"head_ent\", right_on=\"tail_ent\", how=\"outer\")\n",
        "#    lookup_df[\"entity\"] = lookup_df[\"head_ent\"].combine_first(lookup_df[\"tail_ent\"])\n",
        "#\n",
        "#    # ensure na becomes empty list and arrays get converted to list before merging\n",
        "#    lookup_df['head_found_in'] = [[] if x is np.NaN else x.tolist() for x in lookup_df['head_found_in']]\n",
        "#    lookup_df['tail_found_in'] = [[] if x is np.NaN else x.tolist() for x in lookup_df['tail_found_in']]\n",
        "#    lookup_df[\"found_in\"] = (lookup_df[\"head_found_in\"] + lookup_df[\"tail_found_in\"]).map(set).map(list)\n",
        "#    lookup_df = lookup_df[[\"entity\", \"found_in\"]]\n",
        "#\n",
        "#    if filter_graph_viz:\n",
        "#        edgelist_df = edgelist_df[edgelist_df[\"probability\"] > relex_min_probability]\n",
        "#        edgelist_df = edgelist_df[~edgelist_df[\"relation\"].str.contains('|'.join(exclude_relations))]\n",
        "#        \n",
        "#        # grab all connected cliques of minimum node size\n",
        "#        G = nx.from_pandas_edgelist(edgelist_df, \"head_ent\",\"tail_ent\",[\"relation\",\"probability\"])\n",
        "#\n",
        "#        k = list(k_clique_communities(G, clique_min_nodes))\n",
        "#\n",
        "#        # ensure we have something to plot by taking the largest non-zero k for k_clique_communities from the unfiltered graph\n",
        "#        while len(k) == 0:\n",
        "#            if clique_min_nodes==1 and len(k)==0:\n",
        "#                print(\"No relations to graph!\")\n",
        "#                break\n",
        "#            \n",
        "#            clique_min_nodes -= 1\n",
        "#            k = list(k_clique_communities(G, clique_min_nodes))\n",
        "#\n",
        "#        k_clique_nodes = [list(x) for x in k]\n",
        "#        k_clique_nodes = set(n for clq in k_clique_nodes for n in clq)\n",
        "#        K_clique_graph = G.subgraph(k_clique_nodes)\n",
        "#        clique_df = nx.to_pandas_edgelist(K_clique_graph) # grab edgelist instead of actual clique graph for more viz control\n",
        "#\n",
        "#        if display_dataframes:\n",
        "#            print(clique_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Preserve densely-connected subgraphs for interesting visualization and write results out to output container."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#with tracer.span(name='Populate a pyvis graph object from filtered or unfiltered predictions'):\n",
        "#\n",
        "#    # generate the pyvis graph as .html\n",
        "#    nt = Network(directed=True, height='1080px', width='100%', bgcolor='#222222', font_color='white')\n",
        "#    nt.force_atlas_2based(spring_strength=0.01, overlap=1)\n",
        "#    nt.toggle_physics(True) # False for static graph\n",
        "#\n",
        "#    if filter_graph_viz:\n",
        "#        edge_data = zip(clique_df['source'], clique_df['target'], clique_df['relation'])\n",
        "#    else:\n",
        "#        edge_data = zip(edgelist_df['head_ent'], edgelist_df['tail_ent'], edgelist_df['relation'])\n",
        "#\n",
        "#    for e in edge_data:\n",
        "#        src, dst, edge_label = e[0], e[1], e[2]\n",
        "#\n",
        "#        nt.add_node(src, src, title=src)\n",
        "#        nt.add_node(dst, dst, title=dst)\n",
        "#        nt.add_edge(src, dst, title=edge_label, arrowStrikethrough=True)\n",
        "#\n",
        "#    for node in nt.nodes:\n",
        "#        outbound_rels_df = edgelist_df.loc[edgelist_df[\"tail_ent\"] == node[\"id\"], [\"relation\",\"head_ent\"]]\n",
        "#        outbound_rels = (outbound_rels_df[\"relation\"] + \" -> \" + outbound_rels_df[\"head_ent\"]).tolist()\n",
        "#        node_docs = lookup_df.loc[lookup_df[\"entity\"] == node[\"id\"], \"found_in\"].item()\n",
        "#        \n",
        "#        node['title'] += '\\n\\nSuggested relations:\\n'+'\\n'.join(outbound_rels)+'\\n\\nFound in:\\n'+'\\n'.join(node_docs)\n",
        "#        node['value'] = len(node_docs) # circle diameter based on document frequency\n",
        "#\n",
        "#    out_cont_sas_tkn = generate_container_sas(account_name=blob_account_name, \n",
        "#                            container_name=output_container,\n",
        "#                            account_key=storage_account_key,\n",
        "#                            permission=ContainerSasPermissions(read=True, list=True, write=True, add=True, create=True, update=True),\n",
        "#                            expiry=datetime.utcnow() + timedelta(hours=1))\n",
        "#    connection_string = f'DefaultEndpointsProtocol=https;AccountName={blob_account_name};AccountKey={storage_account_key};EndpointSuffix={azure_storage_domain}'\n",
        "#    blob = BlobClient.from_connection_string(conn_str=connection_string, container_name=f'{output_container}', blob_name=f'{batch_num}/{batch_num}_relex_graph.html', credential=out_cont_sas_tkn)\n",
        "#\n",
        "#    file_name = f'{batch_num}_relex_graph.html'\n",
        "#    #Write XML to file \n",
        "#    with open(file_name, mode='w') as f:\n",
        "#        f.write(nt.generate_html())\n",
        "#    \n",
        "#    # Write xml to output_container\n",
        "#    with open(file_name, \"rb\") as data:\n",
        "#        blob.upload_blob(data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from time import sleep\r\n",
        "import datetime\r\n",
        "# Update Status Table\r\n",
        "def get_recent_status(batch_num, driver, dedicated_sql_endpoint, dedicated_database, sql_user_name, sql_user_pwd):\r\n",
        "    query = f\"\"\"\r\n",
        "        SELECT TOP (1) \r\n",
        "        [num_stages_complete], [description]\r\n",
        "        FROM [dbo].[batch_status] \r\n",
        "        WHERE [batch_id] = ?\r\n",
        "        ORDER BY [num_stages_complete] DESC;\r\n",
        "    \"\"\"\r\n",
        "    with pyodbc.connect(f'DRIVER={driver};SERVER=tcp:{dedicated_sql_endpoint};PORT=1433;DATABASE={dedicated_database};UID={sql_user_name};PWD={sql_user_pwd}',autocommit=True) as conn:\r\n",
        "        with conn.cursor() as cursor:\r\n",
        "            cursor.execute(query, batch_num)\r\n",
        "            num_stages_complete, description = cursor.fetchone()\r\n",
        "            return num_stages_complete, description\r\n",
        "\r\n",
        "def update_status_table(status_text, minted_tables_path, batch_num, driver, dedicated_sql_endpoint, sql_user_name, sql_user_pwd):\r\n",
        "    retries = 0 \r\n",
        "    exc = ''\r\n",
        "    while retries < 10:\r\n",
        "        try:\r\n",
        "            stages_complete, description = get_recent_status(batch_num, driver, dedicated_sql_endpoint, dedicated_database, sql_user_name, sql_user_pwd)\r\n",
        "            stages_complete += 1\r\n",
        "            status = f'[{stages_complete}/10] {status_text}'\r\n",
        "            x = datetime.datetime.now()\r\n",
        "            time_stamp = x.strftime(\"%Y-%m-%d %H:%M:%S\")\r\n",
        "\r\n",
        "            sql_command = f\"UPDATE batch_status SET status = ?, update_time_stamp = ?, num_stages_complete = ? WHERE batch_id = ?\"\r\n",
        "            with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+dedicated_sql_endpoint+';PORT=1433;DATABASE='+dedicated_database+';UID='+sql_user_name+';PWD='+ sql_user_pwd+'',autocommit=True) as conn:\r\n",
        "                with conn.cursor() as cursor:\r\n",
        "                    cursor.execute(sql_command, status, time_stamp, stages_complete, batch_num)\r\n",
        "                    cursor.commit()\r\n",
        "            return \r\n",
        "        except Exception as e:\r\n",
        "            exc_str = str(e)\r\n",
        "            exc = e \r\n",
        "            logger.warning(f'Failed to update status table: {exc_str}, retrying . . .')\r\n",
        "            retries += 1\r\n",
        "            sleep(3)\r\n",
        "\r\n",
        "    raise exc\r\n",
        "\r\n",
        "update_status_table('Text Relation Extraction Complete', minted_tables_output_path, batch_num, driver, dedicated_sql_endpoint, sql_user_name, sql_user_pwd)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
