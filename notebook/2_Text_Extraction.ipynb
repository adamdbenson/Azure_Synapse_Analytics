{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "val documents_tbl_name = \"\"\n",
        "val documents_cracked_view_name = \"\"\n",
        "val file_system = \"\"\n",
        "val minted_tables_output_path = \"\"\n",
        "// Parameters that will get overriden by the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import scala.util.{Try,Success,Failure}\n",
        "\n",
        "import org.apache.spark.sql.functions._\n",
        "import org.apache.hadoop.conf.Configuration\n",
        "import org.apache.hadoop.fs.{Path, FileSystem}\n",
        "\n",
        "// Tika\n",
        "import org.apache.tika.Tika\n",
        "import org.apache.tika.metadata.Metadata\n",
        "import org.apache.tika.parser.{AutoDetectParser, Parser, ParseContext}\n",
        "import org.apache.tika.sax.BodyContentHandler\n",
        "import org.apache.tika.config.TikaConfig\n",
        "import org.xml.sax.ContentHandler\n",
        "\n",
        "def parseUnicodeText(is: java.io.InputStream): String = {\n",
        "    var content = new java.io.BufferedReader(\n",
        "            new java.io.InputStreamReader(is, java.nio.charset.StandardCharsets.UTF_8)\n",
        "        )\n",
        "        .lines()\n",
        "        .collect(java.util.stream.Collectors.joining(\"\\n\")\n",
        "    )\n",
        "    content\n",
        "}\n",
        "\n",
        "def extractTextWithTika(is: java.io.InputStream): String = {\n",
        "    // Allow Tika to not be restricted in the size of the content\n",
        "    val handler:BodyContentHandler = new BodyContentHandler(-1)\n",
        "    val metaData = new Metadata()\n",
        "\n",
        "    // Parse the stream\n",
        "    val tikaConfig = TikaConfig.getDefaultConfig();\n",
        "    val parser:Parser = new AutoDetectParser(tikaConfig)\n",
        "\n",
        "    parser.parse(is, handler, metaData, new ParseContext())\n",
        "    handler.toString()\n",
        "}\n",
        "\n",
        "def crack(filePath: String, fileType: String): (String, String) = {\n",
        "    val inputPath:Path = new Path(filePath)\n",
        "\n",
        "    // Get a stream for the file being cracked\n",
        "    val conf = new Configuration();\n",
        "    conf.set(\"fs.defaultFS\", file_system);\n",
        "    var fs = FileSystem.get(conf)\n",
        "    val is: java.io.InputStream = fs.open(inputPath)\n",
        "\n",
        "    // Decide which method use for cracking\n",
        "    var method: (java.io.InputStream => String) = fileType match {\n",
        "        case \"txt\" | \"html\" | \"htm\" | \"json\" | \"TXT\" | \"HTML\" | \"HTM\" | \"JSON\" => {\n",
        "            parseUnicodeText\n",
        "        }\n",
        "        case _ => {\n",
        "            extractTextWithTika\n",
        "        }\n",
        "    }\n",
        "\n",
        "    Try(method(is)) match {\n",
        "        case Success(v) => {\n",
        "            (v, null)\n",
        "        }\n",
        "        case Failure(e) => {\n",
        "            (null, e.toString())\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "val crackUdf = udf[(String, String), String, String](crack)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "val docs = spark.read.parquet(minted_tables_output_path + documents_tbl_name).select(\"file_name\",\"file_path\",\"file_type\")\n",
        "\n",
        "val cracked = docs.\n",
        "    withColumn(\"crack_result\", crackUdf(docs.col(\"file_path\"), docs.col(\"file_type\")))\n",
        "// Running an additional select to unfold the result of crackUdf (which is a tuple)\n",
        "val extracted = cracked.select(\n",
        "    col(\"crack_result\").getItem(\"_1\").alias(\"text_content\"),\n",
        "    col(\"crack_result\").getItem(\"_2\").alias(\"extraction_error\"),\n",
        "    col(\"file_name\"),\n",
        "    col(\"file_type\"),\n",
        "    col(\"file_path\")\n",
        ")\n",
        "\n",
        "extracted.createOrReplaceTempView(documents_cracked_view_name)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "scala",
      "name": "synapse_spark"
    },
    "language_info": {
      "name": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
