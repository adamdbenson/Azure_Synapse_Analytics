{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "     \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "     \"spark.dynamicAllocation.enabled\": true,\n",
        "     \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "     \"spark.dynamicAllocation.maxExecutors\": 8,\n",
        "     \"spark.jars.packages\": \"com.microsoft.azure:synapseml_2.12:0.10.0-19-c3a445c5-SNAPSHOT\",\n",
        "     \"spark.jars.repositories\": \"https://mmlspark.azureedge.net/maven\",\n",
        "     \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12,com.fasterxml.jackson.core:jackson-databind\",\n",
        "     \"spark.yarn.user.classpath.first\": \"true\"\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "azure_storage_domain = \"\"\n",
        "batch_num = \"\"\n",
        "batch_root = \"\"\n",
        "file_system = \"\"\n",
        "image_contents_tbl_name = \"\"\n",
        "blob_account_name = \"\"\n",
        "minted_tables_output_path = \"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "file_path_col = \"path\"\n",
        "\n",
        "# Load secrets\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "image_analytics_keys = mssparkutils.credentials.getSecretWithLS(\"keyvault\", 'ComputerVisonKeys').split(',')\n",
        "\n",
        "image_analytics_key_col = \"image_analytics_key\"\n",
        "\n",
        "cog_svc_concurrency = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'file_system': file_system,\n",
        "    'image_contents_tbl_name': image_contents_tbl_name,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import uuid\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import StringType, StructType, StructField\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from synapse.ml.cognitive import *\n",
        "\n",
        "# Initialise session and config\n",
        "sc = spark.sparkContext\n",
        "spark = SparkSession.builder.appName(f\"TextProcessing {mssparkutils.runtime.context}\").getOrCreate()\n",
        "\n",
        "def read_batch_config(batch_root: str):\n",
        "    \"\"\"\n",
        "    We read the config file using the Java File System API as we do not need to let multiple nodes read individual lines and join it\n",
        "    all back together again\n",
        "    \"\"\"\n",
        "    # Change our file system from 'synapse' to 'input'\n",
        "    sc._jsc.hadoopConfiguration().set(\"fs.defaultFS\", file_system)\n",
        "\n",
        "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
        "    config_path = sc._jvm.org.apache.hadoop.fs.Path(f'{batch_root}/config.json')\n",
        "\n",
        "    # If we don't have a batch config, copy the global one.\n",
        "    if fs.exists(config_path) != True:\n",
        "        logger.error(f'{config_path} not found.')\n",
        "\n",
        "    # Open our file directly rather than through spark\n",
        "    input_stream = fs.open(config_path)  # FSDataInputStream\n",
        "\n",
        "    config_string = sc._jvm.java.io.BufferedReader(\n",
        "        sc._jvm.java.io.InputStreamReader(input_stream, sc._jvm.java.nio.charset.StandardCharsets.UTF_8)\n",
        "        ).lines().collect(sc._jvm.java.util.stream.Collectors.joining(\"\\n\"))\n",
        "\n",
        "    # Load it into json    \n",
        "    return json.loads(''.join(config_string), object_hook=lambda dictionary: SimpleNamespace(**dictionary))\n",
        "\n",
        "with tracer.span(name=f\"Load config: {mssparkutils.runtime.context['notebookname']}\"):\n",
        "    try:\n",
        "        config = read_batch_config(batch_root)\n",
        "    except Exception as e:\n",
        "        logger.exception(e)\n",
        "        raise e\n",
        "\n",
        "    # Set log level\n",
        "    if config.log_level == \"INFO\":\n",
        "        logger.setLevel(logging.INFO)\n",
        "    else:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "        config.log_level = \"ERROR\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyodbc\r\n",
        "from pyspark.sql.functions import current_timestamp\r\n",
        "# serverless SQL config\r\n",
        "database = 'minted'   \r\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\r\n",
        "\r\n",
        "# secrets\r\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\r\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\r\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseServerlessSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Load image contents table'):\n",
        "    df_image_contents = spark.read.parquet(f'{minted_tables_output_path}{image_contents_tbl_name}')\n",
        "\n",
        "with tracer.span(name='Distribute cognitive service keys across rows'):\n",
        "    def rand_key() :\n",
        "        index = random.randint(0, len(image_analytics_keys)-1)\n",
        "        return image_analytics_keys[index]\n",
        "    udf_rand_key = F.udf(rand_key, StringType())\n",
        "\n",
        "    df_image_contents = df_image_contents.withColumn(image_analytics_key_col, udf_rand_key())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Use AnalyzeImage tranformer to process the list of visual features for each image\n",
        "with tracer.span(name='Analyze Images with Cog Services'):\n",
        "    analysis = (AnalyzeImage()\n",
        "            .setLocation(config.location)\n",
        "            .setSubscriptionKeyCol(image_analytics_key_col)\n",
        "            .setVisualFeatures([\"Categories\", \"Color\", \"Description\", \"Faces\", \"Objects\", \"Tags\", \"Adult\"])\n",
        "            .setDetails([\"Landmarks\"])\n",
        "            .setDescriptionExclude([\"Celebrities\"])\n",
        "            .setOutputCol(\"analysis_results\")\n",
        "            .setImageBytesCol(\"content\")\n",
        "            .setErrorCol(\"image_analysis_error\")\n",
        "            .setConcurrency(cog_svc_concurrency)\n",
        "            <<SYNAPSE_ML_ANALYZE_IMAGE_ENDPOINT_CMD>>\n",
        "            )\n",
        "\n",
        "    df_enriched_images = analysis.transform(df_image_contents)\n",
        "\n",
        "# Use AnalyzeImage tranformer to OCR any text within each image\n",
        "with tracer.span(name=\"Read Images with Cognitive Services\"):\n",
        "        read = (ReadImage()\n",
        "                .setLocation(config.location)\n",
        "                .setSubscriptionKeyCol(image_analytics_key_col)\n",
        "                .setImageBytesCol(\"content\")\n",
        "                .setOutputCol(\"read_results\")\n",
        "                .setErrorCol(\"read_error\")\n",
        "                .setConcurrency(cog_svc_concurrency)\n",
        "                .setSuppressMaxRetriesExceededException(True)\n",
        "                <<SYNAPSE_ML_READ_IMAGE_ENDPOINT_CMD>>\n",
        "                )\n",
        "        df_enriched_images = read.transform(df_enriched_images)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Persist enriched images as table'):\n",
        "    enriched_image_tbl_name = f'{batch_num}_enriched_images'\n",
        "    df_enriched_images = df_enriched_images.drop('content')\n",
        "    df_enriched_images.write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{enriched_image_tbl_name}')\n",
        "    sql_command = f'''\n",
        "        IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{enriched_image_tbl_name}') \n",
        "        CREATE EXTERNAL TABLE [{enriched_image_tbl_name}] (\n",
        "            [path] nvarchar(1000), \n",
        "            [modificationTime] datetime2(7), \n",
        "            [length] bigint,\n",
        "            [file_name] nvarchar(1000), \n",
        "            [file_type] nvarchar(1000), \n",
        "            [image_analytics_key] nvarchar(1000),\n",
        "            [image_analysis_error] varchar(max),\n",
        "            [analysis_results] varchar(max),\n",
        "            [read_error] varchar(max),\n",
        "            [read_results] varchar(max),\n",
        "        )\n",
        "        WITH (\n",
        "            LOCATION = 'minted_tables/{enriched_image_tbl_name}/**', \n",
        "            DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], \n",
        "            FILE_FORMAT = [SynapseParquetFormat]\n",
        "        )\n",
        "    '''\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "        with conn.cursor() as cursor:\n",
        "            cursor.execute(sql_command)\n",
        "\n",
        "\n",
        "# return name of new table\n",
        "output = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'enriched_image_tbl_name': enriched_image_tbl_name,\n",
        "    'file_system': file_system,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "\n",
        "# Return the object to the pipeline\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\n",
        "mssparkutils.notebook.exit(output['custom_dimensions'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
