{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%configure -f\r\n",
        "{\r\n",
        "\"conf\": {\r\n",
        "     \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\r\n",
        "     \"spark.dynamicAllocation.enabled\": true,\r\n",
        "     \"spark.dynamicAllocation.minExecutors\": 2,\r\n",
        "     \"spark.dynamicAllocation.maxExecutors\": 8\r\n",
        "   }\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "media_tbl_name = \"\"\r\n",
        "batch_root = \"\"\r\n",
        "batch_num = \"\"\r\n",
        "file_system = \"\"\r\n",
        "azure_storage_domain = \"\"\r\n",
        "blob_account_name = \"\"\r\n",
        "minted_tables_output_path = \"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\r\n",
        "import logging\r\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\r\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\r\n",
        "from opencensus.trace import config_integration\r\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\r\n",
        "from opencensus.trace.tracer import Tracer\r\n",
        "\r\n",
        "config_integration.trace_integrations(['logging'])\r\n",
        "\r\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\r\n",
        "\r\n",
        "logger = logging.getLogger(__name__)\r\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\r\n",
        "logger.setLevel(logging.INFO)\r\n",
        "\r\n",
        "tracer = Tracer(\r\n",
        "    exporter=AzureExporter(\r\n",
        "        connection_string=instrumentation_connection_string\r\n",
        "    ),\r\n",
        "    sampler=AlwaysOnSampler()\r\n",
        ")\r\n",
        "\r\n",
        "# Spool parameters\r\n",
        "run_time_parameters = {'custom_dimensions': {\r\n",
        "  'batch_root': batch_root,\r\n",
        "  'batch_num': batch_num,\r\n",
        "  'file_system': file_system,\r\n",
        "  'media_tbl_name': media_tbl_name,\r\n",
        "  'notebook_name': mssparkutils.runtime.context['notebookname']\r\n",
        "} }\r\n",
        "\r\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import json\r\n",
        "import random\r\n",
        "from types import SimpleNamespace\r\n",
        "from typing import List\r\n",
        "\r\n",
        "import pyspark.sql.functions as F\r\n",
        "from pyspark.sql.functions import col\r\n",
        "from pyspark.sql.types import StringType, StructType, StructField\r\n",
        "from pyspark import SparkContext\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "def read_batch_config(batch_root: str):\r\n",
        "    \"\"\"\r\n",
        "    We read the config file using the Java File System API as we do not need to let multiple nodes read individual lines and join it\r\n",
        "    all back together again\r\n",
        "    \"\"\"\r\n",
        "    # Change our file system from 'synapse' to 'input'\r\n",
        "    sc._jsc.hadoopConfiguration().set(\"fs.defaultFS\", file_system)\r\n",
        "\r\n",
        "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\r\n",
        "    config_path = sc._jvm.org.apache.hadoop.fs.Path(f'{batch_root}/config.json')\r\n",
        "\r\n",
        "    # If we don't have a batch config, copy the global one.\r\n",
        "    if fs.exists(config_path) != True:\r\n",
        "        logger.error(f'{config_path} not found.')\r\n",
        "\r\n",
        "    # Open our file directly rather than through spark\r\n",
        "    input_stream = fs.open(config_path)  # FSDataInputStream\r\n",
        "\r\n",
        "    config_string = sc._jvm.java.io.BufferedReader(\r\n",
        "        sc._jvm.java.io.InputStreamReader(input_stream, sc._jvm.java.nio.charset.StandardCharsets.UTF_8)\r\n",
        "        ).lines().collect(sc._jvm.java.util.stream.Collectors.joining(\"\\n\"))\r\n",
        "\r\n",
        "    # Load it into json    \r\n",
        "    return json.loads(''.join(config_string), object_hook=lambda dictionary: SimpleNamespace(**dictionary))\r\n",
        "\r\n",
        "with tracer.span(name='Initialise Spark session'):\r\n",
        "    sc = spark.sparkContext\r\n",
        "    spark = SparkSession.builder.appName(f\"ImageProcessing {mssparkutils.runtime.context}\").getOrCreate()\r\n",
        "\r\n",
        "with tracer.span(name=f\"Load config: {mssparkutils.runtime.context['notebookname']}\"):\r\n",
        "    try:\r\n",
        "        config = read_batch_config(batch_root)\r\n",
        "    except Exception as e:\r\n",
        "        logger.exception(e)\r\n",
        "        raise e\r\n",
        "\r\n",
        "    # Set log level\r\n",
        "    if config.log_level == \"INFO\":\r\n",
        "        logger.setLevel(logging.INFO)\r\n",
        "    else:\r\n",
        "        logger.setLevel(logging.ERROR)\r\n",
        "        config.log_level = \"ERROR\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyodbc\r\n",
        "from pyspark.sql.functions import current_timestamp\r\n",
        "# Dedicated and serverless SQL config\r\n",
        "dedicated_database = 'dedicated'\r\n",
        "database = 'minted'   \r\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\r\n",
        "\r\n",
        "# secrets\r\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\r\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\r\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseServerlessSQLEndpoint\")\r\n",
        "dedicated_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseDedicatedSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from time import sleep\r\n",
        "import datetime\r\n",
        "# Update Status Table\r\n",
        "def get_recent_status(batch_num, driver, dedicated_sql_endpoint, dedicated_database, sql_user_name, sql_user_pwd):\r\n",
        "    query = f\"\"\"\r\n",
        "        SELECT TOP (1) \r\n",
        "        [num_stages_complete], [description]\r\n",
        "        FROM [dbo].[batch_status] \r\n",
        "        WHERE [batch_id] = ?\r\n",
        "        ORDER BY [num_stages_complete] DESC;\r\n",
        "    \"\"\"\r\n",
        "    with pyodbc.connect(f'DRIVER={driver};SERVER=tcp:{dedicated_sql_endpoint};PORT=1433;DATABASE={dedicated_database};UID={sql_user_name};PWD={sql_user_pwd}',autocommit=True) as conn:\r\n",
        "        with conn.cursor() as cursor:\r\n",
        "            cursor.execute(query, batch_num)\r\n",
        "            num_stages_complete, description = cursor.fetchone()\r\n",
        "            return num_stages_complete, description\r\n",
        "\r\n",
        "def update_status_table(status_text, minted_tables_path, batch_num, driver, dedicated_sql_endpoint, sql_user_name, sql_user_pwd):\r\n",
        "    retries = 0 \r\n",
        "    exc = ''\r\n",
        "    while retries < 10:\r\n",
        "        try:\r\n",
        "            stages_complete, description = get_recent_status(batch_num, driver, dedicated_sql_endpoint, dedicated_database, sql_user_name, sql_user_pwd)\r\n",
        "            stages_complete += 1\r\n",
        "            status = f'[{stages_complete}/10] {status_text}'\r\n",
        "            x = datetime.datetime.now()\r\n",
        "            time_stamp = x.strftime(\"%Y-%m-%d %H:%M:%S\")\r\n",
        "\r\n",
        "            sql_command = f\"UPDATE batch_status SET status = ?, update_time_stamp = ?, num_stages_complete = ? WHERE batch_id = ?\"\r\n",
        "            with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+dedicated_sql_endpoint+';PORT=1433;DATABASE='+dedicated_database+';UID='+sql_user_name+';PWD='+ sql_user_pwd+'',autocommit=True) as conn:\r\n",
        "                with conn.cursor() as cursor:\r\n",
        "                    cursor.execute(sql_command, status, time_stamp, stages_complete, batch_num)\r\n",
        "                    cursor.commit()\r\n",
        "            return \r\n",
        "        except Exception as e:\r\n",
        "            exc_str = str(e)\r\n",
        "            exc = e \r\n",
        "            logger.warning(f'Failed to update status table: {exc_str}, retrying . . .')\r\n",
        "            retries += 1\r\n",
        "            sleep(3)\r\n",
        "\r\n",
        "    raise exc\r\n",
        "\r\n",
        "update_status_table('Media Prep Started', minted_tables_output_path, batch_num, driver, dedicated_sql_endpoint, sql_user_name, sql_user_pwd)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Get media contents'):\r\n",
        "    #Load media contents into table to be used by downstream notebooks. \r\n",
        "    media_df = spark.read.parquet(f'{minted_tables_output_path}{media_tbl_name}')\r\n",
        "    #media_avi = spark.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").option(\"pathGlobFilter\", \"*.avi\").load(f'{batch_root}')\r\n",
        "    #media_mp4 = spark.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").option(\"pathGlobFilter\", \"*.mp4\").load(f'{batch_root}')\r\n",
        "    #media_mp3 = spark.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").option(\"pathGlobFilter\", \"*.mp3\").load(f'{batch_root}')\r\n",
        "\r\n",
        "    #media_mpg = spark.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").option(\"pathGlobFilter\", \"*.mpg\").load(f'{batch_root}')\r\n",
        "    #media_wmv = spark.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").option(\"pathGlobFilter\", \"*.wmv\").load(f'{batch_root}')\r\n",
        "    #media_wav = spark.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").option(\"pathGlobFilter\", \"*.wav\").load(f'{batch_root}')    \r\n",
        "    #media_mov = spark.read.format(\"binaryFile\").option(\"recursiveFileLookup\", \"true\").option(\"pathGlobFilter\", \"*.mov\").load(f'{batch_root}')    \r\n",
        "    \r\n",
        "    #media_content_df = media_avi.union(media_mp4).union(media_mp3).union(media_mpg).union(media_wmv).union(media_wav).union(media_mov)\r\n",
        "    #media_content_df = media_content_df.join(media_df, media_df.file_path == media_content_df.path, 'inner').drop('file_path')\r\n",
        "    media_content_df = media_df.select(col(\"file_path\").alias(\"path\"), col(\"file_name\"), col(\"file_type\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## De-duplicate Media\r\n",
        "For MINTED 2.0 Accelerator, it assumed that the media data provided is already de-duplicated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='De-duplicate media'):\r\n",
        "    #Insert image de-duplication logic here based you the types of image content being consumed. \r\n",
        "    temp = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Persist Media \r\n",
        "with tracer.span(name='Persist final media set to table'):\r\n",
        "    media_contents_tbl_name = f\"{batch_num}_media_contents\"\r\n",
        "    media_content_df.write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{media_contents_tbl_name}')\r\n",
        "    ext_table_command = f\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{media_contents_tbl_name}') CREATE EXTERNAL TABLE [{media_contents_tbl_name}] ([path] nvarchar(4000), [file_name] nvarchar(4000), [file_type] nvarchar(4000)) WITH (LOCATION = 'minted_tables/{media_contents_tbl_name}/**', DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\r\n",
        "    with pyodbc.connect(f'DRIVER={driver};SERVER=tcp:{serverless_sql_endpoint};PORT=1433;DATABASE={database};UID={sql_user_name};PWD={sql_user_pwd}') as conn:\r\n",
        "        with conn.cursor() as cursor:\r\n",
        "            cursor.execute(ext_table_command)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# return values to be used by other notebooks\r\n",
        "output = {'custom_dimensions': {\r\n",
        "    'batch_num': batch_num,\r\n",
        "    'media_contents_tbl_name': media_contents_tbl_name,\r\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\r\n",
        "} }\r\n",
        "\r\n",
        "# Return the object to the pipeline\r\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\r\n",
        "mssparkutils.notebook.exit(output['custom_dimensions'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
