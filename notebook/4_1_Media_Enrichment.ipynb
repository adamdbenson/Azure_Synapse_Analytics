{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "     \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "     \"spark.dynamicAllocation.enabled\": true,\n",
        "     \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "     \"spark.dynamicAllocation.maxExecutors\": 8\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "batch_num = \"\"\n",
        "batch_root = \"\"\n",
        "file_system = \"\"\n",
        "media_contents_tbl_name = \"\"\n",
        "batch_description = \"\"\n",
        "batch_file_count = 0\n",
        "azure_storage_domain = ''\n",
        "minted_tables_output_path = \"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from azure.identity import ClientSecretCredential\n",
        "from azure.eventgrid import EventGridPublisherClient, EventGridEvent\n",
        "from azure.mgmt.keyvault import KeyVaultManagementClient\n",
        "from types import SimpleNamespace\n",
        "from datetime import datetime\n",
        "import json\n",
        "import os\n",
        "import requests as req\n",
        "import time\n",
        "import hashlib\n",
        "import random\n",
        "import sys\n",
        "import urllib.parse\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql.functions import col, to_json, from_json, lit, explode, concat, udf, json_tuple, current_timestamp\n",
        "from pyspark.sql.types import StringType, MapType, BooleanType, StructType, StructField, StringType\n",
        "from requests.structures import CaseInsensitiveDict\n",
        "from azure.identity import ClientSecretCredential"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Load keys, set defaults\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "subscription_id = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SubscriptionId\")\n",
        "resource_group_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"ResourceGroupName\")\n",
        "subscription_id = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SubscriptionId\")\n",
        "tenant_id = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"TenantID\")\n",
        "client_id = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"ADAppRegClientId\")\n",
        "client_secret = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"ADAppRegClientSecret\")\n",
        "storage_account_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"StorageAccountName\")\n",
        "storage_account_key = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"StorageAccountKey\")\n",
        "vi_account_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"VideoIndexerAccountName\")\n",
        "apiUrl = \"<<TF_VAR_azure_avam_api_domain>>\" #api's are documented here... https://api-portal.videoindexer.ai/\n",
        "enrichment_output_path_root = f'abfss://{output_container}@{blob_account_name}.dfs.{azure_storage_domain}/{batch_num}'\n",
        "\n",
        "azure_resource_manager = \"<<TF_VAR_azure_arm_management_api>>\";\n",
        "credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n",
        "\n",
        "callback_params = [(\"batch_num\", batch_num), (\"file_system\", file_system),\n",
        "    (\"batch_root\", batch_root), (\"output_container\", output_container),\n",
        "    (\"batch_description\", batch_description)] \n",
        "callbackurl = \"\".join([\"<<VI_CALLBACK_URL>>&\", \n",
        "    urllib.parse.urlencode(callback_params)])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'file_system': file_system,\n",
        "    'media_contents_tbl_name': media_contents_tbl_name,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initialise session and config\n",
        "sc = spark.sparkContext\n",
        "spark = SparkSession.builder.appName(f\"TextProcessing {mssparkutils.runtime.context}\").getOrCreate()\n",
        "\n",
        "def read_batch_config(batch_root: str):\n",
        "    \"\"\"\n",
        "    We read the config file using the Java File System API as we do not need to let multiple nodes read individual lines and join it\n",
        "    all back together again\n",
        "    \"\"\"\n",
        "    # Change our file system from 'synapse' to 'input'\n",
        "    sc._jsc.hadoopConfiguration().set(\"fs.defaultFS\", file_system)\n",
        "\n",
        "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
        "    config_path = sc._jvm.org.apache.hadoop.fs.Path(f'{batch_root}/config.json')\n",
        "\n",
        "    # If we don't have a batch config, copy the global one.\n",
        "    if fs.exists(config_path) != True:\n",
        "        logger.error(f'{config_path} not found.')\n",
        "\n",
        "    # Open our file directly rather than through spark\n",
        "    input_stream = fs.open(config_path)  # FSDataInputStream\n",
        "\n",
        "    config_string = sc._jvm.java.io.BufferedReader(\n",
        "        sc._jvm.java.io.InputStreamReader(input_stream, sc._jvm.java.nio.charset.StandardCharsets.UTF_8)\n",
        "        ).lines().collect(sc._jvm.java.util.stream.Collectors.joining(\"\\n\"))\n",
        "\n",
        "    # Load it into json    \n",
        "    return json.loads(''.join(config_string), object_hook=lambda dictionary: SimpleNamespace(**dictionary))\n",
        "\n",
        "with tracer.span(name=f\"Load config: {mssparkutils.runtime.context['notebookname']}\"):\n",
        "    try:\n",
        "        config = read_batch_config(batch_root)\n",
        "    except Exception as e:\n",
        "        logger.exception(e)\n",
        "        raise e\n",
        "\n",
        "    # Set log level\n",
        "    if config.log_level == \"INFO\":\n",
        "        logger.setLevel(logging.INFO)\n",
        "    else:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "        config.log_level = \"ERROR\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyodbc\r\n",
        "# Dedicated and serverless SQL config\r\n",
        "dedicated_database = 'dedicated'\r\n",
        "database = 'minted'   \r\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\r\n",
        "\r\n",
        "# secrets\r\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\r\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\r\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseServerlessSQLEndpoint\")\r\n",
        "dedicated_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseDedicatedSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from time import sleep\r\n",
        "from datetime import datetime, timedelta\r\n",
        "# Update Status Table\r\n",
        "def get_recent_status(batch_num, driver, dedicated_sql_endpoint, dedicated_database, sql_user_name, sql_user_pwd):\r\n",
        "    query = f\"\"\"\r\n",
        "        SELECT TOP (1) \r\n",
        "        [num_stages_complete], [description]\r\n",
        "        FROM [dbo].[batch_status] \r\n",
        "        WHERE [batch_id] = ?\r\n",
        "        ORDER BY [num_stages_complete] DESC;\r\n",
        "    \"\"\"\r\n",
        "    with pyodbc.connect(f'DRIVER={driver};SERVER=tcp:{dedicated_sql_endpoint};PORT=1433;DATABASE={dedicated_database};UID={sql_user_name};PWD={sql_user_pwd}',autocommit=True) as conn:\r\n",
        "        with conn.cursor() as cursor:\r\n",
        "            cursor.execute(query, batch_num)\r\n",
        "            num_stages_complete, description = cursor.fetchone()\r\n",
        "            return num_stages_complete, description\r\n",
        "\r\n",
        "def update_status_table(status_text, minted_tables_path, batch_num, driver, dedicated_sql_endpoint, sql_user_name, sql_user_pwd):\r\n",
        "    retries = 0 \r\n",
        "    exc = ''\r\n",
        "    while retries < 10:\r\n",
        "        try:\r\n",
        "            stages_complete, description = get_recent_status(batch_num, driver, dedicated_sql_endpoint, dedicated_database, sql_user_name, sql_user_pwd)\r\n",
        "            stages_complete += 1\r\n",
        "            status = f'[{stages_complete}/10] {status_text}'\r\n",
        "            x = datetime.now()\r\n",
        "            time_stamp = x.strftime(\"%Y-%m-%d %H:%M:%S\")\r\n",
        "\r\n",
        "            sql_command = f\"UPDATE batch_status SET status = ?, update_time_stamp = ?, num_stages_complete = ? WHERE batch_id = ?\"\r\n",
        "            with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+dedicated_sql_endpoint+';PORT=1433;DATABASE='+dedicated_database+';UID='+sql_user_name+';PWD='+ sql_user_pwd+'',autocommit=True) as conn:\r\n",
        "                with conn.cursor() as cursor:\r\n",
        "                    cursor.execute(sql_command, status, time_stamp, stages_complete, batch_num)\r\n",
        "                    cursor.commit()\r\n",
        "            return \r\n",
        "        except Exception as e:\r\n",
        "            exc_str = str(e)\r\n",
        "            exc = e \r\n",
        "            logger.warning(f'Failed to update status table: {exc_str}, retrying . . .')\r\n",
        "            retries += 1\r\n",
        "            sleep(3)\r\n",
        "\r\n",
        "    raise exc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Load media dataframe\n",
        "with tracer.span(name='Load media contents table'):\n",
        "    df_media_contents = spark.read.parquet(f'{minted_tables_output_path}{media_contents_tbl_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Retrieve access tokens and submit media file for processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from datetime import datetime, timedelta\n",
        "api_version = config.video_indexer_api_version\n",
        "\n",
        "#Set initial token refresh time in past to ensure immediate refresh.\n",
        "token_refresh_time = datetime.now() - timedelta(minutes=55)\n",
        "\n",
        "#Get ARM bearer token used to query AVAM for account access token.\n",
        "def get_arm_token():\n",
        "    # Get ARM access token (bearer token)\n",
        "    logger.info('Retrieving ARM token')\n",
        "    token_context = \"<<TF_VAR_azure_arm_management_api>>/.default\"\n",
        "    token = credential.get_token(token_context).token\n",
        "    logger.info(f\"ARM token retreived at {datetime.now()}\")\n",
        "    return token\n",
        "\n",
        "# Get account level access token for Azure Video Analyzer for Media\n",
        "def get_account_access_token(arm_token): \n",
        "    logger.info('Retrieving AVAM access token')\n",
        "    request_url = f'{azure_resource_manager}/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.VideoIndexer/accounts/{vi_account_name}/generateAccessToken?api-version={api_version}'\n",
        "    headers = CaseInsensitiveDict()\n",
        "    headers[\"Accept\"] = \"application/json\"\n",
        "    headers[\"Authorization\"] = \"Bearer \" + arm_token\n",
        "    body = '{\"permissionType\":\"Contributor\",\"scope\":\"Account\",\"projectId\":null,\"videoId\":null}'\n",
        "    body = json.loads(body)\n",
        "    response = req.post(request_url, headers=headers, json=body)\n",
        "    response = response.json()\n",
        "    logger.info(f\"AVAM access token retreived at {datetime.now()}\")\n",
        "    return response[\"accessToken\"]\n",
        "\n",
        "# Refresh access tokens\n",
        "def refresh_access_tokens():\n",
        "    global token_refresh_time\n",
        "    \n",
        "    logger.info('Refreshing ARM and AVAM tokens.')\n",
        "    token_refresh_time = datetime.now() + timedelta(minutes=55)\n",
        "    arm_token = get_arm_token()\n",
        "    # Return token dictionary as both tokens are used elsewhere, ARM token is also needed to get AVAM access token.\n",
        "    return {\n",
        "        \"arm\" : arm_token, \n",
        "        \"avam\" : get_account_access_token(arm_token)}\n",
        "\n",
        "#initial get of each token\n",
        "tokens = refresh_access_tokens()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Get Account information\n",
        "# these top level API's are documented and testable here... https://docs.microsoft.com/en-us/rest/api/videoindexer/accounts/list\n",
        "request_url = f'{azure_resource_manager}/subscriptions/{subscription_id}/resourcegroups/{resource_group_name}/providers/Microsoft.VideoIndexer/accounts/{vi_account_name}/?api-version={api_version}'\n",
        "headers = CaseInsensitiveDict()\n",
        "headers[\"Accept\"] = \"application/json\"\n",
        "headers[\"Authorization\"] = \"Bearer \" + tokens[\"arm\"]\n",
        "response = req.get(request_url, headers=headers)\n",
        "response = response.json()\n",
        "vi_account_id = response['properties']['accountId']\n",
        "vi_account_location = response['location']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# user defined function to generate media file url with SAS\n",
        "from azure.storage.blob import BlobClient, generate_blob_sas, BlobSasPermissions\n",
        "\n",
        "def get_blob_sas(account_name, account_key, container_name, blob_name):\n",
        "    sas_blob = generate_blob_sas(account_name=account_name, \n",
        "                                container_name=container_name,\n",
        "                                blob_name=blob_name,\n",
        "                                account_key=account_key,\n",
        "                                permission=BlobSasPermissions(read=True),\n",
        "                                expiry=datetime.utcnow() + timedelta(hours=1))\n",
        "    return sas_blob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import time\n",
        "\n",
        "## Configuration Variables #################################################################################################################################\n",
        "extra_retry_buffer = 10     # this is the amount of time we can add to the Retry-After we get back to wait until we send another post, just to be safe\n",
        "max_retry_loops = 5         # this is the max times we will retry until we break from the loop and move on\n",
        "############################################################################################################################################################\n",
        "\n",
        "# helper function to generate endcoded callback URL for Video Indexer service\n",
        "def generate_vi_callback_url(json_path, media_path, file_name, callbackurl):\n",
        "    # return the callback url with the enrichment json output path encoded as a param\n",
        "    encoded_callback_params = urllib.parse.urlencode([(\"json_path\", json_path), (\"media_path\", media_path), (\"media_file_name\", file_name)]) \n",
        "    return \"\".join([callbackurl, \"&\", encoded_callback_params])\n",
        "\n",
        "# helper function to submit a media file url with SAS to VI\n",
        "def post_media_for_enrichment(media_path, file_name, file_type):  \n",
        "\n",
        "    #Check to see if ARM and AVAM access tokens need to be refreshed, both expire after 1 hr.\n",
        "    global tokens\n",
        "    if(token_refresh_time < datetime.now()):\n",
        "        tokens = refresh_access_tokens()\n",
        "\n",
        "    # get container for the blob\n",
        "    start_text = 'abfss://'\n",
        "    start_index = media_path.find(start_text)+len(start_text)\n",
        "    end_text = '@'\n",
        "    end_index = media_path.find(end_text)\n",
        "    media_cointainer = media_path[start_index:end_index]\n",
        "    \n",
        "    # get path for the blob\n",
        "    start_text = f'dfs.{azure_storage_domain}/'\n",
        "    start_index = media_path.find(start_text)+len(start_text)\n",
        "    media_path_trunc = media_path[start_index:]\n",
        "    # get blob link with sas\n",
        "    sas_token = get_blob_sas(storage_account_name, storage_account_key, media_cointainer, media_path_trunc)\n",
        "    media_url = 'https://'+storage_account_name+ f'.blob.{azure_storage_domain}/'+media_cointainer+'/'+media_path_trunc+'?'+sas_token\n",
        " \n",
        "    # path to dump enrichments json\n",
        "    json_path = f'{enrichment_output_path_root}/media_processing_json/{file_name}.output.json'\n",
        "\n",
        "    # post media file for processing and provide a callback url to notify when completed\n",
        "    params = CaseInsensitiveDict()\n",
        "    params[\"accessToken\"] = tokens[\"avam\"]\n",
        "    params[\"name\"] = file_name[:79]      # truncated to 80 chars as this is the max supported\n",
        "    params[\"description\"] = json.dumps({\n",
        "        \"media_path\": media_path,\n",
        "        \"file_name\": file_name,\n",
        "        \"batch_number\": batch_num,\n",
        "        \"batch_file_count\": batch_file_count\n",
        "    })\n",
        "    params[\"privacy\"] = \"private\"\n",
        "    params[\"partition\"] = \"partition\"\n",
        "    params[\"videoUrl\"] = media_url\n",
        "    params[\"language\"] = \"auto\"\n",
        "    params[\"callbackUrl\"] = generate_vi_callback_url(json_path, media_path, file_name, callbackurl)\n",
        "    request_url = f'{apiUrl}/{vi_account_location}/Accounts/{vi_account_id}/Videos'\n",
        "\n",
        "    logger.info(f\"Posting to AVAM, Media URL: {media_url}\")\n",
        "    response = req.post(request_url, headers={'Accept': 'application/json'}, params=params)\n",
        "\n",
        "    vi_response_code = response.status_code\n",
        "    logger.info(f\"Post response code from AVAM: {vi_response_code}, Media URL: {media_url}\")\n",
        "\n",
        "    # variable to keep track of how many retry loops we have attempted\n",
        "    post_retry_counter = 1\n",
        "\n",
        "    # checking for response code of 429.  If we get this code, it means the video indexer service is oversaturated\n",
        "    # and we need to pause for the amount of time the service responses back with in the \"Retry-After\" header.\n",
        "    # For more info, see this article: https://docs.microsoft.com/en-us/azure/azure-video-analyzer/video-analyzer-for-media-docs/considerations-when-use-at-scale#respect-throttling\n",
        "    if vi_response_code == 429:\n",
        "        logger.info('AVAM response code == 429, entering retry loop')\n",
        "        vi_response_retry_after_header = int(response.headers['Retry-After'])\n",
        "        while vi_response_code == 429:\n",
        "            # sleep for the amount of time we get back in the Retry-After header, plus a configurable extra buffer time\n",
        "            time.sleep(vi_response_retry_after_header + extra_retry_buffer)    \n",
        "\n",
        "            # retry the post again\n",
        "            logger.info(f'AVAM request post attempt {post_retry_counter}/{max_retry_loops}')\n",
        "            response = req.post(request_url, headers=headers, params=params)\n",
        "            # capture the new response status code\n",
        "            vi_response_code = response.status_code\n",
        "            \n",
        "            # keep track of how many retry loops we have done\n",
        "            post_retry_counter += 1\n",
        "\n",
        "            # if we are past the max number of retry loops then break from the loop and move on\n",
        "            if post_retry_counter > max_retry_loops:\n",
        "                logger.error('AVAM request post has reached max retry attempts.')\n",
        "                break\n",
        "\n",
        "            pass\n",
        "        \n",
        "        pass\n",
        "\n",
        "    logger.info(f\"Media: {media_url}, successfully posted to AVAM.\")\n",
        "\n",
        "    response = response.json()\n",
        "    return json.dumps(response)\n",
        "\n",
        "    \n",
        "\n",
        "# convert function into UDF for later usage in dataframe\n",
        "# NOTE: not using currently as workaround for video indexer issue with oversaturation\n",
        "udf_post_media_for_enrichment = udf(post_media_for_enrichment, StringType())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "############################################\n",
        "##### Submit via withColumn()\n",
        "############################################\n",
        "\n",
        "'''with tracer.span(name='Starting cell withColumn thru each video and call the VI endpoint...'):\n",
        "    # Submit all media files using the UDF and add fresponse to df\n",
        "    df_media_contents_submitted = df_media_contents \\\n",
        "        .withColumn(\"vi_submission_detail\", udf_post_media_for_enrichment(\n",
        "            df_media_contents.path, df_media_contents.file_name, df_media_contents.file_type)) '''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "############################################\n",
        "##### Submit via Looper\n",
        "############################################\n",
        "\n",
        "with tracer.span(name='Starting cell to loop thru each video and call the VI endpoint...'):\n",
        "    import time\n",
        "\n",
        "    # Create empty Dataframe with proper shape to use if no media files are in manifest.txt\n",
        "    empty_RDD = spark.sparkContext.emptyRDD() \n",
        "    columns = StructType([\n",
        "        StructField(\"path\", StringType()),\n",
        "        StructField(\"file_name\", StringType()),\n",
        "        StructField(\"file_type\", StringType()),\n",
        "        StructField(\"vi_submission_detail\", StringType()),\n",
        "    ])\n",
        "    empty_df = spark.createDataFrame(data = empty_RDD, schema = columns)\n",
        "    ## Configuration Variables #################################################################################################################################\n",
        "    ## variables to configure the amount of time to sleep per loop through each video and\n",
        "    ## the modulus for how many to run in a row before a longer pause if needed\n",
        "    sleep_modulus = 5                   # the amount of videos per batch to run through before sleeping for a longer time period\n",
        "    sleep_time_per_normal_loop = 0      # the amount of time to sleep per video in general\n",
        "    sleep_time_per_modulus_loop = 0     # the amount of time to sleep after a given batch of videos have been processed\n",
        "    ############################################################################################################################################################\n",
        "\n",
        "    # convert the dataframe with each video into a simple list to make easier to loop through\n",
        "    # in an old fashion for loop, as the azure credential object is non-picklable\n",
        "    media_contents_list = df_media_contents.collect()\n",
        "    submitted_list = []\n",
        "    submitted_list_cols = [\"path\", \"file_name\", \"file_type\", \"vi_submission_detail\"]\n",
        "    totalrows = df_media_contents.count()\n",
        "    counter = 1\n",
        "\n",
        "    # main loop to process all videos to video indexer service\n",
        "    for row in media_contents_list:\n",
        "\n",
        "        path = row['path']\n",
        "        file_name = row['file_name']\n",
        "        file_type = row['file_type']\n",
        "\n",
        "        # call helper function to submit video to the video indexer service\n",
        "        response = post_media_for_enrichment(path, file_name, file_type)\n",
        "        # append file info and response results of the video indexer service call\n",
        "        # to a new list to later convert back into a dataframe and save into a table\n",
        "        # for later review\n",
        "        submitted_list.append([path, file_name, file_type, str(response)])\n",
        "\n",
        "        # use this section only if needing to add sleeps per video or per batch of videos\n",
        "        if counter % sleep_modulus == 0:\n",
        "            time.sleep(sleep_time_per_modulus_loop)\n",
        "        else:\n",
        "            time.sleep(sleep_time_per_normal_loop)\n",
        "        counter += 1  \n",
        "\n",
        "        pass\n",
        "\n",
        "    # convert our new list of processing resuslts into a dataframe for later saving into a table\n",
        "    df_media_contents_submitted = empty_df\n",
        "    if len(media_contents_list) > 0: \n",
        "        df_media_contents_submitted = spark.createDataFrame(submitted_list, schema=submitted_list_cols)\n",
        "    else:\n",
        "        update_status_table('Media Processing Complete', minted_tables_output_path, batch_num, driver, dedicated_sql_endpoint, sql_user_name, sql_user_pwd)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Extract any anomaly responses, anything apart from \"State: Processing\", to a new column to identify these files easily\n",
        "# https://sparkbyexamples.com/pyspark/pyspark-json-functions-with-examples/\n",
        "df_media_contents_submitted = df_media_contents_submitted\n",
        "df_media_contents_submitted = df_media_contents_submitted.select(col(\"path\"),col(\"file_name\"),col(\"file_type\"),col(\"vi_submission_detail\"),json_tuple(col(\"vi_submission_detail\"),\"state\")) \\\n",
        "    .toDF(\"path\",\"file_name\",\"file_type\",\"vi_submission_detail\", \"state\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span(name='Persist submitted media details as table'):\n",
        "    submitted_media_tbl_name = f'{batch_num}_submitted_media'\n",
        "    df_media_contents_submitted \\\n",
        "        .write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{submitted_media_tbl_name}')\n",
        "    ext_table_command = (\n",
        "        f\"IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{submitted_media_tbl_name}') \"\n",
        "        f\"CREATE EXTERNAL TABLE [{submitted_media_tbl_name}] (\"\n",
        "            \"[path] nvarchar(4000), \"\n",
        "            \"[file_name] nvarchar(4000), \"\n",
        "            \"[file_type] nvarchar(4000), \"\n",
        "            \"[vi_submission_detail] nvarchar(4000),\"\n",
        "            \"[state] nvarchar(4000)\"\n",
        "        \") \"\n",
        "        f\"WITH (LOCATION = 'minted_tables/{submitted_media_tbl_name}/**', DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "    )\n",
        "    with pyodbc.connect(f'DRIVER={driver};SERVER=tcp:{serverless_sql_endpoint};PORT=1433;DATABASE={database};UID={sql_user_name};PWD={sql_user_pwd}') as conn:\n",
        "        with conn.cursor() as cursor:\n",
        "            cursor.execute(ext_table_command)\n",
        "            \n",
        "# return name of new table\n",
        "output = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'submitted_media_tbl_name': submitted_media_tbl_name,\n",
        "    'file_system': file_system,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "\n",
        "# Return the object to the pipeline\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\n",
        "mssparkutils.notebook.exit(output['custom_dimensions'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
