{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "     \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "     \"spark.dynamicAllocation.enabled\": true,\n",
        "     \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "     \"spark.dynamicAllocation.maxExecutors\": 8\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "summarized_text_xsum_tbl_name = ''\n",
        "summarized_text_dailymail_tbl_name = ''\n",
        "documents_contents_tbl_name = ''\n",
        "batch_root = ''\n",
        "batch_num = ''\n",
        "batch_description = ''\n",
        "input_container=''\n",
        "output_container=''\n",
        "blob_account_name = ''\n",
        "rule_set_eval_timeout = 180\n",
        "azure_storage_domain = ''\n",
        "minted_tables_output_path = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Column names \n",
        "file_path_col = \"file_path\"\n",
        "summarized_text_xsum_col = \"summarized_text_xsum\"\n",
        "summarized_text_dailymail_col = \"summarized_text_dailymail\"\n",
        "\n",
        "summarization_xsum_error_col = \"summarization_xsum_error\"\n",
        "summarization_dailymail_error_col = \"summarization_dailymail_error\"\n",
        "errors_col = \"errors\"\n",
        "\n",
        "key_col = file_path_col\n",
        "\n",
        "error_cols = [\n",
        "    summarization_xsum_error_col,\n",
        "    summarization_dailymail_error_col\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "import datetime\n",
        "\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'documents_contents_tbl_name': documents_contents_tbl_name,\n",
        "    'summarized_text_xsum_tbl_name': summarized_text_xsum_tbl_name,\n",
        "    'summarized_text_dailymail_tbl_name': summarized_text_dailymail_tbl_name,\n",
        "    'batch_description': batch_description,\n",
        "    'batch_root': batch_root,\n",
        "    'batch_num': batch_num,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "import os\n",
        "import csv\n",
        "from types import SimpleNamespace\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql.functions import col\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Initialise session and config\n",
        "sc = spark.sparkContext\n",
        "spark = SparkSession.builder.appName(f\"TextProcessing {mssparkutils.runtime.context}\").getOrCreate()\n",
        "\n",
        "config = json.loads(''.join(sc.textFile(f'{batch_root}/config.json').collect()), object_hook=lambda dictionary: SimpleNamespace(**dictionary))\n",
        "\n",
        "# Set log level\n",
        "if config.log_level == \"INFO\":\n",
        "    logger.setLevel(logging.INFO)\n",
        "else:\n",
        "    logger.setLevel(logging.ERROR)\n",
        "    config.log_level = \"ERROR\"\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyodbc\r\n",
        "from pyspark.sql.functions import current_timestamp\r\n",
        "# Dedicated and serverless SQL config\r\n",
        "dedicated_database = 'dedicated'\r\n",
        "database = 'minted'   \r\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\r\n",
        "\r\n",
        "# secrets\r\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\r\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\r\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseServerlessSQLEndpoint\")\r\n",
        "dedicated_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseDedicatedSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with tracer.span(name=f'Read and join all dataframes'):\n",
        "    \n",
        "    # Load Dataframes\n",
        "    document_contents_df = spark.read.parquet(f'{minted_tables_output_path}{documents_contents_tbl_name}')\n",
        "    xsum_df = spark.read.parquet(f'{minted_tables_output_path}{summarized_text_xsum_tbl_name}')\n",
        "    dailymail_df = spark.read.parquet(f'{minted_tables_output_path}{summarized_text_dailymail_tbl_name}')\n",
        "\n",
        "    # Join Dataframes\n",
        "    docs_df = ( document_contents_df\n",
        "        .join(xsum_df, file_path_col, 'left_outer')\n",
        "        .join(dailymail_df, file_path_col, 'left_outer')\n",
        "        .select(\n",
        "            document_contents_df['file_path'],\n",
        "            document_contents_df['file_name'],\n",
        "            document_contents_df['file_type'],\n",
        "            document_contents_df['text_content'],\n",
        "            document_contents_df['original_lang'],\n",
        "            document_contents_df['text_content_target_lang'],\n",
        "            xsum_df[summarized_text_xsum_col],\n",
        "            xsum_df[summarization_xsum_error_col],\n",
        "            dailymail_df[summarized_text_dailymail_col],\n",
        "            dailymail_df[summarization_dailymail_error_col]\n",
        "        )\n",
        "        .withColumn('batch_num', F.lit(batch_num))\n",
        "    )\n",
        "# Re-group errors to simplify processing on the UI\n",
        "docs_df = docs_df.withColumn(errors_col, F.array(*[ \n",
        "    F.struct(F.when((docs_df[column].isNull() | (docs_df[column] == \"\")), F.lit(\"\")).otherwise(docs_df[column]).alias(\"message\"),\n",
        "    F.lit(column.replace(\"_error\", \"\")).alias(\"stage\")) \n",
        "    for column in error_cols]))\n",
        "docs_df = docs_df.drop(*error_cols)\n",
        "\n",
        "with tracer.span(name='Persist processed text as SQL tables'):\n",
        "    processed_summarised_text_tbl_name = f'{batch_num}_summarised_processed_text'\n",
        "    docs_df.write.mode(\"overwrite\").parquet(f'{minted_tables_output_path}{processed_summarised_text_tbl_name}')\n",
        "    sql_command = f'''\n",
        "        IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{processed_summarised_text_tbl_name}') \n",
        "        CREATE EXTERNAL TABLE [{processed_summarised_text_tbl_name}] (\n",
        "            [file_path] nvarchar(4000), \n",
        "            [file_name] nvarchar(4000), \n",
        "            [file_type] nvarchar(4000), \n",
        "            [text_content] nvarchar(max),\n",
        "            [original_lang] nvarchar(4000),\n",
        "            [text_content_target_lang] nvarchar(max),\n",
        "            [summarized_text_xsum] nvarchar(max),\n",
        "            [summarized_text_dailymail] nvarchar(max),\n",
        "            [batch_num] nvarchar(4000),\n",
        "            [errors] varchar(max)\n",
        "        )\n",
        "        WITH (\n",
        "            LOCATION = 'minted_tables/{processed_summarised_text_tbl_name}/**', \n",
        "            DATA_SOURCE = [synapse_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], \n",
        "            FILE_FORMAT = [SynapseParquetFormat]\n",
        "        )\n",
        "    '''\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "        with conn.cursor() as cursor:\n",
        "            cursor.execute(sql_command)\n",
        "\n",
        "\n",
        "with tracer.span(name='Persist processed text as json'):\n",
        "    output_path = f'abfss://{output_container}@{blob_account_name}.dfs.{azure_storage_domain}/{batch_num}'\n",
        "\n",
        "    docs_df = docs_df \\\n",
        "        .withColumn(\"json\", F.to_json(F.struct(col(\"*\"))))\n",
        "\n",
        "    out_lst = docs_df.collect()\n",
        "\n",
        "    for row in out_lst:\n",
        "        p = f'{output_path}/text_summarisation_processing_json/{row.file_name}.output.json'\n",
        "        mssparkutils.fs.put(p, row.json, overwrite=True)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from time import sleep\r\n",
        "# Update Status Table\r\n",
        "def get_recent_status(batch_num, driver, dedicated_sql_endpoint, dedicated_database, sql_user_name, sql_user_pwd):\r\n",
        "    query = f\"\"\"\r\n",
        "        SELECT TOP (1) \r\n",
        "        [num_stages_complete], [description]\r\n",
        "        FROM [dbo].[batch_status] \r\n",
        "        WHERE [batch_id] = ?\r\n",
        "        ORDER BY [num_stages_complete] DESC;\r\n",
        "    \"\"\"\r\n",
        "    with pyodbc.connect(f'DRIVER={driver};SERVER=tcp:{dedicated_sql_endpoint};PORT=1433;DATABASE={dedicated_database};UID={sql_user_name};PWD={sql_user_pwd}',autocommit=True) as conn:\r\n",
        "        with conn.cursor() as cursor:\r\n",
        "            cursor.execute(query, batch_num)\r\n",
        "            num_stages_complete, description = cursor.fetchone()\r\n",
        "            return num_stages_complete, description\r\n",
        "\r\n",
        "def update_status_table(status_text, minted_tables_path, batch_num, driver, dedicated_sql_endpoint, sql_user_name, sql_user_pwd):\r\n",
        "    retries = 0 \r\n",
        "    exc = ''\r\n",
        "    while retries < 10:\r\n",
        "        try:\r\n",
        "            stages_complete, description = get_recent_status(batch_num, driver, dedicated_sql_endpoint, dedicated_database, sql_user_name, sql_user_pwd)\r\n",
        "            stages_complete += 1\r\n",
        "            status = f'[{stages_complete}/10] {status_text}'\r\n",
        "            x = datetime.datetime.now()\r\n",
        "            time_stamp = x.strftime(\"%Y-%m-%d %H:%M:%S\")\r\n",
        "\r\n",
        "            sql_command = f\"UPDATE batch_status SET status = ?, update_time_stamp = ?, num_stages_complete = ? WHERE batch_id = ?\"\r\n",
        "            with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+dedicated_sql_endpoint+';PORT=1433;DATABASE='+dedicated_database+';UID='+sql_user_name+';PWD='+ sql_user_pwd+'',autocommit=True) as conn:\r\n",
        "                with conn.cursor() as cursor:\r\n",
        "                    cursor.execute(sql_command, status, time_stamp, stages_complete, batch_num)\r\n",
        "                    cursor.commit()\r\n",
        "            return \r\n",
        "        except Exception as e:\r\n",
        "            exc_str = str(e)\r\n",
        "            exc = e \r\n",
        "            logger.warning(f'Failed to update status table: {exc_str}, retrying . . .')\r\n",
        "            retries += 1\r\n",
        "            sleep(3)\r\n",
        "\r\n",
        "    raise exc\r\n",
        "\r\n",
        "update_status_table('Text Summarization Complete', minted_tables_output_path, batch_num, driver, dedicated_sql_endpoint, sql_user_name, sql_user_pwd)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# return name of new table\n",
        "output = {'custom_dimensions': {\n",
        "    'batch_num': batch_num,\n",
        "    'processed_text_tbl_name': processed_summarised_text_tbl_name,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "\n",
        "# Return the object to the pipeline\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: OUTPUT\", extra=output)\n",
        "mssparkutils.notebook.exit(output['custom_dimensions'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
